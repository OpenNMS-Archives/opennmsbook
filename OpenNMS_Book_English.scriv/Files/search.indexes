<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="121">
            <Title>Ports</Title>
            <Text>OpenNMS Server
The following Ports are used on the OpenNMS Server.
Note: those are the default values which may be changed in the appropriate config files.
httpAdaptor     8180
rtc             5817
jetty           8980
jetty / https   8443
jetty / ajp     8981

syslogd         514 or 1514
snmp trap       162
(vulnscan        1241)
[edit] Ports scanned by OpenNMS
There is a large number of ports which can be scannd by OpenNMS. Look in capsd and poller configuration for explanations.
</Text>
        </Document>
        <Document ID="117">
            <Title>Managing Configuration</Title>
            <Text>OpenNMS's configuration is very powerful, but with that unfortunately comes a level of complexity that can make it difficult to manage upgrades. One of the easiest ways to mitigate this is by using git to manage your configuration upgrades.
Contents

[hide]
	•	1 Getting Started
	◦	1.1 Start with a Pristine etc Directory
	◦	1.2 Initialize Git
	◦	1.3 Create Your Branch
	◦	1.4 Make Your Changes
	•	2 Doing an Upgrade
	◦	2.1 Stop OpenNMS
	◦	2.2 Make Sure All Changes are Committed
	◦	2.3 Switch to the Pristine Branch
	◦	2.4 Upgrade OpenNMS
	◦	2.5 Switch to Your Modified Branch
	◦	2.6 Apply Changes from Master
	◦	2.7 Check for Other Changes
	◦	2.8 Start OpenNMS
[edit] Getting Started
First, if you're not familiar with Git, you may want to read our developing with Git page, as well as the Git tutorial.
If you have an existing OpenNMS install, the easiest way to get started is to make sure you have a version of OpenNMS which provides the "etc-pristine" directory (in the 1.6.x series, OpenNMS 1.6.9 or higher, and in the 1.7/1.8 series, OpenNMS 1.7.9 or higher).
If this is your first time installing OpenNMS, you can skip to the Create Your Branch section.
Don't forget to stop OpenNMS before messing with the etc directory!
[edit] Start with a Pristine etc Directory
If you have an existing OpenNMS installation, move your existing $OPENNMS_HOME/etc directory out of the way:
 mv $OPENNMS_HOME/etc $OPENNMS_HOME/etc.bak
Then, copy etc-pristine to $OPENNMS_HOME/etc. On RPM-based installations, this is in $OPENNMS_HOME/share/etc-pristine, on Debian, it's in /usr/share/opennms/etc-pristine.
 cp -pR $OPENNMS_HOME/share/etc-pristine $OPENNMS_HOME/etc
[edit] Initialize Git
Next, you'll turn your etc directory into a git repository, and add the pristine files as the first commit.
 cd $OPENNMS_HOME/etc
 git init 
 git add .
 git commit -m "Initial checkin of OpenNMS x.x.x configuration."
[edit] Create Your Branch
Finally, you'll create your branch to make local modifications. This way, whenever it's time to upgrade, you can put your git repo back in "pristine" mode to catch changes since the last version.
 cd $OPENNMS_HOME/etc
 git branch local-modifications
 git checkout local-modifications
[edit] Make Your Changes
Now, just edit your configuration normally, and you're all set. If you have an existing config that was backed up, you can just run:
 rsync -avr $OPENNMS_HOME/etc.bak/ $OPENNMS_HOME/etc/
Whenever you're done making changes to your etc directory, add and commit them, like so:
 cd $OPENNMS_HOME/etc
 git add .
 git commit -m "Added initial discovery ranges."
Now you can start OpenNMS back up again.
[edit] Doing an Upgrade
Now that you've got your etc directory managed by Git, it should be much easier to perform an upgrade. You just need to switch back to the pristine copy, upgrade, switch back, and merge the changes. Git has a very good merge algorithm, and will merge most configuration changes without any need for manual intervention.
[edit] Stop OpenNMS
Of course, the first step before doing an upgrade is to stop OpenNMS. =)
 /etc/init.d/opennms stop
[edit] Make Sure All Changes are Committed
Don't forget that if any files are modified, you'll want to add and commit them before doing the upgrade.
 git status
If it says there are modifications, follow the instructions in Make Your Changes above.
[edit] Switch to the Pristine Branch
The next step is to put your $OPENNMS_HOME/etc back to its pristine condition. To do this, all you need to do is check out the "master" branch, which contains the version of files from Initialize Git above.
 git checkout master
[edit] Upgrade OpenNMS
Now, upgrade OpenNMS in the usual manner (yum upgrade opennms, apt-get upgrade opennms, etc.). You should have no conflicts, since the etc files looked exactly like they did in the original package.
If you now run status:
 git status
... you'll see any changes to the default configs since the version of OpenNMS you've installed.
If any files have been deleted, you'll need to run 'git rm &lt;filename>' to tell git that they're gone.
 git rm CHANGELOG
 git rm map.disable
...and so on.
Then you'll need to add the new and changed files, and save these changes to the pristine/master branch, by adding and committing them:
 git add .
 git commit -m 'Upgraded to OpenNMS x.x.x.'
[edit] Switch to Your Modified Branch
Now that OpenNMS is upgraded, and Git knows about the changes to the configs since your previous release, it's time to switch back to your modified branch. All you need to do is check it out:
 git checkout local-modifications
[edit] Apply Changes from Master
Finally, you need to apply the changes from master to your local branch. To do so, you just need to 'merge' master into the current working branch ("local-modifications") like so:
 git merge master
If all goes well, you will see that the files that were changed in the master branch were auto-applied to your "local-modifications" working copy.
If not, you'll see something like this:
 git merge master
 Removing CHANGELOG
 Removing map.disable
 Auto-merging surveillance-views.xml
 CONFLICT (content): Merge conflict in surveillance-views.xml
 Automatic merge failed; fix conflicts and then commit the result.
A 'git status' will reveal the things that need modification with a "both modified":
 ...
 # Unmerged paths:
 #   (use "git reset HEAD &lt;file>..." to unstage)
 #   (use "git add &lt;file>..." to mark resolution)
 #
 #	both modified:      surveillance-views.xml
Git will insert a standard CVS-style conflict marker in each file where there are conflicts between your local changes and the updates. The changes from your branch will be first, and the changes from the pristine copies will be second, like so:

      &lt;rows>
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
        &lt;row-def row="1" label="Monkey Heads" >
          &lt;category name="Monkey Heads"/>
        &lt;/row-def>
        &lt;row-def row="2" label="Routers" >
          &lt;category name="Routers"/>
        &lt;/row-def>
        &lt;row-def row="3" label="Switches" >
          &lt;category name="Switches" />
        &lt;/row-def>
        &lt;row-def row="4" label="Servers" >
=======
        &lt;row-def label="Routers" >
          &lt;category name="Routers"/>
        &lt;/row-def>
        &lt;row-def label="Switches" >
          &lt;category name="Switches" />
        &lt;/row-def>
        &lt;row-def label="Servers" >
>>>>>>> master
          &lt;category name="Servers" />
        &lt;/row-def>
...just edit the files until they look like you'd expect (in this case, the 'row="n"' bits were removed from surveillance-views.xml) and add and commit the changes:
 git add .
 git commit
[edit] Check for Other Changes
If you're curious what changed in the last upgrade (ie, the last change done to the 'master' branch), you can diff master against its previous commit like so:
 git diff master~1..master
This works no matter which branch you're in at the time.
[edit] Start OpenNMS
Now that everything's merged, just start OpenNMS back up, and you should be in business!
</Text>
        </Document>
        <Document ID="72">
            <Title>Mobile Access</Title>
        </Document>
        <Document ID="68">
            <Title>Scheduled Outages</Title>
        </Document>
        <Document ID="73">
            <Title>Performance Tuning</Title>
            <Text>Tuning Overview
[edit] Hardware
	•	If you design a new OpenNMS system carefully read the hardware considerations below.
	•	If you have already a running system, you might still find some possibilities to get nearer to the design described below
	•	Disk I/O and system memory are the points you should look at
	•	Remember that a 64-bit CPU is required in order for a single process to address more than about 2GB of memory, even with a PAE-aware kernel.
[edit] Operating system
	•	There are some parameters regarding the filesystems for database and collected data to tune
	•	System's shared memory pool might need increasing for the database
	•	If you have 64-bit hardware, be sure to install a 64-bit operating system in order to address more than 4GB of physical memory
[edit] Database PostgreSQL
	•	most difficult but very important part as there are a lot of parameters to tune
[edit] Java virtual machine
	•	Heap space, permanent generation size, and garbage collection
[edit] OpenNMS
	•	here you can generate a lot of data so carefully design what you really need
	•	logging
	•	data collection
	•	data storage and consolidation
	•	discovery
	•	polling
	•	houskeeping
[edit] Hardware considerations
If at all possible, use a server with a 64-bit CPU as this will enable the CPU to address more than 4GB of physical memory. Remember that even with a PAE-aware kernel / operating system, most 32-bit OSes don't allow a given process to address more than about 2GB of memory.
Probably the biggest performance improvement on systems that are collecting a lot of RRD data is to move PostgreSQL and Tomcat to a separate system from OpenNMS daemons! Huge difference.
On a server with hardware RAID, consider investing in a battery-backed write cache. On a HP DL380 G4, the I/O wait of the server dropped from an average of 15% to almost nil with the addition of a 128 MB BBWC. Additionally, ensure that you have ample memory on the system, on a HP G4 - single processor 4 Gigs of memory monitoring about 300 devices with 700 interfaces, our I/O wait time steadily began to climb. The CPU wait time was obsessively hogging all of the processor, making OpenNMS crawl, we resolved this by upping our memory to 12 Gigs of memory, which in turn brought the wait time back down to 1%.
For a small collection of monitored nodes, moving the RRD data area into a tmpfs / RAM drive may also alleviate the I/O wait caused by all of the writing required by the RRD data. The trade-off is that a server crash or power-down will cause the RRD files to be lost, unless you implement a sync tool to sync the RAM drive to a disk backup.
[edit] Disk Tuning
Because OpenNMS is well-equipped for gathering and recording details regarding network and systems performance and behavior, it tends to be a write-heavy application. If your environment offers a very large number of data points to be managed, it would serve you well to ensure that a large degree of spindle separation exists. In particular and where possible, ensure that:
	•	OpenNMS SNMP Collection
	•	OpenNMS Response Time Collection
	•	OpenNMS (and system) logging
	•	PostgreSQL Database
	•	PostgreSQL Writeahead logging
 ..occur on separate spindles, and in some cases separate drives or separate devices. Further, in a *Nix environment, it may behoove you to ensure that the RRD's end up on different mounts, so one has the option of mounting with the noatime and nodiratime directives without compromising other aspects of the system configuration.
The defaults for the opennms directories mentioned above are
 /opt/opennms/share/rrd/snmp
 /opt/opennms/share/rrd/response
 /opt/opennms/logs or /var/log/opennms
but watch out for symbolic links!
As a filesystem, the best performance is achieved with XFS. EXT(2,3) have built-in limitations in the number of file descriptors per directory and can not be used on larger installations.
The data storage is the critical factor, hence the capacity of the storage must match the size of the installation: Best performance is achieved with SAN's (FibreChannel + Netapp or EMC or ..). The important point is that the IO Queue is kept on the "other" device and not on the OpenNMS Server.
Recently good results for smaller systems have been reported with SSD Drives.
To tell if you have a bottle neck with your disk you can use a couple of quick things. In "top" you can look for the waiting CPU percentage. For instance in top you hit "1" to break out all of the individual cores/CPU's and see that one of the CPU's has 100% wait. This could be from the swap file or any of the directories listed above.
The "nmon" program can show more detailed information. You will be able to see what spindles are being used when and how much read it has versus writes.
[edit] Memory-backed File Systems
One option, if your server has a lot of RAM, is to modify the OpenNMS startup scripts to maintain a memory-backed file system, combined with automatic backups and restores that handle any internally decided risk levels/SLAs. In Linux, this would be a tmpfs file system.
       # XXX Custom code herein for dealing with memory drives
       mount | grep -q rrd
       if [ $? -ne 0 ]; then
               # RRD location is not present, create it and
               # unpack our data.
               mount -t tmpfs -o size=2G,nr_inodes=200k,mode=0700 tmpfs /opt/opennms/share/rrd
               cd /
               tar xf /mnt/db-backup/opennms-rrd.tar
       fi
       # XXX End custom code
This modification to /opt/opennms/bin/opennms is matched with a crontab entry that generates the opennms-rrd.tar file periodically.
In-the-field: On a DL380 G4, with 6 GB of RAM, 2 GB of RAM was allocated to a memory-backed file system. This reduced the disk I/O load (one shared RAID-10 for Postgres, OS and JRBs; with battery-backed cache) from 300 IOPS to 10 IOPS, along with a correlated drop in load average and response times for the OpenNMS UI.
N.B. In Linux, a tmpfs file system will go to swap if memory pressure demands real memory for applications. This can have a very negative effect on the I/O load and system performance.
[edit] Operating system
	1.	Do run a 64-bit kernel so that OpenNMS will be able to address more than 2GB of memory.
	2.	Don't run in a VM.
	3.	Don't put DB or RRD data on file systems managed by LVM.
	4.	Don't put DB or RRD data on file systems on RAID-5.
	5.	Do put OpenNMS logs and RRDs and PostgreSQL data on separate spindles or separate RAID sets. Read details for postgres and RRD below.
	6.	Do run on a modern kernel. Linux 2.6 and later as well as Solaris 10 or newer are good. Stay away from Linux 2.4, in particular.
	7.	Set noatime mount flag on filesystems hosting data for #4 above.
	8.	Adapt the systems shared memory to the database, see Performance tuning#PostgreSQL and system's shared memory
	9.	Solaris 10 systems may require increasing ICMP buffer size if polling large numbers of systems (ndd -set /dev/icmp icmp_max_buf 2097152). Use 'netstat -s -P icmp' and check the value of 'icmpInOverflows' to determine if you're overflowing the ICMP buffer.
[edit] Database PostgreSQL
The default shared_buffers parameter in postgresql.conf is extremely conservative, and in most cases with modern servers, this can be significantly tweaked for a big performance boost, and drop in I/O wait time. This change will need to be in-line with kernel parameter changes to shmmax. See Postgres Wiki tuning page and this PostgreSQL performance page for recommendations on this and other postgresql settings.
If you want to put PostgreSQL on a different box then you want to change the SQL host look in opennms-datasources.xml. The PostgreSQL server will also need iplike installed and configured.
To clean up extra events out of the database try this Event_Configuration_How-To#The_Database
[edit] PostgreSQL 8.1 and later
These changes to postgresql.conf will probably improve your DB performance if you have a enough RAM (about 2GB installed RAM for a dedicated server) to support the changes. (YMMV) You'll probably need to make adjustments to the shmmax kernel attribute on your system.
shared_buffers = 20000
work_mem = 16348
maintenance_work_mem = 65536
vacuum_cost_delay = 50
checkpoint_segments = 20
checkpoint_timeout = 900
wal_buffers = 64
stats_start_collector = on
stats_row_level = on
autovacuum = on
I've also set these higher values on *bigger* systems:
wal_buffers = 256
work_mem = 32768
maintenance_work_mem = 524288
On postgres 8.3 systems they've changed the format to allow you to specify amounts as memory allocations instead of number of blocks. Here are the equivalents:
shared_buffers = 164MB
work_mem = 16MB
maintenance_work_mem = 64MB
vacuum_cost_delay = 50
checkpoint_segments = 20
checkpoint_timeout = 15min
wal_buffers = 256kB
stats_start_collector = on
stats_row_level = on
autovacuum = on
If you need the bigger values for larger systems here they are:
wal_buffers = 2048kB
work_mem = 32MB
maintenance_work_mem = 512MB
[edit] Systems with lots of RAM and PostgreSQL 8.2
Recently, we've found that changing the max_fsm_pages and max_fsm_releations 10 fold on systems with plenty of memory (4G+), improves performance dramatically.
#max_fsm_pages = 204800		# min max_fsm_relations*16, 6 bytes each
max_fsm_pages = 2048000
#max_fsm_relations = 1000		# min 100, ~70 bytes each
max_fsm_relations = 10000
(Note that the free space map has been reimplemented in PostgreSQL 8.4 and is now self-maintaining, so the max_fsm_* settings above are not necessary if you're running PostgreSQL 8.4.1 or later - note that 8.4.0 is not supported due to a nasty bug.)
As well as really bumping these:
work_mem = 100MB
maintenance_work_mem = 128MB
Note: To make adjustments to shmmax, do the following:
Start postgresql from the command line:
sudo -u postgres pg_ctl -D /var/lib/pgsql/data start
(adjusting paths as necessary) and look at the error message:
# FATAL:  could not create shared memory segment: Invalid argument
DETAIL:  Failed system call was shmget(key=5432001, size=170639360, 03600).
HINT:  This error usually means that PostgreSQL's request for a shared memory segment exceeded your kernel's SHMMAX parameter.  
You can either reduce the request size or reconfigure the kernel with larger SHMMAX. 
 To reduce the request size (currently 170639360 bytes), reduce PostgreSQL's shared_buffers parameter (currently 20000) 
and/or its max_connections parameter (currently 100).
Notice the value of "size".
Then up the value of shmmax:
sysctl -w kernel.shmmax=170639360
And restart postgresql (using the normal method such as "service postgresql start")
Finally, edit /etc/sysctl and add the line
kernel.shmmax=170639360
so it will survive a reboot.
[edit] PostgreSQL and system's shared memory
If your OpenNMS system tend to have long response times and has
	•	no disk I/O-waits
	•	a lot of CPU idle time
then try to increase your operating systems shared memory (and that of postgres) as described above. The values written above are the absolut minimum values. Increasing the systems shared memory may greatly boost OpenNMS performance as it will speed up the communication between OpenNMS and the database. Try different values for the system's shared memory, even up to 10 times or more of the minimum value as described above. For further details see the links to postgres Wiki doku mentioned above.
[edit] PostgreSQL *any* Version
One additional configuration that seems to make a tremendous amount of peformance improvement is having the write-head logs on a separate spindle (even better a separate disk controller/channel). The way to do this is:
	1.	shutdown opennms / tomcat
	2.	shutdown postgresql
	3.	cd to $PG_DATA
	4.	mv pg_xlog &lt;file system on different spindle>
	5.	ln -s &lt;file system on different spindle>/pg_xlog pg_xlog
	6.	restart postgresql
Make sure postgres data and write-ahead logs do not live on a RAID-5 disk subsystem.
[edit] iplike stored procedure
See the documentation in iplike to be sure you have the best version of iplike running
[edit] postgres and disk I/O waits
Standard postgres configuration writes transactions to the disk before comitting them. If there are I/O-problems (waitstates) database transactions suffer, high application responsetimes are the result. On test machines -running most times on inappropriate hardware- synchronous writes may be disabled. In case of a system crash database inconsistencies may result, requesting rollback of the transaction log etc.. For test systems this is normally no problem.
Try with following configuration changes in postgresql.conf on postgres 8.3 (or newer):
fsync = off
synchronous_commit = on
commit_delay = 1000
[edit] find problems due to long-running queries
If there is a reasonable suspicion that some queries are running for a very long time edit the postgresql.conf and change the parameter (PostgreSQL up to 8.3)
log_min_duration_statement=1000
This will log all queries running for more than 1000 ms to postgresql.log.
After this change a stop/start of opennms and postgres is required. Don't forget to remove this configuration after debugging is finished.
Probably you will find that most times "bad database response time" is not due to a single query running for a long time but due to thousands of queries running for a very short time.
[edit] optimization for a lot of small queries
If anybody knows how to optimize PostgreSQL / OpenNMS for this please add it here! There are parameters like max_connections in postgresql.conf and c3p0.maxPoolSize in $OPENNMS_HOME/etc/c3p0.properties which might help here.
[edit] Java Virtual Machine (JVM)
The following phaenomena of opennms are typical for running low on memory in the java virtual machine:
	•	long response times
	•	garbage collection is running very often and takes a lot of time (see below)
	•	alarms that should have been cleared automatically are still listed as alarms
[edit] Tuning heap size
Enable extensive garbage collection logging (see below) to see the behaviour looking at output.log. If garbage collections regularly take a lot of time (0.5 seconds is an empirical threshold) or are running very often (more than every 10-20 seconds) the java heap size should be increased. If it's running every 10 seconds and takes 9 seconds the system is stuck...
Parameters for tuning java may be added in $OPENNMS_HOME/etc/opennms.conf. If that file doesn't already exist, check in $OPENNMS_HOME/etc/examples/opennms.conf for a template.
The most important parameter is the java heap size
JAVA_HEAP_SIZE=size_in_MBytes
The default value is 256 which is sufficient only for test cases with one to five managed devices.
You can roughly test performance improvement opening the event list from opennms, adding ?limit=250 to the url and pressing Return
http://opennms:8980/opennms/event/list?limit=250
Now there should be 250 events in your list. Press F5 (at least with Firefox and IE this is the Reload-Page button) and stop the time until the page finished to refresh. Repeat this several times to get a good mean value. Now stop opennms, change the heap size as described above, restart opennms and wait for about 10 minutes to let it settle down after starting. Repeat the measurements then increase the heap size again as described above. You will get a table like
heap refresh time
1536 5-7 sec.
2048 3-4 sec.
3072 1-2 sec.
Watch out for memory and swap on your system (by example using top) and decide which value to keep in the config file.
To speed up the start phase of the java virtual machine you might want to add
ADDITIONAL_MANAGER_OPTIONS="-Xms"$JAVA_HEAP_SIZE"m
though speeding up the startup time in most cases is not a big problem and the parameter sometimes doesn't help at all.
[edit] Tuning the maximum Permanent Generation size
If you're seeing messages in your logs containing a mention of:
java.lang.OutOfMemoryError: PermGen space
Then you probably need to allocate more memory to the garbage collector's permanent generation. This section of JVM memory is allocated separately from the heap, and its default maximum size varies according to the platform on which the JVM is running. The OpenNMS 1.8 start script on UNIX and Linux platforms sets the maximum size to 128MB, but you can adjust this value in $OPENNMS_HOME/etc/opennms.conf. For example:
ADDITIONAL_MANAGER_OPTIONS="-XX:MaxPermSize=192m"
[edit] Tuning garbage collection
If you have a system with a lot of cores and threads like sun's niagara cpu you might run into a problem known as "Amdahl's Law", see http://en.wikipedia.org/wiki/Amdahl%27s_law. You can try to optimize garbage collection using different garbage collectors, see http://java.sun.com/docs/hotspot/gc1.4.2/#3.%20Sizing%20the%20Generations|outline.
Using
ADDITIONAL_MANAGER_OPTIONS="-XX:+UseParallelGC \
-verbose:gc \
-XX:+PrintGCDetails \
-XX:+PrintTenuringDistribution \
-XX:+PrintGCTimeStamps"
you will get a lot of time information about garbage collection in the output.log of opennms. The default garbage collector used by opennms is incgc (e.g. -XX:+incgc), others to try are ConcMarkSweepGC (-XX:+UseConcMarkSweepGC) and the ParallelGC (-XX:+UseParallelGC) which might be the best if you have a lot of cores/threads. If you have settled down you configuration remove the lines containing verbose and Print from the options:
ADDITIONAL_MANAGER_OPTIONS="-Xms"$JAVA_HEAP_SIZE"m -XX:+UseParallelGC"
[edit] Parallel thread library on Solaris systems
It is also useful to use libumem instead of standard IO libraries on Solaris 10. If you want to enable libumem on an existing application, you can use the LD_PRELOAD environment variable (or LD_PRELOAD_64 for 64 bit applications) to interpose the library on the application and cause it to use the malloc() family of functions from libumem instead of libc.

LD_PRELOAD=libumem.so opennms start
LD_PRELOAD_64=libumem.so opennms start
 To confirm that you are using libumem, you can use the pldd(1) command to list the dynamic libraries being used by your application. For example:
$ pgrep -l opennms
2239 opennms
$ pldd 2239
2239:    opennms
/lib/libumem.so.1
/usr/lib/libc/libc_hwcap2.so.1
[edit] OpenNMS
[edit] Logging
By default the daemons log at WARN and webapp log at DEBUG level. This causes a lot of extra disk I/O. You can reduce the logging substantially by setting the level to WARN in /opt/opennms/etc/log4j.properties and /opt/opennms/webapps/opennms/WEB-INF/log4j.properties. Just add this line:
   log4j.threshold=WARN
There is also /opt/opennms/jetty-webapps/opennms/WEB-INF/log4j.properties, but even though this file is read on startup, it seems not to matter; I didn't need to modify it.
After restarting, you should no longer see messages labelled DEBUG or INFO in /opt/opennms/logs/daemon/* and /opt/opennms/logs/webapp/*, except for the startup log (/opt/opennms/logs/daemon/output.log).
[edit] Data Collection
High disk I/O load due to data collection is the major reason for performance problems in many OpenNMS systems. Hardware and filesystem layout as described above helps a lot.
Another approach is to omit all unnessecary data collections.
[edit] Don't collect what you don't need
While the "default" snmp-collection definitions in datacollection-config.xml provide an easy-to-go data collection definition for small network systems in larger environements it's undesireable to collect everything that can be collected. Probably in those environements a better approach would be to NOT use default data collection but to start with defining packages in collectd-configuration.xml and corresponding snmp-collections in datacollection-config.xml to ensure only those values are collected you really care about. See Docu-overview/Data Collection for details.
[edit] Don't try to collect what you don't get
If you try to collect a lot of data from nodes which don't provide those values you will get a lot of threads waiting for timeouts or getting errors. If you have specific nodes with problems look in your $OPENNMS_HOME/share/rrd/snmp/[nodeid] directory for the node(s) in question and note all the mib objects that are actually being collected.
Another possibility is to change the logging for collectd from WARN to DEBUG:
$OPENNMS_HOME/etc/log4j.properties:
# Collectd
log4j.category.OpenNMS.Collectd=DEBUG, COLLECTD
and then fgrep for "node[your_nodeid]" in collectd.log.
There you should see which variables OpenNMS tries to collect and which variables are successfully collected. The successful ones normally end up in the jRRD files, all others defined in data-collection for this [type of] node can't be collected for some reason.
If there are too many unsuccessful tries change your datacollection-config.xml. You may omit those values for all devices or create new collection groups that contain only those mib objects the node(s) provide values for. Add a systemDef for your node(s) providing the the same values. In collectd-configuration.xml define a separate package for your node and reference the snmp-collection you just created in datacollection-config.xml. Make sure the node is only in this one package. This gives you an environment to work in that is free of any extra clutter and avoids requesting extraneous mib objects that you won't get a response for. Then experiment with different values for max-vars-per-pdu, timeout and also snmp v1 or v2c.
Don't forget to change back logging to WARN once you have finished debugging.
[edit] RRDTool/JRobin
Writing all the snmp-collected data and the results from polling the service (response times) to rrd files produces a lot of disk I/O, so look for disk tuning below. For further tuning see the fundamentals and some more detailed pages like
	•	RRD performance fundamentals
	•	RRD_store_by_group_feature
	•	Queueing_RRD
[edit] Tomcat (if not using built-in Jetty server)
Note that there's no need to use Tomcat since OpenNMS version 1.3.7 unless you have a specific requirement that the built-in Jetty server in OpenNMS cannot meet.
If not already done at installation time; To allow Tomcat to access more memory than the default. The easiest way to do this is via the CATALINA_OPTS environment variable. If the Tomcat software being used has a configuration file as above, it can be added to that file. Otherwise it is best just to add it to catalina.sh. CATALINA_OPTS="-Xmx1024m"
 The -Xmx option allows Tomcat to access up to 1GB of memory. Of course, the assumes that there is 1GB of available memory on the system. It will need to be tuned to the particular server in use.
[edit] OpenNMS daemon
[edit] OpenNMS webapp
[edit] Capsd service discovery / rescan
If discovery or rescanning of a node takes a long time, you can turn up the maximum number of threads for initial discovery of services (max-suspect-thread-pool-size) or rescans (max-rescan-thread-pool-size) at the top of capsd-configuration.xml.
Change logging for capsd in log4j.properties from WARN to DEBUG and check the capsd.log file for the number after "Pool-fibern". If n is most of the time the same as the maximum number of threads configured you should increase the maximum number of threads. Most servers will easily handle 50 threads or even more as the threads are most of the time waiting for services that don't answer. Don't forget to change logging back to WARN.
Capsd will check every service defined in capsd-configuration.xml for every interface of the device during a rescan. For every service you can define the number of retries and the timeout value. If you have a device with a lot (hundred) of interfaces and the default capsd configuration it has to check about 30 services (default for opennms 1.6.x) for every interface. If the interfaces are just "ip interfaces" with no other service like dns, dhcp, http etc. you have about 30 services to time out for every interface, and probably there are retries, too.
To get an estimate of the time this needs take
time = number of interfaces * number of services * ((number of retries)+1) * (timeout value/1000) 
Note: timeout is defined in milliseconds!
By example
time = 100 [interfaces] * 30 [services] * (1 [retry] +1) *(2000 [timeout in ms]/1000)
     = 12.000 seconds
     = 200 min.
     = 3.3 hours
Try to reduce the ip-ranges, the number of services to check, the timeout- and retry-values to something reasonable for your environment.
[edit] Poller threads
If you have good hardware and find your pollers are not completing in time, you can turn up the maximum number of poller threads at the top of poller-configuration.xml.
To find out how many threads are actually being used, make sure DEBUG level logging is enabled for daemon/poller.log, then run:
   $ tail -f poller.log | egrep 'PollerScheduler.*adjust:'
   ...
   2007-09-05 10:30:32,755 DEBUG [PollerScheduler-45 Pool] RunnableConsumerThreadPool$SizingFifoQueue:
       adjust: started fiber PollerScheduler-45 Pool-fiber2 ratio = 1.0227273, alive = 44
   
   ...
   
   2007-09-05 10:30:12,783 DEBUG [PollerScheduler-45 Pool-fiber29] RunnableConsumerThreadPool$SizingFifoQueue:
       adjust: calling stop on fiber PollerScheduler-45 Pool-fiber3
Watch the output for a while after startup. The "alive" count shows the number of active poller threads (minus one -- the new thread isn't counted). If the number of threads is continually pegged at the maximum (default 30), you might want to add more threads.
[edit] Event Handling
All incoming events have to be checked against the configured events to classify them and to handle the parameters correctly. There are a lot of predefined events in opennms. Incoming events are compared to the list of configured events until the first match is found. If you have a lot of incoming events you might consider to make the following changes in $OPENNMS_HOME/etc/eventconf.xml
	•	comment out vendor events that you don't need
	•	put the vendor events that make most of your incoming events on top of the list
	•	Take care that Standard, default and programmatic events keep their place at the end of the list.
As there probably are a lot of events hitting the Standard- or default-events configured at the end of the list resorting the event list won't help as much as commenting out.
[edit] Event Archiving
In the OpenNMS "contrib" directory, we have a small script for helping performance by archiving events into a historical event table and updating the references to the archived event to an event place holder.
You can download the latest version of the script here.
It is recommended that you run this script by passing in a timestamp argument such that you archive one day's worth of events beginning with the oldest day up to the point you want to keep live events (default is 9 weeks). Then run this script without a timestamp parameter, from cron as often as you like from there out.
./maint_events.sh "2008/01/01"
To analyze why your event table is so large, have a look at Event_Maintenance.
</Text>
        </Document>
        <Document ID="69">
            <Title>Surveillance Views</Title>
        </Document>
        <Document ID="122">
            <Title>Security Considerations</Title>
        </Document>
        <Document ID="118">
            <Title>OpenNMS HA</Title>
            <Text>Making OpenNMS highly available
This page describes how to use OpenNMS in an HA cluster consisting of pacemaker and corosync. The example will also use DRBD, but that is an optional component used because the author does not have access to a shared storage device.
[edit] Motivation
Why would you want to do this? Mainly because you do not want to sit there, being called by your customers or your boss about something being down just because some component of OpenNMS (be it Hardware, Software, Database related) failed and did not automatically recover and therefore not notify you about the problem.
The basic need is the need to monitor the monitoring system. With this How-To, you can make it highly available instead of just monitoring it.
[edit] Abstract and terms
An HA cluster is a group of computers, nodes, providing a service. A service is typically formed by several resources. A resource is managed by a resource agent, which in most cases is basically a shell script. A resource agent is the interface for the cluster resource manager to manage the resource.
The most widely used cluster configuration is the 2-node-cluster, which is - well - a cluster formed by 2 nodes. And with 2-node-clusters, the most common configuration is to have one active node, that runs your service, and one passive node, that will jump into place if the first one should fail.
[edit] Pacemaker abstract
In pacemaker, you configure resources and rules (constraints). You basically say "Okay, so I have a webserver, a database and an IP address. I want the webserver and the IP address to run on node cliff, the database to run on node jason. If not possible otherwise, they may also run on the same node. Oh, oh, and if the network connection of a node should fail, please move everything away from that node to one with a working connection."
What pacemaker basically does is to execute shell scripts for your resources and evaluate the return code. If the return code does not match the expected return code, pacemaker computes a path of actions that have to be taken in order to meet the policy you configured. Let's take OpenNMS and its underlying PostgreSQL database as an example: If the database should die, it is quite likely that OpenNMS also needs a restart after PostgreSQL has been restarted. So the path would be:
	1.	stop opennms
	2.	stop postgresql (in order to make sure it is shutdown safely)
	3.	start postgres
	4.	start opennms
In order to start a resource, it will execute the configured resource agent with the parameter "start". It will stop a resource by calling the script with the "stop" parameter and in order to monitor it - well - it will call it with the "monitor" parameter.
There are two cases in which pacemaker needs help of a hardware device
	1.	A node does not respond
	2.	A stop operation failed
In these situations, pacemaker has no way of knowing the current state of the node. Therefore it powercycles the node and tries to start the resources elsewhere. If this sounds a bit harsh to you, read #stonith.
[edit] Prerequisites
In order to run OpenNMS in an HA cluster, you need to meet certain prerequisites.
Hardware Requirements

	1.	You need at least 2 nodes
	2.	Each node needs at least 2 NICs
	3.	You need some sort of shared storage for the configuration- and datacollection files (if you do not have a shared storage device, you can use #DRBD)
	4.	It is strongly encouraged to run the cluster nodes on managable power supplies (see #stonith)
Software Requirements

	1.	Corosync or heartbeat (corosync in this How-To)
	2.	pacemaker
	3.	Make sure OpenNMS' init-script is LSB compliant (see Bug 3457)
Corosync and heartbeat (you need to choose one) will deal with node level failure and establish the communication channels between the nodes, pacemaker will deal with resource level failure and resource management.
[edit] DRBD
DRBD is an open source project primarilly run by Linbit (Vienna/Austria).
If you do not have a shared storage device, you can use DRBD in order to have your configuration- and datacollection files accessible on both nodes. DRBD is basically a RAID-1 over ethernet. It will sync everything you write to your active node over to your passive node in real time. At each point in time, it will ensure that the data on the active node is identical to the data on the passive node.
It is completely oblivious to applications, which makes it an affordable way to basically have "everything" highly available. It sits between the Application and the lower level storage device, which is illustrated in the following pictures.
#
1. The application writes to the DRBD device
￼
2. DRBD writes to the lower level storage and sends data over to the peer
￼
3. The peer receives the data and writes to its lower level device
￼
4. The lower level storage (hopefully) acknowledges the write
￼
5. The peer acknowledges the write
￼
6. DRBD acknowledges the write to the application
DRBD resources always are in one of two roles: Primary or secondary. Only a resource in the primary role is writable. The secondary role is not even readable.
It has a couple of really nice features like the online data verification, with which you can make sure the data on both nodes is identical without taking the data offline.
Another great feature is the data digest algorythm. It will make sure the data that was sent from memory is actually the same as the data that was sent over the wire. With bad NICs/drivers it may happen that bits switch between your memory and wire. That would corrupt your replicated data. DRBD can detect and recover from this.
Additional information on DRBD is available in #Documentation
[edit] IP addresses
One thing you have to think about when it comes to clustering OpenNMS is the nodes' IP addresses. Each node will obviously have its own "physical" IP address. If you'd just start OpenNMS on another node without also moving the IP address, you're likely to run into problems, most likely SNMP-based.
SNMP-agents are typically configured to send traps to one destination and they also are very often set up to just allow access from one source address. So if you just moved OpenNMS to another machine (that has another IP address), it is very likely that you will not receive traps and that you will not be able to perform SNMP data collection from that node.
Therefore, the cluster needs to be configured with a "floating" (some say "virtual") IP address. Moving this over to the other node alongside with OpenNMS will make sure you will receive traps on that node. But it will not necessarily also allow you to query SNMP. By default, additional IP addresses are not used for outgoing connections. The source IP of outgoing connections is usually the "physical" (or "primary") IP address. So the cluster also needs to be configured with a rule that makes outgoing connections use the floating IP.
[edit] Install software
The software usually ships with most of the linux distributions. The current versions of OpenSuSE, RedHat and Ubuntu have it. If you are having trouble finding packages, have a look at the clusterlabs install page.
[edit] Actual configuration
[edit] Starting situation
We have a node "cliff", which is our current OpenNMS host. Its OpenNMS is living in /opt/opennms, its PostgreSQL is living in /opt/postgres, it has /dev/sda3 mounted to /opt with ~80 GB of space and it is configured with IP address 10.2.50.106. SNMP-agents allow connections from this IP and send traps to this IP only.
We do not have a shared storage device.
￼
￼
[edit] Target situation
￼
￼
We have a new node "jason". The data is being replicated using DRBD. If OpenNMS cannot run on cliff, it shall be started on jason. Traps and SNMP GET requests must also work on jason.
[edit] First steps
First of all, you need to configure a new primary IP address for cliff. How that is done depends on your distribution. On my OpenSuSE linux, I needed to edit /etc/sysconfig/network/ifcfg-br0 (which is cliffs default network connection) and then run "rcnetwork restart".
The next step is to set up the second machine. I'd recommend using the same hardware platform, the same operating system and version and the same partitioning layout.
[edit] Prepare lower level storage for DRBD
/dev/sda3 has 80633 1M blocks.
  # df -m
  Filesystem           1M-blocks   Used Available Use% Mounted on
  /dev/sda1                40313  33239      5026 87% /
  /dev/sda3                80633  41412     35125 55% /opt
In order for DRBD to write its meta-data on that device, we need to shrink the file system. As of this writing, DRBD will not need more than 128 MB of space for its meta-data, so this example actually is a bit too generous.
  # fsck -f /dev/sda3
  # resize2fs -p /dev/sda3 80000M
[edit] Configure DRBD
vi /etc/drbd.conf

resource opennms {
        device          /dev/drbd0;
        disk            /dev/sda3;
        meta-disk       internal;
        on cliff {
                address 10.250.250.104:7788;
        }
        on jason {
                address 10.250.250.105:7788;
        }
}
You configure a name for the resource (opennms), a name of the drbd device (drbd0), the device file of the lower level storage (/dev/sda3), where the meta-data should live (internal) and the connection information for the drbd replication connections.
Put this file on both machines, then
modprobe drbd
The next step is to create the meta-data on the device. On cliff, drbd will warn you that it found an existing file system and that your operation might destroy data.
drbdadm create-md opennms
warning:
Found ext3 filesystem which uses 81920000 kB
current configuration leaves usable 83880800 kB
 ==> This might destroy existing data! &lt;==
Do you want to proceed?
[need to type 'yes' to confirm] yes

As long as the number after "current configuration leaves usable" is larger than the one above, you should be fine and type "yes" at the prompt. Otherwise you should not continue here since that would destroy your filesystem. Look into #Prepare lower level storage for DRBD in order to sort this out or use external meta-data.
After you have created the meta-data (on both machines!), you can bring up the resource using the command
drbdadm up opennms
This also needs to be done on each node.
You can then look at DRBD's proc interface
cat /proc/drbd
version: 8.3.7 (api:88/proto:86-91)
GIT-hash: 61b7f4c2fc34fe3d2acf7be6bcc1fc2684708a7d build by root@cliff, 2010-04-26
08:46:57
 0: cs:Connected st:Secondary/Secondary ds:Inconsistent/Inconsistent C r---
    ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:d oos:83880800
In cs you see the connection state. st tells you about the roles of the drbd peers. In ds you will find the current data status. In this situation, each peer is secondary and the data is inconsistent. This is expected as you have not yet told DRBD which side has the "good" data.
Doing this is the next step. Make absolutely sure you do this on the correct machine (cliff in this case). Otherwise you overwrite your data.
drbdadm -- --overwrite-data-of-peer primary opennms
If you then have a look at the proc interface again, you will see something like this:
cat /proc/drbd
version: 8.3.7 (api:88/proto:86-91)
GIT-hash: 61b7f4c2fc34fe3d2acf7be6bcc1fc2684708a7d build by root@cliff, 2010-04-26
08:46:57
 0: cs:SyncSource st:Primary/Secondary ds:UpToDate/Inconsistent C r---
    ns:36864 nr:0 dw:0 dr:37128 al:0 bm:2 lo:0 pe:0 ua:0 ap:0 ep:1 wo:d oos:83843936
[>....................] sync'ed: 0.1% (81878/81914)M
finish: 0:37:33 speed: 36,864 (36,864) K/sec
The local side is now the SyncSource in the Primary role and its data is UpToDate. The peer is in Secondary role and its data is still Inconsistent, since the replication has not finished yet.
On cliff, you can now continue to build your cluster, you don't have to wait for DRBD to finish the initial replication.
ip addr add 10.2.50.106/24 dev br0
ip route replace default via 10.2.50.11 src 10.2.50.106
/etc/init.d/postgres start
/etc/init.d/opennms start
This configures the formerly primary IP address of cliff as a secondary address and uses this for outgoing traffic, too. Then PostgreSQL and OpenNMS is started.
Now you need to make sure things still work. Send some traps, see if data is collected, etc. Once you are confident things still work, shut down OpenNMS, PostgreSQL, unmount the file system, bring down the secondary IP and the demote the DRBD device to the secondary role. Then, repeat these steps on jason and make sure things also work there.
drbdadm primary opennms
mount /dev/drbd0 /opt
ip addr add 10.2.50.106/24 dev br0
ip route replace default via 10.2.50.11 src 10.2.50.106
/etc/init.d/postgres start
/etc/init.d/opennms start
At this point, you can already do manual failover. So in case the cliff node should die, you could manually start OpenNMS just as shown above.
[edit] Configure corosync
You need to generate an authentication key for corosync. That's easily done with the command
corosync-keygen
Then you need to edit /etc/corosync/corosync.conf. It comes with sane default values, you just need to adopt your network configuration:
      interface {
              ringnumber: 0
              bindnetaddr: 10.250.250.0
              mcastaddr: 226.94.1.1
              mcastport: 5405
      }
and at the end of the file, tell corosync to spawn pacemaker.
 service {
 	# Load the Pacemaker Cluster Resource Manager
 	name: pacemaker
 	ver:  0
 }
Apply these changes on both nodes.
For a complete sample configuration file, look at the clusterlabs site
You should now be able to start corosync using its init-script. Remember to do this on both nodes.
/etc/init.d/corosync start
[edit] Configuring pacemaker
In this situation, the output of the cluster resource manager monitor crm_mon should look something like this:
# crm_mon -rf1
============
Last updated: Mon Mar 8 13:48:53 2010
Stack: openais
Current DC: jason - partition with quorum
Version: 1.0.7-d3fa20fc76c7947d6de66db7e52526dc6bd7d782
2 Nodes configured, 2 expected votes
0 Resources configured.
============
Online: [ cliff jason ]
Full list of resources:
Migration summary:
* Node cliff:
* Node jason:
We have two online nodes and - as of now - zero resources.
pacemaker is configured using the crm shell. You enter the configure mode with the command
crm configure
and after you have added or modified the configuration, you need to issue the commit command in order to save the changes.
The first thing to do in a 2-node-cluster is to configure pacemaker to ignore quorum. Quorum is the idea of having only a defined set of nodes providing the service in case of a cluster split. Say there is a problem in your network and your 5 node cluster is divided into partitions of 2 and 3 nodes. It would be bad if both partitions would now start the service. Quorum is something a cluster partition has if it is larger than 50% of the cluster. If a partition has quorum, it may run resources. Otherwise nodes have to shut down every resources they might have run at that time. This is another safety mechanism.
In a 2-node-cluster however, there is no such thing as >50% of the cluster if the nodes cannot see each other. Each node is 50% of the cluster and can never be more than that. So in order to allow a node that does not see the other node (which is likely to happen in case of a failure), you need to configure
property no-quorum-policy="ignore"
in crm configure mode.
Then you can configure DRBD as a resource for pacemaker:
#  Configure the DRBD resource
 primitive drbd-opennms ocf:linbit:drbd \
  params drbd_resource="opennms" \
  op monitor interval="15s

#  Then make it a multistate resource that runs as a master on one, as a slave on the other node
 ms ms-opennms drbd-opennms \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" \
  notify="true" globally-unique="false"
The cluster resource manager monitor will now look something like this (I cut the output header here)
# crm_mon -rf1
Online: [ cliff jason ]
Full list of resources:
 Master/Slave Set: ms-opennms
     Masters: [ cliff ]
     Slaves: [ jason ]
Migration summary:
* Node cliff:
* Node jason:
The next step is to configure the dependencies for OpenNMS:
# IP address
 primitive ipaddress ocf:heartbeat:IPaddr2 \
  params ip=10.2.50.106 cidr_netmask=24 nic=br0 \
  op monitor interval="60s" timeout="40s"

# Filesystem
 primitive fs-opt ocf:heartbeat:Filesystem \
  params device="/dev/drbd0" directory="/opt" fstype="ext3" \
  op monitor interval="60s" timeout="40s"

# Postgres DB
 primitive postgres ocf:heartbeat:pgsql \
  params pgctl="/usr/bin/pg_ctl" pgdata="/opt/postgres/data" \
  logfile="/opt/postgres/data/logfile" \
  op monitor interval="60s" timeout="40s"

# Source IP rule
 primitive srcipaddress ocf:heartbeat:IPsrcaddr \
  params ipaddress="10.2.50.106" \
  op monitor interval="60s" timeout="40s"

# Group the dependencies
 group dependencies ipaddress fs-opt postgres srcipaddress

# Run group where the DRBD Master runs
 colocation dependencies-on-ms-opennms-master inf: dependencies ms-opennms:Master

# Start group after you promoted DRBD to Master mode
 order ms-opennms-promote-before-dependencies-start inf: ms-opennms:promote \
  dependencies:start
The monitor should now look something like this:
# crm_mon -rf1
Online: [ cliff jason ]
Full list of resources:
 Master/Slave Set: ms-opennms
     Masters: [ cliff ]
     Slaves: [ jason ]
 Resource Group: dependencies
     ipaddress (ocf::heartbeat:IPaddr2):        Started cliff
     fs-opt     (ocf::heartbeat:Filesystem):    Started cliff
     postgres (ocf::heartbeat:pgsql): Started cliff
     srcipaddress       (ocf::heartbeat:IPsrcaddr):     Started cliff
Migration summary:
* Node cliff:
* Node jason:
So finally, you can configure OpenNMS as a resource in pacemaker:
# OpenNMS
 primitive opennms lsb:opennms \
  op start timeout=300s \
  op stop timeout=120s \
  op monitor interval=60s timeout=40s

# Run OpenNMS on the Master
  colocation opennms-on-ms-opennms-master inf: opennms ms-opennms:Master

# Start it after the dependencies
 order dependencies-start-before-opennms-start inf: dependencies:start \
  opennms:start
The monitor will reflect the changes like this:
# crm_mon -rf1
Online: [ cliff jason ]
Full list of resources:
 Master/Slave Set: ms-opennms
     Masters: [ cliff ]
     Slaves: [ jason ]
 Resource Group: dependencies
     ipaddress (ocf::heartbeat:IPaddr2):         Started cliff
     fs-opt      (ocf::heartbeat:Filesystem):    Started cliff
     postgres (ocf::heartbeat:pgsql): Started cliff
     srcipaddress        (ocf::heartbeat:IPsrcaddr):     Started cliff
opennms     (lsb:opennms): Started cliff
Migration summary:
* Node cliff:
* Node jason:
You're almost there. What's left is to configure network connection checking. This is done by a cluster resource named the ping daemon. It continuously sends icmp echo requests to a configurable list of nodes and by the number of nodes it receives a respond from, it sets a node attribute. Then you can set rules that look at the value of this attribute in order to allow or forbid placement of resources to a node.
# Run a “ping” process to a couple of IP addresses in order to tell whether the network connection is working
 primitive pingd ocf:pacemaker:pingd \
  params host_list="10.2.50.12 10.2.50.11 10.2.50.19 10.2.50.40" \
  op monitor interval=60s timeout=40s

# Run this ping process on all nodes (clone it)
  clone cl-pingd pingd

# Locate the Master role of DRBD to a node where the ping attribute is >0
 location ms-opennms-master-connected ms-opennms \
  rule $id="ms-opennms-master-connected-rule-1" \
  $role="Master" -inf: not_defined pingd or pingd lte 0
Since I configured 4 ping nodes, the attribute "pingd" has a value of 4 on both nodes now (see th last 2 lines):
# crm_mon -rf1
Online: [ cliff jason ]
Full list of resources:
 Master/Slave Set: ms-opennms
     Masters: [ cliff ]
     Slaves: [ jason ]
 Resource Group: dependencies
     ipaddress (ocf::heartbeat:IPaddr2):         Started cliff
     fs-opt      (ocf::heartbeat:Filesystem):    Started cliff
     postgres (ocf::heartbeat:pgsql): Started cliff
     srcipaddress        (ocf::heartbeat:IPsrcaddr):     Started cliff
opennms     (lsb:opennms): Started cliff
 Clone Set: cl-pingd
     Started: [ cliff jason ]
apcstonith-cliff (stonith:apcmastersnmp): Started jason
apcstonith-jason (stonith:apcmastersnmp): Started cliff
Migration summary:
* Node cliff: pingd=4
* Node jason: pingd=4
The last thing you need to do is to configure your stonith devices. While this is hardware dependent, my example will likely not help you, but i'll post it for completeness:
# Configure the stonith devices (depends on your hardware)
 primitive apcstonith-cliff stonith:apcmastersnmp params community="testcom"
  port="161" ipaddr="10.2.50.154" op monitor interval="3600" timeout="120" op
  start requires="nothing"
 primitive apcstonith-jason stonith:apcmastersnmp params community="testcom"
  port="161" ipaddr="10.2.50.155" op monitor interval="3600" timeout="120" op
  start requires="nothing"
[edit] Start testing
Now you can start testing your cluster.
Pull network cables, pull power plugs, kill processes, shutdown switches. The more you test, the better you will understand the cluster and what it will do in which situation.
[edit] Documentation
http://www.clusterlabs.org
http://www.drbd.org
[edit] Support
Commercial support for pacemaker, corosync and drbd is available from Linbit, Novell and RedHat (this list is alphabetically sorted and incomplete!).
[edit] stonith
A question people tend to ask when they hear about pacemaker powercycling nodes in case of certain failures is "is this really necessary". And I'd just like to ask a question in return: How do you know what is going on on a node that you cannot login to? Say the node is not responding to ping, it is not acception an SSH connection.
The only thing you can do here is to assume what is going on there.
	•	Maybe it is still running "something"
	•	Maybe it is still using the shared data
	•	Maybe it really does not do anything
But there is no way to know what is really happening there.
The same applies for a resource that has just failed to stop. What is the cluster supposed to do about this? Just pretend it worked and probably damage your data by starting the resource elsewhere resulting in concurrent access? Try "stop" again?
In my - and in the one of many cluster developers - opinion, there is no way of knowing what is going on there. If you know a way - let us know!
So the cluster needs a mechanism that makes sure this node does not access the shared data before it starts the service anywhere else. Having concurrent access on shared data (like an ext3 filesystem for example) might otherwise blow things to bits within seconds. This is a mechanism called Fencing. Some clusters implement this by managing (turning off) switch or SAN ports, pacemaker implements it by powercycling a node.
If you pull the power plug, the machine definitely does not do anything anymore and you can safely start using the data on another node. It turns the assumption "the node is dead" into a fact.
The cluster component doing this is called "STONITH", which is short for "shoot the other node in the head".
This will protect your data. You can disable it, but you are strongly encouraged not to.
</Text>
        </Document>
        <Document ID="74">
            <Title>Jetty and HTTPS</Title>
            <Text>ntroduction
If users will be accessing the OpenNMS web UI across untrusted networks, it is desirable to protect web sessions using HTTPS. This article explains how to configure OpenNMS' built-in Jetty web server to support HTTPS with no dependencies on external software.
[edit] For the Impatient
These instructions are not appropriate for production environments
If you want only to see that this feature works, and are not yet concerned with configuring HTTPS for the web UI in a production environment, complete the following steps. You will end up with an HTTPS-enabled version of the OpenNMS web UI that presents a completely bogus SSL certificate.
	•	Stop OpenNMS
OPENNMS_HOME/bin/opennms stop
	•	Set an HTTPS port in the top-level OpenNMS properties file
	◦	Open OPENNMS_HOME/etc/opennms.properties in your favorite editor
	◦	Uncomment the following line:
org.opennms.netmgt.jetty.https-port = 8443
	•	Start OpenNMS
OPENNMS_HOME/bin/opennms start
	•	In your web browser, visit the following URL: https://127.0.0.1:8443/opennms/. Your browser will warn you that the server's certificate cannot be verified because it is expired and is issued by an untrusted authority. If you opt to continue anyway, you will be presented with the OpenNMS login page.
[edit] For the Patient
The following instructions explain how to set up Jetty as a standalone HTTPS server so that it is suitable for use in production environments. You will end up with an HTTPS-enabled version of the OpenNMS web UI that presents an SSL certificate customized for your environment. The path of this section branches to allow you to choose whether to obtain an SSL certificate signed by a trusted certifying authority or to make do with a self-signed SSL certificate.
[edit] Create a new Java keystore
Using the keytool utility that ships with Sun's Java distributions, create a new keystore and populate it with a new key. For the first question ("What is your first and last name"), enter the fully-qualified domain name by which people will be accessing your OpenNMS server's web UI. Choose this name correctly, as you will have to start over if you ever need to change it. Answer the remaining questions according to the specifics of your organization and locality.
Be sure to specify an appropriate number of days for the validity parameter. After this number of days elapses, the key you are generating will expire and you may no longer be able to use it to create new certificates. The example below specifies 731 days, which will make the key valid for two years (accounting for a possible leap year).
It is important that you choose good passwords for the keystore and for the key itself. These passwords may be the same or different to each other. Using different and strong passwords here protects your server's private key in the event the keystore file falls into the wrong hands. You should take precautions to keep this from happening, including setting filesystem user and group permissions so that unauthorized individuals with accounts on the OpenNMS server will not have read (or write) access to the keystore.
By default, keytool will create DSA keys. There are reports that Jetty does not work well with DSA keys. The solution is to use an RSA key by adding
-keyalg RSA
to the command below.
$ keytool -genkey -validity 731 -keystore /tmp/propercert/proper.keystore 
Enter keystore password:  aGoodStrongKeystorePassword
What is your first and last name?
  [Unknown]:  opennms.example.org
What is the name of your organizational unit?
  [Unknown]:  Network Management Division
What is the name of your organization?
  [Unknown]:  The Example Organization
What is the name of your City or Locality?
  [Unknown]:  Marina del Rey
What is the name of your State or Province?
  [Unknown]:  California
What is the two-letter country code for this unit?
  [Unknown]:  US
Is CN=opennms.example.org, OU=Network Management Division, O=The Example Organization, L=Marina del Rey, ST=California, C=US correct?
  [no]:  yes
[edit] Option A: Certificate signed by a trusted CA
Generate a certificate signing request (or CSR) from the key that you created above. The command to do this is straightforward. The filename you specify for the file parameter will contain the CSR after the command completes.
$ keytool -certreq -keystore /tmp/propercert/proper.keystore -file /tmp/propercert/proper.csr
Enter keystore password:  aGoodStrongKeystorePassword
Enter key password for &lt;mykey>anotherGoodStrongPassword
The CSR output file is a text file whose contents will look similar to the following:
-----BEGIN NEW CERTIFICATE REQUEST-----
MIICjjCCAkwCAQAwgYkxCzAJBgNVBAYTAlVTMRcwFQYDVQQIEw5Ob3J0aCBDYXJvbGluYTESMBAG
A1UEBxMJUGl0dHNib3JvMSAwHgYDVQQKExdUaGUgT3Blbk5NUyBHcm91cCwgSW5jLjEQMA4GA1UE
CxMHVW5rbm93bjEZMBcGA1UEAxMQZGVtby5vcGVubm1zLm9yZzCCAbcwggEsBgcqhkjOOAQBMIIB
HwKBgQD9f1OBHXUSKVLfSpwu7OTn9hG3UjzvRADDHj+AtlEmaUVdQCJR+1k9jVj6v8X1ujD2y5tV
bNeBO4AdNG/yZmC3a5lQpaSfn+gEexAiwk+7qdf+t8Yb+DtX58aophUPBPuD9tPFHsMCNVQTWhaR
MvZ1864rYdcq7/IiAxmd0UgBxwIVAJdgUI8VIwvMspK5gqLrhAvwWBz1AoGBAPfhoIXWmz3ey7yr
XDa4V7l5lK+7+jrqgvlXTAs9B4JnUVlXjrrUWU/mcQcQgYC0SRZxI+hMKBYTt88JMozIpuE8FnqL
VHyNKOCjrh4rs6Z1kW6jfwv6ITVi8ftiegEkO8yk8b6oUZCJqIPf4VrlnwaSi2ZegHtVJWQBTDv+
z0kqA4GEAAKBgHQVrC0ysQbtmvu3Btjsz1n+6MJIm6mB0Y28fzKFC/azpMB+hultKdUFsnnb13BN
fnfuUoULeLTu/cGvAsqFtCpJuAmcAzXxmTp0BTYj4o8jJYi0dLIKnox3Shy4VTr+qJlzn9Y1auWy
rpwdD03e6Kq32rKpgU5fwC53L0J8dI6yoAAwCwYHKoZIzjgEAwUAAy8AMCwCFFtHtT6XNS8A/0Xu
OfVnTnL+zrTPAhR7QgO0Y3Sd0u39l3uOvhX3G5//zw==
-----END NEW CERTIFICATE REQUEST-----
There is no particular need to protect the CSR, as it contains no data that would aid an attacker.
[edit] Submit the CSR to the trusted CA
Send the certificate signing request (CSR) that you just generated to a certificate authority (CA) that you trust. This could be a commercial CA whose signatures most or all modern browsers will honor (for discussion, see this Wikipedia article) or a CA that is internal to your organization and whose public key is installed into all browsers on your organization's computers. The specifics of this step are entirely outside the scope of this article.
[edit] Import the signed certificate
After you submit the certificate signing request to the CA, and fulfill the CA's requirements for validating your identity and trustworthiness, the CA will send you a certificate file that bears the CA's signature.
The signed certificate consists of the public half of the key that you generated in the first step plus a signature performed using the CA's private key. Therefore, the signed certificate (like the CSR) contains no information that will aid an attacker if it is disclosed.
In order for your OpenNMS server to use the signed certificate, you must import the signed certificate into the keystore that you created in the first step.
$ keytool -import -keystore /tmp/propercert/proper.keystore -file /path/to/signed-cert.txt 
Enter keystore password:  aGoodStrongKeystorePassword
Note: This requires that your keystore already trust the CA that has signed your certificate. If you see the error "keytool error: java.lang.Exception: Public keys in reply and keystore don't match", this indicates that you need to import your certificate authority's CA certificate first:
$ keytool -import -trustcacerts -alias &lt;my-CA-root> -keystore /tmp/propercert/proper.keystore -file /path/to/CA-cert.txt
Enter keystore password:  aGoodStrongKeystorePassword
Now retry the import of the server cert.
[edit] Option B: Self-signed certificate
If you are content with a self-signed certificate, you need to perform just one step to add a signature to your new SSL certificate.
As in the key generation process above, be sure that you specify an appropriate number of days for the validity parameter.
$ keytool -selfcert -validity 721 -keystore /tmp/propercert/proper.keystore 
Enter keystore password:  aGoodStrongKeystorePassword
Enter key password for &lt;mykey>anotherGoodStrongPassword
[edit] Copy the keystore into place
Now that the keystore you created contains your server's signed SSL certificate, you must copy the keystore to a place where OpenNMS can find it. In the real world, you might want to place the keystore in a different location, perhaps one that is not included in your nightly backups (unless you trust your backup operator completely). In extreme cases it might be desirable to put the keystore on a filesystem that is unmounted except when starting or restarting OpenNMS. For the purposes of this article, we will assume that you copied the keystore to /opt/opennms/etc/jetty.keystore.
[edit] Configure the Jetty HTTPS parameters in OpenNMS
In your favorite editor, open the file OPENNMS_HOME/etc/opennms.properties.
Uncomment (or add if not present) the line that sets the property org.opennms.netmgt.jetty.https-keystore, and change the value of this property to the location of the keystore that now contains your server's signed SSL certificate:
org.opennms.netmgt.jetty.https-keystore = /opt/opennms/etc/jetty.keystore
Uncomment (or add if not present) the lines that set the properties org.opennms.netmgt.jetty.https-keystorepassword and org.opennms.netmgt.jetty.https-keypassword, and change the values of each property to match the password you used for the keystore and the key itself in the first step:
org.opennms.netmgt.jetty.https-keystorepassword = aGoodStrongKeystorePassword
org.opennms.netmgt.jetty.https-keypassword = anotherGoodStrongPassword
Uncomment (or add if not present) the line that sets the property org.opennms.netmgt.jetty.https-port, and optionally change the value to suit your needs:
org.opennms.netmgt.jetty.https-port = 8443
[edit] Restrict access to the plain-HTTP listener
Although the steps you have completed so far have configured OpenNMS to start a Jetty HTTPS listener on port 8443, they have not disabled the plain HTTP listener that is present by default on port 890. This listener must be present so that the OpenNMS real-time console can update the availability statistics shown in the web UI. Since you have done all the work to enable HTTPS, you probably do not want users using HTTP, so you will need to restrict access to the plain-HTTP listener.
There are two ways to accomplish this task. The first is to tell the plain-HTTP listener to bind only to an interface that is not accessible from any untrusted networks. In a setup where the OpenNMS web UI runs on the same server as the other OpenNMS daemons, it makes sense to use the loopback interface for this purpose. You can restrict the plain-HTTP listener to bind only to the localhost interface (which always has the IP address 127.0.0.1) by uncommenting the line that sets the property org.opennms.netmgt.jetty.host:
org.opennms.netmgt.jetty.host = 127.0.0.1
The second way to restrict access to the plain-HTTP listener is to use firewall rules. These rules may be local to the OpenNMS web UI server (e.g.iptables on Linux or ipf on Solaris) or they may be configured in a discrete firewall external that stands between the OpenNMS web UI server and the rest of the network. Configuring these rules is beyond the scope of this article.
[edit] Restrict access to the HTTPS listener
Although HTTPS is considered secure, there are valid reasons to restrict the interfaces on which the OpenNMS Jetty HTTPS listener is reachable. Currently it is possible to bind the HTTPS listener to all interfaces (the default) or to a single interface. To bind the HTTPS listener to a single interface, uncomment the line that sets the property org.opennms.netmgt.jetty.https-host:
org.opennms.netmgt.jetty.https-host = 10.11.12.13
[edit] Using a Pre-existing Private Key and Certificate
Many users may already have deployed significant amounts of SSL private keys and certs throughout their networks. In many cases, the system on which ONMS is running may already have a private key and certificate. Consequently, it would be a shame to manage yet another set of keys and certs. Unfortunately, the keytool utility does not make it easy to import an existing private key and cert into a Java keystore. Thankfully, this web page summarizes how to do just that.
We summarize here just in case this web page goes away. We assume the private key is in key.pem and the cert is in cert.pem (both are in PEM format).
[edit] Convert the key and cert from PEM format to DER format using openssl command
Use openssl to convert from PEM to DER format.
openssl pkcs8 -topk8 -nocrypt -in key.pem -inform PEM -out key.der -outform DER
openssl x509 -in cert.pem -inform PEM -out cert.der -outform DER
[edit] Put key and cert into a new Java Keystore
Use the ImportKey.java class to take the key and cert and place it in a newly constructed JKS keystore. I modified the ImportKey java source to use the keystore password changeit and to use the key alias importkey and to save the resulting keystore in the file jetty.keystore
java ImportKey key.der cert.der
[edit] Howto: Create and use a certificate signed by the CAcert community
CAcert is a community that offers "free trust" - free signed certificates. If you are interested in using them, read their website. This howto assumes that you already have an account at CAcert and that you use the Debian package of OpenNMS (or: the path to they keystore is like it is in the debian package).
	•	Create a fresh keypair in a fresh keystore
keytool -keyalg RSA -genkey -validity 731 -keystore /usr/share/opennms/etc/jetty.keystore
Note: Use your FQDN when you are asked for first and last name!
	•	Create a certificate signing request (CSR)
keytool -certreq -keystore /usr/share/opennms/etc/jetty.keystore -file /tmp/opennms.services.net-lab.net.csr
	•	Let a cacert CA sign the CSR
Copy and paste /tmp/opennms.services.net-lab.net.csr into cacert's web UI and save the resulting (signed) certificate into a new file /tmp/opennms.services.net-lab.net.cert
	•	Import cacert.org root certificate and class3 certficate into your keystore:
wget --no-check-certificate -q -O - https://www.cacert.org/certs/root.crt | keytool -import -noprompt -alias cacertroot -trustcacerts -storepass ***yourpass*** -keystore /usr/share/opennms/etc/jetty.keystore
wget --no-check-certificate -q -O - https://www.cacert.org/certs/class3.crt | keytool -import -noprompt -alias cacertclass3 -trustcacerts -storepass ***yourpass*** -keystore /usr/share/opennms/etc/jetty.keystore
Note: You have to import the CAcert root certificate once into your browser, in order to make your browser trust the CAcert certficates (Some Linux distris already did that, see InclusionStatus )
	•	Import your cert into the keystore
keytool -import -noprompt -storepass valvoja -keystore /usr/share/opennms/etc/jetty.keystore -file /tmp/opennms.services.net-lab.net.cert

[edit] Version History/Availability
	•	This feature was added in version 1.3.10
</Text>
        </Document>
        <Document ID="75">
            <Title>User Management</Title>
            <Text>Setting Up a new user
This document shows you how to create a new local defined user and how to grant admin rights to users.
If you want to use LDAP authentication see LDAP Authentication
[edit] Starting out
From the main window of OpenNMS select Admin from the menu options then Configure Users, Groups and Roles. From the Configure Users, Groups and Roles window, select the Configure Users.
[edit] Create a User
	1.	Click on the Add New User button
	2.	Enter the User ID and the password for the new user and klick on OK
	3.	Enter the detail information for the user and klick on FINISH
If you would add the new user to a new group do the following:
	1.	Edit /opt/opennms/etc/groups.xml and add your new group to this file.
	2.	Return to the "Users and Groups" page and select "Configure Groups"
	3.	Klick on "Edit" on the new group
	4.	Add your user to the group and press "FINISH"
Check if the new user can log in to opennms.
[edit] Grant Admin privilege to an User
Set admin rights to the new user:
	1.	Edit /opt/opennms/etc/magic-users.properties and add the userid of the new user to "role.admin.users=" like
	2.	role.admin.users=admin, NewUser
	3.	and save the file.
</Text>
        </Document>
        <Document ID="123">
            <Title>Configuration File INdex</Title>
            <Text>This page shows all the configuration files (in $OPENNMS_HOME/etc/) you can configure in OpenNMS. Some files require a restart of OpenNMS for manual edits to take effect; some are re-written/managed automatically by OpenNMS and the Web GUI, and others may have an associated event that can be sent (e.g. using send-event.pl) to trigger re-reading of the file. Where a "Reload event" is documented below, sending it, with the documented parameter if required, will cause the given configuration file to be reloaded from disk.
For example, to reload the Ackd configuration:
$OPENNMS_HOME/bin/send-event.pl -p 'daemonName Ackd ' uei.opennms.org/internal/reloadDaemonConfig
Restart requirements have reduced over time as more forced reload events and automatic reload functionality has been implemented; see http://issues.opennms.org/browse/NMS-2388 for the status of the final push to make it all consistent under one UEI. But for now, there are several columns indicating the functionality available for different versions. In some cases the events that should be used have changed between versions; this is noted where relevant.
Note also that it is possible to restart individual daemons: see daemons for more details.
File
Restart Required
Reload Event
More information

1.3.7+
1.6+
1.8+


ackd-configuration.xml
N/A
N/A
No
uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Ackd)
Ackd configuration
actiond-configuration.xml
Yes
Yes
Yes
-
Actiond configuration
AvailabilityReports.xsl
No
No
No
-
XSL used for formatting Availability Reports. Read every time a report is generated; no reload required
c3p0.properties
Yes
Yes
Yes
-
Database connection pooling configuration. Read only at startup. See Jetty#Advanced_Configuration_.281.3.7.2B_only.29
capsd-configuration.xml
Yes
Yes
Yes
-
Discovery#Capabilities
castor.properties
Yes
Yes
N/A
-
Internal use only. Not present in 1.8+.
categories.xml
Yes
Yes
Yes
-
Categories
chart-configuration.xml
Yes
Yes
Yes
-
-
collectd-configuration.xml
Yes
No
No
uei.opennms.org/internal/schedOutagesChanged (required side effect)
Collectd
database-schema.xml
Yes
Yes
Yes
-
Internal use
datacollection-config.xml
Yes
Yes
Yes
-
Data_Collection_Configuration_How-To#datacollection-config.xml
destinationPaths.xml
No
No
No
Will automatically re-read as required, but in 1.8+ a (somewhat pointless) reload can be forced with uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Scriptd).
Configuring_notifications
dhcpd-configuration.xml
Yes
Yes
Yes
-
 ?
discovery-configuration.xml
Yes
No
No
uei.opennms.org/internal/discoveryConfigChange
Controls OpenNMS's auto discovery of nodes
eventconf.xml &amp;&amp; events/*
Yes
No
No
Up to 1.6.9 or so: uei.opennms.org/internal/reloadEventConfig. 1.8+ uei.opennms.org/internal/eventsConfigChange, or uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Eventd)
See Event Configuration
eventd-configuration.xml
Yes
Yes
Yes
-
eventd
events-archiver-configuration.xml
Yes
Yes
Yes
-

events.archiver.properties
Yes
Yes
Yes
-

exclude-ueis.properties (pending verification)
Yes
Yes
N/A
-
Not used as of at least 1.8.10
groups.xml
No
No
No
Reloaded automatically if changes made
 ?
http-datacollection-config.xml
Yes
Yes
Yes
-
HTTP_Collector
javamail-configuration.properties
No
No
No
Read every time javamail is used
Java_mailer
jcifs.properties
Yes
Yes
Yes
-
 ?
jmx-datacollection-config.xml
Yes
Yes
Yes
-
JMX_Collector
ksc-performance-reports.xml
No
No
No
Used and maintained by the web interface; manual edits not required
KSC_Reports
linkd-configuration.xml
Yes
Yes
Yes
-
Linkd
log4j-controller.properties
No
No
No
Reloaded automatically when changed

log4j.properties
No
No
No
Reloaded automatically when changed

magic-users.properties
No
No
No
-
User
map.disable
Yes
Yes
Yes
-
Disables map generation
map.properties
Yes
Yes
Yes
-
Defines properties for maps
model-importer.properties
Yes
Yes
Yes
uei.opennms.org/internal/importer/reloadImport
 ?
monitoring-locations.xml
Yes
Yes
Yes
-
Remote_Monitoring#monitoring-locations.xml
notifd-configuration.xml
No
No
No
Automatically reloaded when it changes
Notifd
notificationCommands.xml
Yes
No
No
1.6+: Unknown. 1.8+: uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Notifd)
Notifd
notifications.xml
No
No
No
Automatically reloaded when it changes
Notifd
nsclient-config.xml
Yes
Yes
Yes
-
Data_collection_from_Windows_Performance_Counters_with_NSClient++
nsclient-datacollection-config.xml
Yes
Yes
Yes
-
Data_collection_from_Windows_Performance_Counters_with_NSClient++
opennms-database.xml
Yes
Yes
N/A
-
Not in use from 1.8+ (or earlier?)
opennms-datasources.xml
Yes
Yes
Yes
-
The JDBC datasources for opennms and opennms-admin. The later is used by the installer to create the opennms database.
opennms.properties
Yes
Yes
Yes
Read at startup only

opennms-server.xml
Yes
Yes
Yes
-
 ?
poll-outages.xml
No
No
No
uei.opennms.org/internal/schedOutagesChanged
Configuration of scheduled polling outages
poller-config.properties
No
No
No
-
Maintained by the Web UI (functionality currently disabled); reloaded by the UI on every page load.
poller-configuration.xml
Yes
No
No
uei.opennms.org/internal/schedOutagesChanged
 ?
PDFAvailReport.xsl (maybe)
No
No
No
-
XSL used for formatting Availability Reports. Read every time a report is generated; no reload required
reportd-configuration.xml
Yes
Yes
Yes
-

rrd-configuration.properties
Yes
Yes
Yes
-

rtc-configuration.xml
Yes
Yes
Yes
-

response-adhoc-graph.properties
No
No
No
-
Managed by the Web UI.
response-graph.properties
No
No
No
Re-read automatically by the Web UI as required

scriptd-configuration.xml
Yes
No
No
1.6+: uei.opennms.org/internal/reloadScriptConfig, 1.8+: uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Scriptd).
Scriptd
service-configuration.xml
Yes
Yes
Yes
-
 ?
site-status-views.xml
No
No
No
Re-read automatically when it changes
 ?
snmp-adhoc-graph.properties
No
No
No
-
Managed by the Web UI.
snmp-config.xml
No
No
No
uei.opennms.org/internal/configureSNMP
 ?
snmp-graph.properties / snmp-graph.properties.d
No
No
No
Re-read automatically by the Web UI as required
Defines graphs for displaying collected data
statsd-configuration.xml
Yes
Yes
No
uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Statsd)
Statsd
surveillance-views.xml
No
No
No
-
 ?
SVGAvailReport.xsl
No
No
No
-
XSL used for formatting Availability Reports. Read every time a report is generated; no reload required
syslogd-configuration.xml
Yes
No
Yes
uei.opennms.org/internal/syslogdConfigChange. Event seems to have been removed by 1.8; not entirely sure when (To be investigated)

thresholds.xml
No
No
No
1.6+ uei.opennms.org/internal/thresholdConfigChange, 1.8+ uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Threshd *and* configFile thresholds.xml)
Thresholding
threshd-configuration.xml
Yes
Yes
No
1.8+: uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Threshd)
Thresholding
translator-configuration.xml
Yes
Yes
No
uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Translator)
Event_Translator
trapd-configuration.xml
Yes
Yes
Yes
-
 ?
users.xml
No
No
No
-
User
vacuumd-configuration.xml
No
No
No
1.6+: uei.opennms.org/internal/reloadVacuumdConfig, 1.8+: uei.opennms.org/internal/reloadDaemonConfig (with daemonName = Vacuumd).
See Automations
viewsdisplay.xml
Yes
Yes
No
Reloaded automatically when it has changed
 ?
vulnscand-configuration.xml
Yes
Yes
Yes
-
 ?
webui-colors.xml
Yes
Yes
No
Read every time as required
 ?
xmlrpcd-configuration.xml
Yes
Yes
Yes
-
 ?
xmpp-configuration.properties
Yes
Yes
Yes
-
 ?
</Text>
        </Document>
        <Document ID="119">
            <Title>Security Considerations</Title>
            <Text>This page is designed to keep various tips and tricks for running a secure OpenNMS installation. Since OpenNMS theorically has views into your entire network, keeping that information safe is very important.
For a fairly exhaustive list of TCP and UDP ports and ICMP datagram types that OpenNMS uses, see Firewall Policy and OpenNMS.
Contents
[hide]
	•	1 Tomcat Ports
	◦	1.1 ajp13 8009
	◦	1.2 http 8080
	•	2 The RTC User
[edit] Tomcat Ports
By default, in its 5.5 incarnation (as used by OpenNMS 1.3.2), a standard tomcat from tomcat.apache.org will start listeners on port 8080 (http connector), 8009 (ajp13 connector) and 8005 (shutdown service). The shutdown service binds directly to the loopback address, so we can effectively ignore this.
[edit] ajp13 8009
The ajp connector is only required if you intend to connect to tomcat via ajp13 from apache using mod_jk, mod_jk2 or mod_proxy. Most installations will be happy to turn this off by commenting it out. The configuration directives are found in $TOMCAT_HOME/conf/server.xml thus:
       &lt;Connector port="8009"
              enableLookups="false" redirectPort="8443" protocol="AJP/1.3" />    
You will need comment these out in the usual way and restart tomcat for this to take effect.
[edit] http 8080
In order to restrict access to tomcat's applications, you can use it's [remote address filter]. For simplicity's sake, it's probably best to configure this at the highest (Engine) level within tomcat's configuration heriarchy, for example, just under the default Engine configuration:

   &lt;Engine name="Catalina" defaultHost="localhost">

   &lt;Valve className="org.apache.catalina.valves.RemoteAddrValve" allow="192.168.1., 192.168.2." /> 





[edit] The RTC User
OpenNMS employs a Real-Time Console that allows the opennms daemon to communicate network status updates with the front-end servlet engine in near-real-time. Since all of the servlets are protected by a realm module, the opennms daemon must authenticate to the servlets. A special user, rtc was created for this purpose. In the default configuration, though, it is insecure.
By default, all OpenNMS users are part of the OpenNMS User role. This includes the rtc users. Since the default password for the rtc user is simply rtc, this means anyone who knows this information and has the ability to contact your OpenNMS installation can log in with this user and view your OpenNMS installation. The information contained in your OpenNMS installation is likely not public knowledge and should not be viewable by non-staff. Now, in all fairness, your OpenNMS installation should be protected from the Internet at large if at all possible, but you may have need for it to face the Internet, or you may have internal users who can get to your installation but should not be able to log in.
Lucky for us, changing the password, and even the username, for the rtc user is an easy task. The first place to change this information for versions 1.3.6 and older is the web.xml deployer file. Find the RTC Subscription parameters section in the web.xml file and change it to be the following:
  &lt;!-- RTC Subscription parameters -->  
  &lt;context-param>
    &lt;param-name>opennms.rtc-client.http-post.username&lt;/param-name>
    &lt;param-value>RTCUSER&lt;/param-value>
    &lt;description>The username the RTC uses when authenticating itself in an HTTP POST.&lt;/de  &lt;/context-param>
  &lt;context-param>
    &lt;param-name>opennms.rtc-client.http-post.password&lt;/param-name>
    &lt;param-value>RTCPASSWORD&lt;/param-value>
    &lt;description>The password the RTC uses when authenticating itself in an HTTP POST.&lt;/de  &lt;/context-param>
  &lt;context-param>
    &lt;param-name>opennms.rtc-client.http-post.base-url&lt;/param-name>
    &lt;param-value>http://localhost:8080/opennms/rtc/post&lt;/param-value>
    &lt;description>
      The base of a URL that RTC clients use when creating a RTC subscription URL. 
      IMPORTANT: This URL must NOT contain a slash at the end.       
    &lt;/description>
  &lt;/context-param>
Set the RTCUSER to your rtc user's username. Next, set the RTCPASSWORD to your chosen password.
For versions 1.3.7 and newer, change the rtc username and password in the file WEB-INF/configuration.properties if you use Tomcat, or in the file etc/opennms.properties if you use jetty.
If you change the rtc username, you must make sure the new username is a member of the OpenNMS RTC Daemon role in the etc/magic-users.properties file. Find the lines
role.rtc.name=OpenNMS RTC Daemon
role.rtc.users=RTCUSER, admin
and change RTCUSER to your new rtc user name.
If you're using LDAP Authentication, then you're done. Just make sure the password for the rtc user in your LDAP directory matches what you set above. If you're using the default OpenNMS realm module, you need tt make some additional changes to the magic-users.properties file in the OpenNMS configuration directory (/opt/OpenNMS/etc in the Linux RPMs).
If you changed the rtc username as described above, you need to change it in the magic-users.properties file as well. In addition, you'll need to change the rtc password in this file. Find the lines below and make the appropriate changes:
users=RTCUSER,otherusers

user.RTCUSER.username=RTCUSER
user.RTCUSER.password=RTCPASSWORD
Change all instances of RTCUSER to your new rtc user name you set above, and change the RTCPASSWORD to the same password you set above.
That's it. Restart OpenNMS and Tomcat (if you're using it), and you're done.
</Text>
        </Document>
        <Document ID="80">
            <Title>Discovery</Title>
            <Text>Introduction
[edit] Purpose
This How-To is one in a series designed to serve as a reference for getting started with OpenNMS. Eventually, these documents will cover everything necessary to get OpenNMS installed and running in your environment.
[edit] Copyright
Content is available under a Creative Commons Attribution-NonCommercial-ShareAlike2.5 License.
[edit] Corrections and Omissions
Please submit any corrections and omissions to the author.
[edit] Overview
OpenNMS is an enterprise-grade network management platform developed under the open-source model. Unlike traditional network management products which are very focused on network elements such as interfaces on switches and routers, OpenNMS focuses on the services network resources provide: web pages, database access, DNS, DHCP, etc. (although information on network elements is also available).
Since the majority of network services are provided using the TCP/IP protocol, OpenNMS is very IP-centric. The basic monitored "element" is called an "interface", and an interface is uniquely identified by an IP address. Services are mapped to interfaces, and if a number of interfaces are discovered to be on the same device (either via SNMP or SMB) then they may be grouped together as a "node".
Discovery in OpenNMS consists of two parts: discovering an IP address to monitor and then discovering the services supported by that IP address. The first part is much simpler than the second.
[edit] Discovery
[edit] Discovery User Interface
The most straight forward way of initiating Discovery is through the web interface. Navigate to Admin > Configure Discovery.
There you are presented with 2 options.
	1.	Specifics - which allows you to enter IP addresses of known individual interfaces.
	2.	Include URLs - which allows you to specify a file containing IP addresses to be included in discovery.
	3.	Include Ranges - which instructs OpenNMS to scan a range of IP addresses for active interfaces.
Nodes will appear in the Node List as they are discovered.
[edit] The Discovery Configuration File
Discovery in OpenNMS is controlled by the discovery-configuration.xml file (located in the /opt/OpenNMS/etc directory.
Let's look at that file:
&lt;discovery-configuration threads="1" packets-per-second="1"
                         initial-sleep-time="300000"
                         restart-sleep-time="86400000"
                         retries="3" timeout="800">

  &lt;include-range retries="2" timeout="3000">
    &lt;begin>192.168.0.1&lt;/begin>
    &lt;end>192.168.0.254&lt;/end>
  &lt;/include-range>

  &lt;include-url>file:/opt/OpenNMS/etc/include&lt;/include-url>
&lt;/discovery-configuration>
Now, all this file controls is a process that will send an ICMP "ping" to a particular set of IP addresses. If there is a response within the timeout, a "new suspect" event is generated. Otherwise, the IP address is ignored.
The global discovery attributes are:
threads 
This is the number of threads that will be used for discovery. By default this is set to 1.
packets-per-second 
This is the number of ICMP packets that will be generated each second. The default is 1. Note that there is a relationship between the packets-per-second and the number of threads. If your network has an average latency of 500ms, then setting packets-per-second to 2 would double the speed at which NewSuspect messages were created. But if there is only one thread available, setting this number to 3 would have little effect - the single thread would be processing as many packets as it could as fast as it could.
initial-sleep-time 
This is the time, in milliseconds, before the discovery process will commence after OpenNMS is started (by default 5 minutes). This delay is put in place to allow the product to fully start before generating new events.
restart-sleep-time 
Once the discovery process has completed, this is the time, in milliseconds, before it will start again. By default, the process will repeat 24 hours after the last discovery run has completed.
timeout 
this is the amount of time, in milliseconds, that the discovery process will wait for a response from a given IP address before deciding that there is nothing there. This can be overridden later in the file.
retries 
this is the number of attempts that will be made to query a given IP address before deciding that there is nothing there. This can be overridden later in the file.
Once the defaults are in place (defaults meaning the global values that will be used if they are not overridden in the tags below), the only thing left to tell the discovery process is which IP addresses to try. This is controlled by four different tags:
specific 
specify a IP address to be discovered. Multiple specific tags can be used.
&lt;specific>ip-address&lt;/specific>
Where ip-address is the address you want discovered. Note the lack of spaces between the tags.
include-range 
Specify a range of IP addresses to be discovered. Multiple include-range tags can be used.
&lt;include-range>
  &lt;begin>start-ip-address&lt;/begin>
  &lt;end>end-ip-address&lt;/end>
&lt;/include-range>
Where start-ip-address is the beginning of a range to be scanned and end-ip-address is the end of that range.
exclude-range 
Specify a range of IP address to be excluded from discovery.
&lt;exclude-range>
  &lt;begin>start-ip-address&lt;/begin>
  &lt;end>end-ip-address&lt;/end>
&lt;/exclude-range>
Where start-ip-address is the beginning of a range to be excluded and end-ip-address is the end of that range. Note that the exclude-range tag will only override addresses in an include-range. It will not override specific IP addresses or addresses included in a file. There is no "specific" version of the exclude tag - if you want to exclude a specific IP address use an exclude-range where the beginning and ending IP addresses are the same.
include-url 
Specify a file containing IP addresses to be included in discovery.
&lt;include-url>file:filename&lt;/include-url>
Where filename is the full path to a text file listing IP addresses, one to a line. Comments can be imbedded in this file. Any line that begins with a "#" character will be ignored, as will the remainder of any line that includes a space followed by "#".
All tags are optional and unbounded (you can have as many as you wish).
[edit] Another Way to Discover Interfaces
Now that the discovery configuration file has been explained, there are two short-comings that need to be pointed out. First, any changes to this file, like most of the configuration files within OpenNMS, requires that OpenNMS be restarted. Second, what if you want to discover a service, such as a web server, on a device you cannot ping?
Remember that all the discover process does is generate a newSuspect event. Included in the /opt/OpenNMS/bin directory is a Perl script called send-event.pl. You can use this script to generate an internal NewSuspect event - bypassing the discovery process altogether. Combined with a script, you could generate any number of NewSuspect events (just make sure that the IP address really does have some services that can be monitored by OpenNMS. Otherwise, you will have an interface in the system with no services associated with it).
The format of the send-event.pl is as follows:
/opt/opennms/bin/send-event.pl --interface ip-address uei.opennms.org/internal/discovery/newSuspect
 Replace ip-address with the address you want discovered.
For csv with FQDN as hosts location
cat fqdnlist.csv | awk -F "," '{print "host " $1}' | sh |awk '{print "./send-event.pl --interface " $4 " uei.opennms.org/internal/discovery/newSuspect"}'
For csv file with IP for host location
cat ipaddresss.csv |gawk -F "," '{print "host " $1 }' |sh |gawk '{print "./send-event.pl --interface " $1 " uei.opennms.org/internal/discovery/newSuspect"}'
note: use same format as exported assets.
Depending on what Perl modules you have installed, you may get an error running this script (such as a complaint about Getopt::Mixed). To automatically add the necessary modules, try:
perl -MCPAN -e 'install mod_name'
Replace mod_name with the name of the missing module.
Or, if you hate use CPAN (there is some problems, the CPAN can cose to lib's), you can just use rpm pakage
wget ftp://ftp.debian.nl/disk1/redhat-contrib/libc5/i386/Getopt-Mixed.pm-1.008-4.i386.rpm

rpm -Uv Getopt-Mixed.pm-1.008-4.i386.rpm 
(END)
[edit] Logs
You can watch the discovery process by examining the discovery.log file in the /opt/opennms/logs/daemon directory.
[edit] Capabilities
Okay, if the discovery process just generates NewSuspect events, what does all the work? This would be the capabilities daemon, capsd. capsd is responsible for discovering all the services to be monitored, such as httpd, DNS, etc., as well as if any collectors are present (at the time this is only SNMP).
The capsd process is controlled by the capsd-configuration.xml file. This file consists of some basic parameters and a collection of "protocols" to be tested. If the protocol is not in the file, then OpenNMS will not discover it.
On a restart of OpenNMS it schedules the scans based on the last capsd scan timestamps in the DB and the configured rescan interval.
[edit] Process Parameters for capsd
The first few lines of the capsd-configuration.xml file control how capsd will behave.
&lt;capsd-configuration rescan-frequency="86400000"
                     initial-sleep-time="300000"
                     management-policy="managed"
                     max-suspect-thread-pool-size="6"
                     max-rescan-thread-pool-size="3"
                     abort-protocol-scans-if-no-route="false">
	•	rescan-frequency  capsd will continue to check each interface to see if new services have been added. The frequency of these rescans is controlled by this parameter. The default value is 24 hours in milliseconds.  initial-sleep-time  like the discovery process, capsd will sleep for a certain amount of time after OpenNMS starts. The default value is 5 minutes in milliseconds.  management-policy  this parameter controls the default behavior of capsd. If it is set to "managed", then all IP addresses in NewSuspect events will be scanned, unless included in an "unmanaged" range defined at the end of this file. If this parameter is set to "unmanaged", then all NewSuspect events will be ignored unless the IP address in the event is expressly included in a "managed" range (also defined at the end of this file).  max-suspect-thread-pool-size  This value determines how many threads will be created to perform capability scans on IP addresses supplied by NewSuspect events. Increasing this value will make the initial discovery move more quickly at the cost of more system resources.  max-rescan-thread-pool-size  This value determines how many threads will be created to perform capability scans on interfaces that have already been discovered. Rescans are either automatically scheduled (see rescan-frequency) or generated ad hoc through the Web UI.  abort-protocol-scans-if-no-route  This is an extremely important parameter for modifying the behavior of capsd. When attempting to connect to a specific port to test for a service, it is possible to receive a "no route to host" exception. In theory, this is because the host is not reachable, but in practice any number of things, such as firewalls, can cause this error. If this parameter is set to "false", these "no route to host" messages are ignored. But if it is set to "true", then capsd will stop checking for additional services. This can greatly improve the speed of discovery if the capsd file has been "tuned" (discussed below).   [edit] Protocols  OpenNMS tests the existence of a particular network service through the use of "protocols". At the most basic, this could be a connection to a TCP port to test for a particular banner, but there are also special classes for a variety of other protocols. The current protocols supported out of the box are: 
	◦	Citrix
	◦	DHCP
	◦	DNS
	◦	Domino IIOP
	◦	FTP
	◦	General Purpose (script based)
	◦	HTTP
	◦	HTTPS
	◦	ICMP
	◦	IMAP
	◦	JBOSS
	◦	JDBC
	◦	JDBC Stored Procedure
	◦	JSR160
	◦	K5
	◦	LDAP
	◦	Microsoft Exchange
	◦	MX4J
	◦	Notes HTTP
	◦	NSClient (Nagios Agent)
	◦	NRPE (Nagios Remote Plugin Executor)
	◦	NTP
	◦	POP3
	◦	Radius
	◦	SMB
	◦	SMTP
	◦	SNMP
	◦	SSH
	◦	TCP
	◦	Windows Services (SNMP-based)
	•	When a newSuspect event is received by capsd and the management policy for the IP address in that event is "managed," the capsd process will work its way through this file testing one protocol after another, in the order they are listed in this file. The first protocol to be tested is ICMP:  &lt;protocol-plugin protocol="ICMP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.IcmpPlugin"
	•	                 scan="on">
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 Each protocol starts with a protocol-plugin tag. This tag has four attributes:  protocol  This is the name of the protocol.  class-name  This defines the protocol class that will be used to test for the service.  scan  Capsd scans can be turned "on" or "off" per protocol with this attribute.  user-defined  In versions prior to something like 1.6.9 and 1.7.9 there has been a tag user-defined="value" (see example below) which could have had the values true or false. This tag obviously was never used in the code and has now been removed.   In addition, each protocol-plugin can have a number of properties defined by a key and a value. The possible properties for each protocol will be discussed in the next section, although almost all include a timeout value and the number of times to try to make a connection.  There is a little-known feature available in capsd. This is the ability to configure each protocol based on IP addresses. This is through the protocol-configuration tag. The best way to describe this is through an example. Let's take the ICMP configuration from above and modify it:  &lt;protocol-plugin protocol="ICMP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.IcmpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;protocol-configuration scan="on" user-defined="false">
	•	    &lt;range begin="192.168.10.0" end="192.168.10.254"/>
	•	    &lt;property key="timeout" value="4000"/>
	•	    &lt;property key="retry" value="3"/>
	•	  &lt;/protocol-configuration>
	•	
	•	  &lt;protocol-configuration scan="off" user-defined="false">
	•	    &lt;range begin="192.168.20.0" end="192.168.20.254"/>
	•	  &lt;/protocol-configuration>
	•	
	•	  &lt;protocol-configuration scan="enable" user-defined="false">
	•	    &lt;specific>192.168.30.1&lt;/specific>
	•	  &lt;/protocol-configuration>
	•	
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 There are three protocol-configuration tags that have been added. Suppose you have one subnet that is over a slow link and it may take a little longer for an ICMP request to be returned. In the first example, the 192.168.10.0 subnet is allowed a 4 second response instead of the default of 2, and three retries.  Suppose you have another segment that you just don't want to scan for ICMP. In the second example, scan is set to "off", and that range will not be tested for ICMP.  Finally, the third example demonstrates setting scan to "enable", which forces the protocol to be associated with the device without testing for it. This is useful if you know the protocol is going to exist on a device, but for some reason it has not been added yet or it is down. Note that "enable" only works for protocol-configuration tags and not the main plugin tag.  [edit] Plugin Properties  The following table shows all of the property tags that are available for each protocol plugin. The default values are the ones hard-coded into the plugin itself, not the defaults in the configuration file.  [edit] Citrix  port  The port to connect to. Default is "1494".  timeout  The time in milliseconds to wait for a response. The default is "5000".  retries  The number of attempts made to detect the service. The default is "0".   [edit] DHCP  port  The port to connect to. Default is "67".  timeout  The time in milliseconds to wait for a response. Default is "3000".  retries  The number of attempts made to detect the service. Default is "3".   [edit] DNS  port  The port to connect to. Default is "53".  timeout  The time in milliseconds to wait for a response. Default is "3000".  retries  The number of attempts made to detect the service. Default is "3".  lookup  The default host name to attempt to resolve. Default is "localhost".   [edit] Domino IIOP  ports  The port to connect to. Default is "63148".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  IOR port  Port to look for the IOR via HTTP. Default is "80".   [edit] FTP  port  The port to connect to. Default is "21".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] HTTP  ports  The port to connect to (can be more than one, separated by a comma). Default is "80,8080,8000".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  max-ret-code (1.3.2 and later)  The highest numerical HTTP response code that will be taken to indicate success. Default is 399 if a URL is specified, 600 if not.  check-return-code (1.3.2 and later)  Boolean indicating whether or not to check the HTTP response code for success/failure. Default is "true". Note that illegal return codes (99 &lt;= code >= 600, per RFC1945) still indicate failure.   [edit] HTTPS  ports  The port to connect to (can be more than one, separated by a comma). Default is "443".  timeout  The time in milliseconds to wait for a response. Default is "30000".  retries  The number of attempts made to detect the service. Default is "1".  max-ret-code (1.3.2 and later)  The highest numerical HTTP response code that will be taken to indicate success. Default is 399 if a URL is specified, 600 if not.  check-return-code (1.3.2 and later)  Boolean indicating whether or not to check the HTTP response code for success/failure. Default is "true". Note that illegal return codes (99 &lt;= code >= 600, per RFC1945) still indicate failure.   [edit] ICMP  timeout  The time in milliseconds to wait for a response. Default is "800".  retries  The number of attempts made to detect the service. Default is "2".   [edit] IMAP  port  The port to connect to. Default is "143".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] JBOSS factory  The method of connecting to JMX. Default is "RMI". The other acceptable value is "HTTP".  timeout  The time in milliseconds to wait for a response. Default is "3000".  version  The version of JBOSS being detected. Default is "4".  port  The TCP port to use for the connection. Default is "1099".   [edit] JDBC Unlike nearly all the other plugins, the JDBC plugin is *highly* unlikely to work with the default configuration values. You will have to configure user. password, url and driver to match your database before this will work.  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  user  The username with which to authenticate to the database. Default is "sa"  password  The password corresponding to the username. Default is blank  url  The url of the database (JDBC url foramt). Default is "jdbc:sybase:Tds:OPENNMS_JDBC_HOSTNAME/tempdb"  driver  The JDBC driver class to create the connection from. Default is "com.sybase.jdbc2.jdbc.SybDriver"  host  The host the database lives on. Default is "OPENNMS_JDBC_HOSTNAME"   [edit] JDBC Stored Procedure Configuration is as for the JDBC plugin, except there is an additional parameter to define the stored procedure to run. Caveats regarding configuration of the JDBC plugin apply here also. The additional parameter:  stored-procedure  The name of the stored procedure to run after connecting to the database. Default is "isRunning". The stored procedure must have a single output parameter of type java.sql.Types.BIT. The actual return value is discarded   [edit] JSR160 timeout  The time in milliseconds to wait for a response. Default is "5000".   [edit] LDAP  port  The TCP port on which to look for the LDAP service. Default is "389".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] Microsoft Exchange  timeout  The port to connect to. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0"  pop3 port  The port to look for the POP3 protocol. Default is "110".  imap port  The port to look for the IMAP protocol. Default is "143".  mapi port  The port to look for the MAPI protocol. This port/service is used by Exchange for doing RPC over HTTP. Default is "593".   [edit] MX4J timeout  The time in milliseconds to wait for a response. Default is "5000".   [edit] Notes HTTP ports  The port to connect to (can be more than one, separated by a comma). Looks for the string "Notes" in the banner. Default is "80,8080,8000".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] NRPE command  The command to send to the NRPE agent. Default is the NRPE Hello command ("_NRPE_CHECK")  port  The port the NRPE agent is listening on. Default is "5666"  padding  The padding to use in the packet. Default is 2  timeout  The time in milliseconds to wait for a response. Default is "5000".  retry  The number of attempts made to detect the service. Default is "0".  usessl (available from OpenNMS 1.3.10) Whether to use NRPE over SSL. Default is "false". Set to "true" to enable.   [edit] NSClient command  The command to send to the NSClient agent. Default is the client version check ("1").  port  The port on which the agent is listening. Default is "1248"  parameter  A parameter to send along with the command. Default is null  criticalPercent  If the command sent returns a value which can be compared, this value is the comparison value for a critical level. Default is "0"  warningPercent  If the command sent returns a value which can be compared, this value is the comparison value for a warning level. Default is "0"  password  The password needed to connect to the agent. Default is "None"  timeout  The time in milliseconds to wait for a response. Default is "5000".  retry  The number of attempts made to detect the service. Default is "0".   [edit] NTP port  The port to connect to. Default is "123".  timeout  The time in milliseconds to wait for a response. Default is "3000".  retries  The number of attempts made to detect the service. Default is "3".   [edit] POP3  port  The port to connect to. Default is "110".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] Radius authport  The port the radius daemon uses for authentication. Default is 1812  acctport  The port the radius daemon uses for accounting. Default is 1813  authtype  The type of authentication the radius daemon requires. Default is "pap"  user  A username that can be used to test authentication. Default is "OpenNMS"  password  A corresponding password that can be used to test authentication. Default is "OpenNMS"  secret  The shared secret with the radius daemon. Default is "secret"  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] SMB  No properties for SMB plugin.  [edit] SMTP  port  The port to connect to. Default is "25".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] SNMP  port  The port to connect to. Default is "161".  timeout  The time in milliseconds to wait for a response. Default is null.  retries  The number of attempts made to detect the service. Default is null.  force version  The protocol version (SNMPv1 or SNMPv2) to use to check for the service. Default is null.  vbname  The OID to query. Default is ".1.3.6.1.2.1.1.2" (this is SNMPv2-MIB::sysObjectID.0).  vbvalue  The (optional) value to check for if the OID returns one. Default is null.   [edit] SSH timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  port  The port the ssh daemon is listening on. Default is "22".  match  A regular expression to check for in the response from the SSH server. Default is null  banner  If match is not defined, another regular expression to check for in the response. Default is null   [edit] TCP  port  The port to connect to. Default is null.  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  banner  Check the "banner" string against the string that is returned if the connection is successful. Default is null.   [edit] Windows Services (Win32ServicePlugin)  service-name  The name of the Windows service you are interested in, e.g. "Task Scheduler". Default is "Server".   This plugin extends the SnmpPlugin and therefore also accepts all of its properties.  [edit] Mapping Protocol Plugins to Services  Note that the protocol plugins represent the code that is used to determine if a particular service exists. It is not the service itself. The capsd-configuration.xml file is where the services are actually defined.  For example, look at the HTTP service:  &lt;protocol-plugin protocol="HTTP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.HttpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="ports" value="80"/>
	•	  &lt;property key="timeout" value="3000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 This service will use the HTTP plugin to check for a service on port 80 and create a service called "HTTP". With a simple change of port number, you can create a new service:  &lt;protocol-plugin protocol="HTTP-8080"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.HttpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="ports" value="8080"/>
	•	  &lt;property key="timeout" value="3000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 Same protocol plugin, but a completely different service as far as OpenNMS is concerned. In some cases, mainly with HTTP, you can check multiple ports. If you do this, then the service will be considered to exist if a valid response is received on any or all of the ports tested.  One of the more powerful plugins is the TCP plugin. Here it is used to test for the existence of secure shell:  &lt;protocol-plugin protocol="SSH"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.TcpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="banner" value="SSH"/>
	•	  &lt;property key="port" value="22"/>
	•	  &lt;property key="timeout" value="3000"/>
	•	  &lt;property key="retry" value="3"/>
	•	&lt;/protocol-plugin>
	•	 This will connect to port 22 and look for the string "SSH" to be returned. By using this banner check you could create different services for different version of software, such as Oracle7 versus Oracle8, as long as the information was included in the banner (to check the banner, you can use telnet ip-address port). Currently, the match is strictly a substring search. In future versions regular expression may be allowed.  [edit] Server Message Block (SMB)  SMB is used by Windows servers to share files, similar to NFS. OpenNMS does not poll either SMB or NFS, but it can use some of the information provided by SMB to name nodes and group interfaces into nodes. If SMB is discovered on a device, it will be noted on the node page. You can allow OpenNMS to "log in" to an SMB share using the following tag:  &lt;smb-config>
	•	  &lt;smb-auth user="guest" password="guest" type="domain">WORKGROUP&lt;/smb-auth>
	•	&lt;/smb-config>
	•	 Here you can enter in a valid username, password and domain for OpenNMS to use when trying to connect to an interface.  [edit] Management Policies  As mentioned in the beginning of this section, the default management policy is "managed", which means that capsd will attempt a services scan on all interfaces in newSuspect events. This can be overridden with the ip-management tag. From the default capsd-configuration.xml configuration file:  &lt;ip-management policy="managed">
	•	  &lt;range begin="192.168.0.0" end="192.168.0.255"/>
	•	  &lt;include-url>file:/opt/OpenNMS/etc/include&lt;/include-url>
	•	&lt;/ip-management>
	•	
	•	&lt;ip-management policy="unmanaged">
	•	  &lt;specific>0.0.0.0&lt;/specific>
	•	  &lt;range begin="127.0.0.0" end="127.255.255.255"/>
	•	&lt;/ip-management>
	•	 This tag has a policy attribute which can be either managed or unmanaged. Then you can define ranges, specific IP addresses and files as needed. Note that the "managed" example is used specifically as an example: since the default policy is "managed" it is not needed.  [edit] SNMP  The SNMP protocol is a special case. While most of the other services will eventually be polled, the SNMP service is used to collect data. Let's look at its definition in the configuration file:  &lt;protocol-plugin protocol="SNMP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.SnmpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="force version" value="SNMPv1"/>
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="3"/>
	•	&lt;/protocol-plugin>
	•	 Note the force version property. Since SNMP version 2 agents will respond to SNMP version 1 requests, this test will find both agents. This property has nothing to do with how the data will be collected. The SNMP collector automatically checks for SNMPv2 and will use GET-BULK commands to retrieve the data (unless overridden in the snmp-config.xml file). But if you wanted to manage a service called "SNMPv2" you could create one with:  &lt;protocol-plugin protocol="SNMPv2"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.SnmpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="force version" value="SNMPv2"/>
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="3"/>
	•	&lt;/protocol-plugin>
	•	 Note that the "SNMPv2" that existed in early 0.9 is no longer checked by default.  [edit] The snmp-config.xml File  The parameters used to connect with SNMP agents are defined in the snmp-config.xml file. Here is an example:  &lt;snmp-config retry="3" timeout="800"
	•	             read-community="public" write-community="private">
	•	  &lt;definition version="v2c">
	•	    &lt;specific>192.168.0.5&lt;/specific>
	•	  &lt;/definition>
	•	
	•	  &lt;definition retry="4" timeout="2000">
	•	    &lt;range begin="192.168.1.1" end="192.168.1.254"/>
	•	    &lt;range begin="192.168.3.1" end="192.168.3.254"/>
	•	  &lt;/definition>
	•	
	•	  &lt;definition read-community="bubba" write-community="zeke">
	•	    &lt;range begin="192.168.2.1" end="192.168.2.254"/>
	•	  &lt;/definition>
	•	
	•	  &lt;definition port="1161">
	•	    &lt;specific>192.168.5.50&lt;/specific>
	•	  &lt;/definition>
	•	&lt;/snmp-config>
	•	 The attributes for the snmp-config tag are as follows:  retry  The number of attempts that will be made to connect to the SNMP agent.  timeout  The amount of time, in milliseconds, that OpenNMS will wait for a response from the agent.  read-community  The default "read" community string for SNMP queries.  write-community  The default "write" community string for SNMP queries. Note that this is for future development - OpenNMS does not perform SNMP "sets" at the moment.   All of the global parameters can be overridden with definition tags. These new SNMP definitions can apply to ranges or specific IP addresses. In addition, there are two other attributes available:  port  This overrides the default port of 161.  version  Here you can force either SNMP version 1 "v1" or version 2c "v2c".   [edit] capsd and SNMP  When testing SNMP, capsd makes an attempt to receive the sysObjectID for the device using the community string and port defined in snmp-config.xml. If this succeeds, the SNMP protocol is marked as "true" for this IP address. Note that it takes the first valid match in snmp-config.xml for that IP address, something to look for if the address is included in multiple ranges.  Once all of the protocols have been tested, if SNMP is true for this IP address, more tests are performed by capsd.  First, three threads are generated three SNMP requests are made to collect the data from the system tree, the ipAddrTable and ifTable.  If, for some reason, the ipAddrTable or ifTable are unavailable, the process stops (but the SNMP system data may show up on the node page - this happens a lot with UC-Davis SNMP agents where only the system tree is available to a query using the "public" community string).  Second, all of the sub-target IP addresses in the ipAddrTable are run through the capsd capabilities scan. Note that this is regardless of how management is configured in the configuration file. This only happens on the initial scan and on forced rescans. On normal rescans (by default, every 24 hours), IP addresses that are "unmanaged" in capsd are not polled.  Third, every IP address in the ipAddrTable that supports SNMP is tested to see if it maps to a valid ifIndex in the ifTable. If this is true, the IP address is marked as a secondary SNMP interface and is a contender for becoming the primary SNMP interface.  Finally, all secondary SNMP interfaces are tested to see if they match a valid package in the collectd-configuration file. If more than one valid IP address meets all three criteria (supports SNMP, has a valid ifIndex and is included in a collection package), then the lowest IP address is marked as primary. All SNMP data collection is performed via the primary SNMP interface.  (Note: in the future we will have the ability to change to a secondary SNMP interface should the primary become unavailable).  When the capsd testing process is complete, events are generated, including NodeGainedService events.  SNMP data collection is covered in another How-To (Data Collection Configuration How-To).  [edit] Conclusion  It is hoped that this How-To has proved useful. Please direct corrections and comments to the author.
</Text>
        </Document>
        <Document ID="76">
            <Title>Spring Security and LDAP</Title>
            <Text>Abstract
Centralized authentication is a "big deal" in any enterprise environment. Integrating your OpenNMS implementation with your environment is a Good Thing. Before linking the two, there are some considerations. The main one is if you want to rely solely on your central authenticator for all OpenNMS auth events, or support a hybrid scheme of the built-in local (aka UserDAO) as well as an LDAP or similar authenticator. Keep in mind that OpenNMS, itself, has processes periodically authenticating (think RTC and Provisioning) and for this reason it is better to go the loosely-coupled route by continuing to support the UserDAO mechanism, as described in the latter implementation section below.
Another source of minutiae lies in the specific LDAP implementation you're working with. While AD and OpenLDAP (and Sun DS, etc.) implement the same protocol, their schemas are substantially different. The biggest difference (as highlighted by LDAP search filters referenced below) is the memberOf attribute, which is not supported by default LDAP v3 spec (and is not present in common OpenLDAP installs), but comes standard with AD. This has a significant implication in that you cannot filter a user search by supplementary group membership, and most installs tend not to be fully hierarchical (User's DN doesn't cite a home group). Fortunately, you can either create dedicated per-role LDAP groups or map existing groups to roles by selecting and configuring the appropriate AuthoritiesPopulator. A 'role' is how OpenNMS refers to an access level, btw.
[edit] Sample Configuration "LDAP only"
[edit] OpenNMS Version
OpenNMS version 1.7.5 and later
[edit] Spring Security Version
Spring Security 2.04
[edit] Spring Security
Version 1.7.5 of openNMS never included the spring security framework so you will need to download it from the following URL spring-security-2.0.4.zip.
[edit] Notes about provided configs
I developed these configurations for my company but have tried to sanitize them for public consumption there are fields of note that will be required to be changed in addition to the following you may enable TLS/SSL for your LDAP server if you install root ca's which is beyond the scope of this document.
You will need to configure the following values.
LDAP host and its port : ldap://foo.com:389 LDAP search base : ou=mydepartment,o=mycompany,dc=com
	▪	Note this account only needs search privileges and should be acl limited
User that will do binds to perform lookups : uid=myUser,ou=Users,ou=mydepartment,o=mycompany,dc=ca Password for the above user : myPassword
The group section for the bind is based off the ldif I have provided, customize as you see fit.

[edit] Configuring applicationContext-spring-security.xml
Spring security is configured in $OPENNMS_HOME/jetty-webapps/opennms/WEB-INF/applicationContext-spring-security.xml. You have to add the following definitions there.
  &lt;beans:bean id="upperCaseMd5PasswordEncoder" class="org.opennms.web.springframework.security.UpperCaseMd5PasswordEncoder"/>

&lt;!-- LDAP Start -->
  &lt;beans:bean id="contextSource" class="org.springframework.security.ldap.DefaultSpringSecurityContextSource">

    &lt;!-- Enter your LDAP server address, dc, o and ou  here -->
    &lt;beans:constructor-arg value="ldap://foo.com:389/ou=mydepartment,o=mycompany,dc=com" />

    &lt;!-- Enter User parameters for accessing your LDAP server here -->
    &lt;beans:property name="userDn" value="uid=myUser,ou=Users,ou=mydepartment,o=mycompany,dc=ca"/>
    &lt;beans:property name="password" value="myPassword"/>

  &lt;/beans:bean>
    &lt;beans:bean id="ldapAuthProvider" class="org.springframework.security.providers.ldap.LdapAuthenticationProvider">
      &lt;custom-authentication-provider />
        &lt;beans:constructor-arg>
           &lt;beans:bean class="org.springframework.security.providers.ldap.authenticator.BindAuthenticator">
             &lt;beans:constructor-arg ref="contextSource"/>
             &lt;beans:property name="userSearch" ref="userSearch">
             &lt;/beans:property>
           &lt;/beans:bean>
        &lt;/beans:constructor-arg>
        &lt;beans:constructor-arg>
           &lt;beans:bean class="org.springframework.security.ldap.populator.DefaultLdapAuthoritiesPopulator">
             &lt;beans:constructor-arg ref="contextSource"/>
             &lt;beans:constructor-arg value="ou=ONMS,ou=Groups"/>
             &lt;beans:property name="groupRoleAttribute" value="cn"/>
             &lt;beans:property name="searchSubtree" value="false" />
             &lt;beans:property name="convertToUpperCase" value="true" />
             &lt;beans:property name="rolePrefix" value="ROLE_" />
             &lt;beans:property name="groupSearchFilter" value="memberUid={1}" />
           &lt;/beans:bean>
        &lt;/beans:constructor-arg>
  &lt;/beans:bean>

  &lt;beans:bean id="userSearch" class="org.springframework.security.ldap.search.FilterBasedLdapUserSearch">
    &lt;beans:constructor-arg index="0" value="ou=Users" />
    &lt;beans:constructor-arg index="1" value="(uid={0})" />
    &lt;beans:constructor-arg index="2" ref="contextSource" />
    &lt;beans:property name="searchSubtree" value="true" />
  &lt;/beans:bean>


&lt;!-- LDAP End -->

  &lt;!-- ====================== RADIUS AUTHENTICATION ===================== -->
[edit] Modify the magic-users.properties
###########################################################################
## U S E R S
###########################################################################

# A comma-separated list of user keys.  A user.{KEY}.username and
# user.{KEY}.password property must be set for each key in this property.
users=rtc

# The RTC View Control Manager daemon uses this user to authenticate itself
# while sending RTC data posts.
user.rtc.username=rtc
user.rtc.password=rtc

###########################################################################
## R O L E S
###########################################################################

# A comma-separated list of role keys.  A role.{KEY}.name and
# role.{KEY}.users property must be set for each key in this property.
#roles=rtc, admin, rouser, dashboard, provision
roles=rtc

# This role allows a user to make RTC data posts.
role.rtc.name=OpenNMS RTC Daemon
role.rtc.users=rtc
role.rtc.notInDefaultGroup=true

# This role allows users access to configuration and
# administrative web pages.
#role.admin.name=OpenNMS Administrator
#role.admin.users=admin

# This role disallows user write access
#role.rouser.name=OpenNMS Read-Only User
#role.rouser.users=

# This role allows access to the dashboard only
#role.dashboard.name=OpenNMS Dashboard User
#role.dashboard.users=
#role.dashboard.notInDefaultGroup=true

# This role was added for WIND
#role.provision.name=OpenNMS Provision User
#role.provision.users=
[edit] Sample LDIF For group structure
dn: ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
ou: ONMS
description: Open NMS groups
objectClass: top
objectClass: organizationalUnit

dn: cn=ADMIN, ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
gidNumber: 7010
userPassword:: e2NyeXB0fXg=
memberUid: myAdminUser
objectClass: posixGroup
objectClass: top
cn: ADMIN

dn: cn=ANONYMOUS, ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
gidNumber: 7011
userPassword:: e2NyeXB0fXg=
memberUid: myAnonymousUser
objectClass: posixGroup
objectClass: top
cn: ANONYMOUS

dn: cn=USER, ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
gidNumber: 7012
userPassword:: e2NyeXB0fXg=
memberUid: myRegUser
objectClass: posixGroup
objectClass: top
cn: USER

dn: cn=DASHBOARD, ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
gidNumber: 7013
userPassword:: e2NyeXB0fXg=
memberUid: myDashboardUser
objectClass: posixGroup
objectClass: top
cn: DASHBOARD

dn: cn=PROVISION, ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
gidNumber: 7014
userPassword:: e2NyeXB0fXg=
memberUid: myProvisionUser
objectClass: posixGroup
objectClass: top
cn: PROVISION

dn: cn=RTC, ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
gidNumber: 7015
userPassword:: e2NyeXB0fXg=
memberUid: myRTCUser
objectClass: posixGroup
objectClass: top
cn: RTC

dn: cn=REMOTING, ou=ONMS, ou=Groups,ou=mydepartment,o=mycompany,dc=com
gidNumber: 7015
userPassword:: e2NyeXB0fXg=
memberUid: myRemotePollerUser
objectClass: posixGroup
objectClass: top
cn: REMOTING

[edit] Sample Configuration "Local Authentication and LDAP"
[edit] OpenNMS Version
OpenNMS version 1.7.8 and later
[edit] Spring Security Version
Spring Security 2.04 as integrated in OpenNMS 1.7.8
[edit] Design
	•	OpenNMS Administration is done for some (internal or external) customer
	•	OpenNMS Admin Responsibility should never be gained by the customer
	•	LDAP administration is done by the customer (internal or external)
	•	OpenNMS internal users shouldn't be authenticated by LDAP
	◦	to prevent OpenNMS outages when someone by error changes OpenNMS LDAP users
	◦	for having availability of OpenNMS even when LDAP is down/unreachable
	◦	to avoid OpenNMS performance problems when LDAP responds slowly (opennms internal users heavily use authentication calls)
[edit] Technics
	•	OpenNMS-provided authentication is used for Admin and internal users
	•	LDAP is provided from Active Directory (AD)
	◦	only LDAP users from a specific department (OU=...) are allowed
	◦	only users belonging to one of the groups g_operating or g_helpdesk are allowed
	◦	saMAccountName is used to identify the UserID within LDAP/AD
	◦	as always, there are two people from another department that have to be allowed access, too... unfortunally, this part of the example config below doesn't work as designed so don't use it, but it's left here an idea for further enhancements!
[edit] Solution
	•	use DAO authentication (OpenNMS local authentication) for Admin Users
	•	use DAO authentication for opennms internal users like RTC, MAP etc.
	•	use LDAP authentication for all (internal or external) customers
	•	assign ROLE_USER to all LDAP authenticated users
[edit] Values from LDAP/AD Server
First you need the ip address and Port of your LDAP server, Port typically is 389. To query the LDAP server your LDAP admin should provide you with an userid and a password. Look for "userIDforLDAP-Queries" in the sample config below.
Further you need to know where in the LDAP tree you have to lookup the users you want to authorize. Ask your LDAP admin for those values.
Values from LDAP/AD used in this sample to lookup users (some lines from LDIF of a sample-user):
dn: CN=SampleUserId,OU=SomeDept,OU=SomeOrgUnit,dc=SomeCompany,dc=SomeCountry
...
memberOf: CN=g_operating,OU=SomeDept,OU=SomeOrgUnit,dc=SomeCompany,dc=SomeCountry
...
sAMAccountName: SampleUserID
...
[edit] Spring Security Configuration
Spring security is configured in $OPENNMS_HOME/jetty-webapps/opennms/WEB-INF/applicationContext-spring-security.xml. You have to add the following definitions there. You don't have to alter other configuration files for this LDAP integration. Admin-Users are configured in OpenNMS as described in Docu-overview#Users.
Just add the following lines below the Dao Authentication in applicationContext-spring-security.xml and change the fields like IP-address / Root-DN / Userid / Password for the LDAP-server, UserSearch path and groups the users must belong to.
Take a look at the filters for the groups - they contain the complete path, obviously in this notation neither the root-DN nor the SearchBase part of the path is added to the filter! (Took me some time to figure this out...).

[edit] LDAP Authentication Configuration
Below is the configuration for doing the actual authentication. You will also need to include one of the two AuthoritiesPopulators.
	•	Detailed AD-specific Config (userSearch with group membership checks, etc.)
  &lt;!-- ======================= LDAP AUTHENTICATION ====================== -->

  &lt;!-- you need the following line only if you commented out
       User Dao Authentication above where it is already defined
  -->
  &lt;!--
  &lt;beans:bean id="upperCaseMd5PasswordEncoder" class="org.opennms.web.springframework.security.UpperCaseMd5PasswordEncoder"/>
  -->
 
 
  &lt;!-- this defines your ldap server address, userid, password etc. -->
 
  &lt;beans:bean id="contextSource" class="org.springframework.security.ldap.DefaultSpringSecurityContextSource">
    &lt;beans:constructor-arg value="ldap://11.12.13.14:389/dc=SomeCompany,dc=SomeCountry" />
    &lt;beans:property name="userDn" value="userIDforLDAP-Queries"/>
    &lt;beans:property name="password" value="password"/>
  &lt;/beans:bean>
 
  &lt;!-- this defines LDAP as an authentication provider -->
 
  &lt;beans:bean id="ldapAuthProvider" class="org.springframework.security.providers.ldap.LdapAuthenticationProvider">
    &lt;custom-authentication-provider />      &lt;!-- this adds the ldap authentication method to the ProviderManager -->
    &lt;beans:constructor-arg ref="ldapAuthenticator"/>
    &lt;beans:constructor-arg ref="ldapAuthoritiesPopulator"/>
  &lt;/beans:bean>
 
  &lt;!-- authenticate the user with ldapAuthenticator using userSearch -->
 
  &lt;beans:bean id="ldapAuthenticator" class="org.springframework.security.providers.ldap.authenticator.BindAuthenticator">
    &lt;beans:constructor-arg ref="contextSource"/>
    &lt;beans:property name="userSearch" ref="userSearch">
    &lt;/beans:property>
  &lt;/beans:bean>
 
    &lt;!-- userSearch (alt.: userDnPatterns) -->
        &lt;!-- Start the userSearch as far down in the tree as possible.
             As there are two people from other departments we have to start 
             the search at a higher level, here OU=SomeOrgUnit.
             Else we could start at OU=SomeDept,OU=SomeOrgUnit
         -->
 
    &lt;beans:bean id="userSearch" class="org.springframework.security.ldap.search.FilterBasedLdapUserSearch">
      &lt;beans:constructor-arg index="0" value="OU=SomeOrgUnit" />
 
      &lt;!-- allow only users belonging to specific groups to get authenticated -->
      &lt;beans:constructor-arg index="1" value="(&amp;amp;(sAMAccountName={0})
                 (|(memberOf=cn=g_operating,OU=SomeDept,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)
                   (memberOf=cn=g_helpdesk,OU=SomeDept,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)
                   (cn=userid_1,OU=hisDepartment,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)
                   (cn=userid_2,OU=herDepartment,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)))" />
          &lt;beans:constructor-arg index="2" ref="contextSource" />
          &lt;beans:property name="searchSubtree" value="true" />
    &lt;/beans:bean>

&lt;!-- Unfortunally this idea to add userid_1 and userid_2 from another departement
     didn't work as designed. If anyone knows how to do this please add the correct
     configuration here!

      &lt;beans:constructor-arg index="1" value="(&amp;amp;(sAMAccountName={0})
                (|(memberOf=cn=g_operating,OU=SomeDept,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)
                  (memberOf=cn=g_helpdesk,OU=SomeDept,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)
                  (cn=userid_1,OU=hisDepartment,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)
                  (cn=userid_2,OU=herDepartment,OU=SomeOrgUnit,
                       dc=SomeCompany,dc=SomeCountry)))" />
          &lt;beans:constructor-arg index="2" ref="contextSource" />
          &lt;beans:property name="searchSubtree" value="true" />
    &lt;/beans:bean>
-->
	•	Basic OpenLDAP-friendly userSearch
  &lt;beans:bean id="userSearch" class="org.springframework.security.ldap.search.FilterBasedLdapUserSearch">
    &lt;beans:constructor-arg index="0" value="ou=People" />
    &lt;beans:constructor-arg index="1" value="(uid={0})" />
    &lt;beans:constructor-arg index="2" ref="contextSource" />
    &lt;beans:property name="searchSubtree" value="true" />
  &lt;/beans:bean>
[edit] LDAP Authorization
There are two AuthoritiesPopulator's, these use information gathered from LDAP to provide the authorization within OpenNMS.
[edit] org.springframework.security.ldap.populator.DefaultLdapAuthoritiesPopulator
Use this one when you are going to create role-specific LDAP groups (per the group structure LDIF sample above) and assign users to them for their corresponding OpenNMS access.
 &lt;beans:bean id="ldapAuthoritiesPopulator" class="org.springframework.security.ldap.populator.DefaultLdapAuthoritiesPopulator">
   &lt;beans:constructor-arg ref="contextSource"/>

   &lt;beans:constructor-arg value="OU=SomeDept,OU=SomeOrgUnit"/>

   &lt;beans:property name="groupRoleAttribute" value=""/>
   &lt;beans:property name="rolePrefix" value="" />

   &lt;beans:property name="defaultRole" value="ROLE_USER" />

 &lt;/beans:bean>
[edit] org.opennms.web.springframework.security.UserGroupLdapAuthoritiesPopulator
Use this one when you want to assign OpenNMS roles to an existing LDAP group structure. The value in the bean key is the name (cn) of the LDAP group you want to map to one or more roles (the bean values list).
	•	AD Variant
          &lt;beans:bean id="UserGroupLdapAuthoritiesPopulator" class="org.opennms.web.springframework.security.UserGroupLdapAuthoritiesPopulator">
            &lt;beans:constructor-arg ref="contextSource"/>
            &lt;beans:constructor-arg value="OU=SomeDept,OU=SomeOrgUnit"/>
            &lt;beans:property name="searchSubtree" value="true" />
            &lt;beans:property name="groupRoleAttribute" value="cn" />
            &lt;beans:property name="groupSearchFilter" value="member={0}" />
            &lt;beans:property name="groupToRoleMap">
              &lt;beans:map>
                &lt;beans:entry>
                  &lt;beans:key>&lt;beans:value>myusersgroup&lt;/beans:value>&lt;/beans:key>
                  &lt;beans:list>
                    &lt;beans:value>ROLE_USER&lt;/beans:value>
                  &lt;/beans:list>
                &lt;/beans:entry>
                &lt;beans:entry>
                  &lt;beans:key>&lt;beans:value>myadminsgroup&lt;/beans:value>&lt;/beans:key>
                  &lt;beans:list>
                    &lt;beans:value>ROLE_ADMIN&lt;/beans:value>
                    &lt;beans:value>ROLE_USER&lt;/beans:value>
                  &lt;/beans:list>
                &lt;/beans:entry>
              &lt;/beans:map>
            &lt;/beans:property>
          &lt;/beans:bean>
	•	OpenLDAP variant
         &lt;beans:bean id="UserGroupLdapAuthoritiesPopulator" class="org.opennms.web.springframework.security.UserGroupLdapAuthoritiesPopulator"> 
           &lt;beans:constructor-arg ref="contextSource"/> 
           &lt;beans:constructor-arg value="ou=Group"/> 
           &lt;beans:property name="searchSubtree" value="false" /> 
           &lt;beans:property name="groupRoleAttribute" value="cn" /> 
           &lt;beans:property name="groupSearchFilter" value="memberUid={1}" /> 
           &lt;beans:property name="groupToRoleMap"> 
             &lt;beans:map> 
               &lt;beans:entry> 
                 &lt;beans:key>&lt;beans:value>myusersgroup&lt;/beans:value>&lt;/beans:key> 
                 &lt;beans:list> 
                   &lt;beans:value>ROLE_USER&lt;/beans:value> 
                 &lt;/beans:list> 
               &lt;/beans:entry> 
               &lt;beans:entry> 
                 &lt;beans:key>&lt;beans:value>myadminsgroup&lt;/beans:value>&lt;/beans:key> 
                 &lt;beans:list> 
                   &lt;beans:value>ROLE_ADMIN&lt;/beans:value> 
                   &lt;beans:value>ROLE_USER&lt;/beans:value> 
                 &lt;/beans:list> 
               &lt;/beans:entry> 
             &lt;/beans:map> 
           &lt;/beans:property> 
         &lt;/beans:bean>
[edit] Another Example
  &lt;beans:bean id="upperCaseMd5PasswordEncoder" class="org.opennms.web.springframework.security.UpperCaseMd5PasswordEncoder"/>

&lt;!-- LDAP Start -->
  &lt;beans:bean id="contextSource" class="org.springframework.security.ldap.DefaultSpringSecurityContextSource">

    &lt;!-- Enter your LDAP server address, dc, o and ou  here -->
    &lt;beans:constructor-arg value="ldaps://ldap.example.com:636/dc=example,dc=com" />

    &lt;!-- Enter User parameters for accessing your LDAP server here 
    &lt;beans:property name="userDn" value="uid=myUser,ou=Users,ou=mydepartment,o=mycompany,dc=ca"/>
    &lt;beans:property name="password" value="myPassword"/>
    -->

  &lt;/beans:bean>
  &lt;beans:bean id="ldapAuthProvider" class="org.springframework.security.providers.ldap.LdapAuthenticationProvider">
    &lt;custom-authentication-provider />      &lt;!-- this adds the ldap authentication method to the ProviderManager -->
    &lt;beans:constructor-arg ref="ldapAuthenticator"/>
    &lt;beans:constructor-arg ref="UserGroupLdapAuthoritiesPopulator"/>
  &lt;/beans:bean>
 
  &lt;!-- authenticate the user with ldapAuthenticator using userSearch -->
 
  &lt;beans:bean id="ldapAuthenticator" class="org.springframework.security.providers.ldap.authenticator.BindAuthenticator">
    &lt;beans:constructor-arg ref="contextSource"/>
    &lt;beans:property name="userSearch" ref="userSearch">
    &lt;/beans:property>
  &lt;/beans:bean>

  &lt;beans:bean id="userSearch" class="org.springframework.security.ldap.search.FilterBasedLdapUserSearch">
    &lt;beans:constructor-arg index="0" value="ou=people" />
    &lt;beans:constructor-arg index="1" value="(uid={0})" />
    &lt;beans:constructor-arg index="2" ref="contextSource" />
    &lt;beans:property name="searchSubtree" value="true" />
  &lt;/beans:bean>

         &lt;beans:bean id="UserGroupLdapAuthoritiesPopulator" class="org.opennms.web.springframework.security.UserGroupLdapAuthoritiesPopulator"> 
           &lt;beans:constructor-arg ref="contextSource"/> 
           &lt;beans:constructor-arg value="ou=group"/> 
           &lt;beans:property name="searchSubtree" value="true" /> 
           &lt;beans:property name="groupRoleAttribute" value="cn" /> 
           &lt;beans:property name="groupSearchFilter" value="memberUid={1}" /> 
           &lt;beans:property name="groupToRoleMap"> 
             &lt;beans:map> 
               &lt;beans:entry> 
                 &lt;beans:key>&lt;beans:value>opennms&lt;/beans:value>&lt;/beans:key> 
                 &lt;beans:list> 
                   &lt;beans:value>ROLE_USER&lt;/beans:value> 
                 &lt;/beans:list> 
               &lt;/beans:entry> 
               &lt;beans:entry> 
                 &lt;beans:key>&lt;beans:value>opennms_admin&lt;/beans:value>&lt;/beans:key> 
                 &lt;beans:list> 
                   &lt;beans:value>ROLE_ADMIN&lt;/beans:value> 
                   &lt;beans:value>ROLE_USER&lt;/beans:value> 
                 &lt;/beans:list> 
               &lt;/beans:entry> 
             &lt;/beans:map> 
           &lt;/beans:property> 
         &lt;/beans:bean>


&lt;!-- LDAP End -->

  &lt;!-- ====================== RADIUS AUTHENTICATION ===================== -->
[edit] Troubleshooting
Enable DEBUG in log4j.properties for
log4j.category.org.springframework.security=DEBUG, MISC
log4j.category.OpenNMS.Spring=DEBUG, SPRING
log4j.category.org.springframework=DEBUG, SPRING
Don't forget to disable DEBUG after you finished your tests!
Search for LDAP server IP, userid's / groups or IP address of authenticating clients in spring.log and misc.log.
Look out for Granted Authorities: ROLE_USER. If there are other roles too there is a possibility to gain admin rights for your OpenNMS system by providing the roles in LDAP / AD.
You might try to add the following bean definitions to your applicationContext-springSecurity.xml file.
This bean provides authentication information:
&lt;beans:bean id="authenticationLoggerListener" class="org.springframework.security.authentication.event.LoggerListener" />
This bean provides authorisation information (if you need it):
&lt;beans:bean id="authorisationLoggerListener" class="org.springframework.security.access.event.LoggerListener" />
For authorisation bean, failures are logged at 'WARN' level, success events are logged at 'INFO' level. So if you need success authorisation events, change the logging level to 'INFO'.
[edit] List of Roles
ROLE
DESCRIPTION
ROLE_USER
OpenNMS User &lt;== Must be included in with each group.
ROLE_ADMIN
OpenNMS Administrator
ROLE_READONLY
OpenNMS Read-Only User
ROLE_DASHBOARD
OpenNMS Dashboard User
ROLE_RTC
OpenNMS RTC Daemon
ROLE_PROVISION
OpenNMS Provision User
ROLE_REMOTING
OpenNMS Remote Poller User


Retrieved from "http://www.opennms.org/wiki/Spring_Security_and_LDAP"

</Text>
        </Document>
        <Document ID="81">
            <Title>SNMP Monitor</Title>
            <Text>Purpose
The SNMP monitor enables the OpenNMS poller to request the value of an arbitrary MIB object from an SNMP agent, compare the retrieved value against a flexible set of rules, and based on the evaluation declare a service to be up or down.
[edit] Configuration Overview
In order to use the SNMP monitor to check out values for particular SNMP OIDs, you'll need to first configure capsd-configuration.xml to discover the service, or use the model importer or provisioning groups to provision the service on one or more of a node's interfaces. You'll then configure the poller to monitor the service.
[edit] Configuration Examples
[edit] Provisioning via Capsd
The default usage of the SNMP plugin in capsd-configuration.xml looks like this:
        &lt;protocol-plugin protocol="SNMP" class-name="org.opennms.netmgt.capsd.plugins.SnmpPlugin" scan="on" user-defined="false">
                &lt;property key="timeout" value="2000"/>
                &lt;property key="retry" value="2"/>
        &lt;/protocol-plugin>
[edit] Custom Example: Logged In Users
Suppose you wanted to make sure at least one user was logged into a system at a time. You could use hrSystemNumUsers (.1.3.6.1.2.1.25.1.5.0) to test this:
        &lt;protocol-plugin protocol="NumUsers" class-name="org.opennms.netmgt.capsd.plugins.SnmpPlugin" scan="on" user-defined="false">
                &lt;property key="timeout" value="2000"/>
                &lt;property key="retry" value="2"/>
		&lt;property key="vbname" value=".1.3.6.1.2.1.25.1.5.0"/>
        &lt;/protocol-plugin>
This plugin will test if the object exists, and if so it will assign the NumUsers service to the IP address. You can also test for a specific value (vbvalue). This would be useful if there is an OID that indicates if a service is active, such as ipForwarding that indicates a device is a Router:
[edit] Custom Example: IP Forwarding Enabled
        &lt;protocol-plugin protocol="Router" class-name="org.opennms.netmgt.capsd.plugins.SnmpPlugin" scan="on" user-defined="false">
                &lt;property key="vbname" value=".1.3.6.1.2.1.4.1.0"/>
                &lt;property key="vbvalue" value="1"/>
                &lt;property key="timeout" value="2000"/>
                &lt;property key="retry" value="2"/>
        &lt;/protocol-plugin>
[edit] Configuring Service Monitoring
Once the service has been provisioned, a monitor can be added to poller-configuration.xml. Again, the default service definition for this monitor looks like this (note that it's turned off by default, which is why the SNMP service shows as Not Monitored in the node details):
                &lt;service name="SNMP" interval="300000" user-defined="false" status="off">
                        &lt;parameter key="retry" value="2"/>
                        &lt;parameter key="timeout" value="3000"/>
                        &lt;parameter key="port" value="161"/>
                        &lt;parameter key="oid" value=".1.3.6.1.2.1.1.2.0"/>
                &lt;/service>
...
        &lt;monitor service="SNMP"         class-name="org.opennms.netmgt.poller.monitors.SnmpMonitor"/>
[edit] Do Not Forget The Monitor Definition!
Note well the presence of the &lt;monitor> element near the bottom of the file. Each &lt;service> must have a corresponding &lt;monitor> so that the poller will know what poller monitor class to use to monitor that service.
[edit] Custom Example: Users Logged In
For the NumUsers service to see if at least one person is logged in:
                &lt;service name="NumUsers" interval="300000" user-defined="false" status="on">
                        &lt;parameter key="retry" value="2"/>
                        &lt;parameter key="timeout" value="3000"/>
                        &lt;parameter key="port" value="161"/>
                        &lt;parameter key="oid" value=".1.3.6.1.2.1.25.1.5.0"/>
                        &lt;parameter key="operator" value="&amp;gt;="/>
                        &lt;parameter key="operand" value="1"/>
                &lt;/service>
Note that there are two parameters that allow you to customize the comparison performed on the value returned from the SNMP agent: operator and operand. The operator can be one of:
	•	Less than: "&lt;" (you will need to use an entity &amp;lt;)
	•	Greater than: ">" (&amp;gt;)
	•	Less than or equals: "&lt;=" (&amp;lt;)
	•	Greater than or equals: ">=" (&amp;gt;=)
	•	Equals: "="
	•	Does not equal: "!="
	•	Matches regular expression: "~"
The last value is used when the OID returns a string.
Be sure to add a corresponding monitor line at the bottom of the poller-configuration.xml file when adding new monitors:
        &lt;monitor service="NumUsers"         class-name="org.opennms.netmgt.poller.monitors.SnmpMonitor"/>
[edit] Get Fuzzy: The walk parameter
If you don't know the exact object identifier for the MIB object whose value you wish to compare against your criteria, you can add the walk parameter to your service definition with a value of true. This setting causes the monitor to send a GETNEXT request instead of a directed GET request to the SNMP agent. For example, you could use this setting to check whether a device supports any objects in the ASTERISK-MIB:
&lt;service name="Asterisk_MIB" interval="300000" user-defined="false" status="on">
 &lt;parameter key="oid" value=".1.3.6.1.4.1.22736.1" />
 &lt;parameter key="walk" value="true" />
&lt;/service>
[edit] Using the match-all parameter
If you want to verify that every row of a conceptual MIB table has a value in a given column that matches your criteria, you can use the walk parameter (described above) along with the match-all parameter set to a value of true. For example, you could use this setting to check that every IP route in a node's routing table is statically configured (ipRouteProto has a value of local(2)):
&lt;service name="All_Static_Routes" interval="300000" user-defined="false" status="on">
 &lt;parameter key="oid" value=".1.3.6.1.2.1.4.21.1.9" />
 &lt;parameter key="operator" value="=" />
 &lt;parameter key="operand" value="2" />
 &lt;parameter key="walk" value="true" />
 &lt;parameter key="match-all" value="true" />
&lt;/service>
The default behavior for match-all is true. If you would like see if minimum one entry exist, set match-all to false. The first match with your operand set the monitor to Up.
[edit] Matching a Bounded Number of Rows
As a special case of checking the same column for many rows of a conceptual MIB table, you can set the value of the match-all parameter to count and add a minimum and/or maximum parameter to bound the number of rows that must meet the specified criteria. Extending the static routes example above, you could require that at least three but fewer than ten static routes exist in a node's IP routing table:
&lt;service name="All_Static_Routes" interval="300000" user-defined="false" status="on">
 &lt;parameter key="oid" value=".1.3.6.1.2.1.4.21.1.9" />
 &lt;parameter key="operator" value="=" />
 &lt;parameter key="operand" value="2" />
 &lt;parameter key="walk" value="true" />
 &lt;parameter key="match-all" value="count" />
 &lt;parameter key="minimum" value="3" />
 &lt;parameter key="maximum" value="10" />
&lt;/service>
[edit] Customizing Reason Codes
Note: this feature is available in stable releases 1.6.3 and later, development releases 1.7.1 and later
When the SNMP monitor detects that a service is down according to the configured criteria, it tries to create a human-readable reason code. The default reason codes are quite generic, as this monitor can be used to check the status of an endless variety of values. You can specify a template to be used when creating the reason code by adding a reason-template parameter to your service definition. This feature can make the reason codes for outages more immediately understandable in the context in which you're using the SNMP monitor. For our logged in users example, we might use a reason code template like this one:
&lt;parameter key="reason-template" value="Users logged in must be ${operator} ${operand} but actual value was ${observedValue}" />
Assuming the example service definition above were in effect, but the monitor found that hrSystemNumUsers had a value of zero, the resulting reason code would read:
Users logged in must be >= 1 but actual value was 0
The following tokens, if present in the form ${foo} in the reason code template, will be expanded to the values configured for the corresponding parameters in the service definition:
	•	oid
	•	operator
	•	operand
	•	walk
	•	matchAll (actual parameter key is match-all, camel case conversion is XML convention)
	•	minimum
	•	maximum
	•	timeout
	•	retry (retries is a synonym)
	•	port
The following additional tokens, if present, will be expanded to their runtime values:
ipaddr 
The IP address of the interface on which the service is being polled
observedValue 
The actual value (as a string) returned by the underlying SNMP operation. Note that this token usually does not make sense to use if the "walk" param is set to "true" or if the "match-all" param is set to "count".
matchCount 
The number of matching rows found. Set only if the "match-all" param is set to "count".

[edit] Version History/Availability
	•	This feature was added in version 1.1.5
	•	This feature was enhanced or modified in version 1.3.0
</Text>
        </Document>
        <Document ID="124">
            <Title>LDAP</Title>
        </Document>
        <Document ID="77">
            <Title>Filter</Title>
            <Text>Think of the filter (also referred to as a rule) as a shorthand SQL statement that automatically selects ipaddr from the ipinterface table. You are basically building the WHERE clause with the rule that you write. There is an XML file called database-schema.xml in the /opt/opennms/etc directory that informs the filter code of what tables it can use in building the WHERE clause. Here is an explaination of that file:
Each table used by the filter code is in a &lt;table> tag. If a table has an attribute of 'visible=false' then no columns in that table can be used in the WHERE clause and thus can't appear in the rule. You will get some sort of syntax exception if it sees any non-visible columns in the rule. The same applies to a non-visible column in a table.
A &lt;join> tag tells the filter module how to relate this table to the ipinterface table. The ipinterface table is marked with the attribute 'key=primary', meaning that it is going to be the table we select ipaddr from and join all other tables to.
In brief, you can use C / Java-style comparison operators to data types they apply to ( == and != can be used on strings, as can the SQL LIKE keyword).
For LIKE comparisons, the character _ matches any single character and % matches any series of characters (or none at all). For example, 'F_o%' matches 'Foo', 'Foom' and 'Flowers' but not 'Foip'.
For handling NULL values (which includes cases where you've joined across to a table where there is no matching row), the IS NULL and IS NOT NULL operators can be used. It's important to note that comparing a null value to anything with any other operator always returns false, so
categoryName != 'SomeCategory'
will not return you anything with a null categoryName. Instead you probably need to use
categoryName != 'SomeCategory' | categoryName IS NULL
You can group expressions using parentheses and apply boolean operators anywhere in an expression. Note that, in a departure from C / Java convention, boolean operators are single characters rather than double, so they look more like the bitwise arithmetic operators in C:
AND 
&amp;
OR 
|
NOT 
 !
Each comparision is joined together with the &amp; or | operators meaning logical AND, logical OR operations. Anything delimited by an &amp; or | character gets translated into a sub-select that selects the ipaddr based on the comparision for that clause. Here is an example:
Rule:
(nodesysname == 'something') &amp; (snmpifdescr == 'something else')
SQL:
SELECT ipaddr
FROM ipinterface
WHERE ipaddr in (SELECT ipaddr FROM ipinterface, node
                 WHERE nodesysname='something'
                 AND ipinterface.nodeid =node.nodeid)
AND   ipaddr in (SELECT ipaddr FROM ipinterface, snmpInterface
                 WHERE snmpifdescr='something else'
                 AND ipinterface.ipaddr = snmpInterface.ipaddr);
The IPLIKE function is a short hand to call a Postgres function that was written in C to compare ipaddresses using *, lists and ranges. is&lt;Service> is a shorthand to build a complicated join to match on a service name. notis&lt;Service> can be used since 1.2.7 or so.
[edit] Components of openNMS that use filters
notifd 
Notification rules control whether or not a received event triggers a notification. Each event is tested against the rules in notifications.xml looking for a match. When a match is found, the corresponding notification is sent. The default for most notifications is "IPADDR IPLIKE *.*.*.*", however, any valid rule may be used. To do an exclusive filter use "!(IPADDR IPLIKE 169.254.*.*)". This allows you to be very granular when defining what to alert on.
collectd 
Controls which IP interfaces are included in a collection package. Evaluated at startup.
pollerd 
Controls which IP interfaces are included in a polling package. Evaluated at startup.
rtcd 
Controls which IP interfaces are included in the various categories that are shown on the front page in the webUI. Evaluated at startup and when a "uei.opennms.org/nodes/assetInfoChanged" event is received (usually generated when asset information is changed in the webUI).
threshd 
Controls which IP interfaces are included in a thresholding package. Evaluated at startup.
webUI 
The webUI uses filters to validate rules when configuring notifications. This insures that the rule will be syntactically correct, and it allows the administrator to preview the list of matching interfaces before finalizing the notification.
Filters are also used with rules when configuring path outages to select a list of nodes that lie behind the specified outage path. This list may be previewed before finalizing the configuration.
[edit] So How do I use them?
Rules can be much richer than simply filtering by IP, you can use parameters from nodes, categories etc. Parameters not present in database-schema.xml can be added for use in filter expressions, as long as the existing database table hierarchy is followed. Here is a partial list:
Pollers 
dpNumber
dpName
dpIP
dpComment
dpDiscLimit
dpAdminState
dpRunState
Nodes 
nodeID
dpName
nodeCreateTime
nodeParentID
nodeType
nodeSysOID
nodeSysName
nodeSysDescription
nodeSysLocation
nodeSysContact
nodeLabel
Catagories 
categoryID
categoryName
categoryDescription
Applications
name - This field is used for alerting on user defined Applications, that span multiple systems and services.
IpInterface 
ipAddr
ipHostname
isManaged
ipStatus
ipLastCapsdPoll
SNMP Interface
snmpIpAdEntNetMask
snmpPhysAddr
snmpIfIndex
snmpIfDescr
snmpIfType
snmpIfSpeed
snmpIfAlias (from 1.6.6 and 1.7.4)
snmpIfAdminStatus
snmpIfOperStatus
Services 
serviceName
ifServices 
serviceID
lastGood
lastFail
Server Map 
serverName
Service Map 
serviceMapName
Assets 
displayCategory
notifyCategory
pollerCategory
thresholdCategory
category
manufacturer
vendor
modelnumber
serialnumber
description
circuitid
assetnumber
operatingsystem
rack
slot
port
region
division
department
address1
address2
city
state
zip
building
floor
room
vendorphone
vendorfax
vendorassetnumber
lease
leaseexpires
supportphone
maintcontract
maintcontractexpires
comment
managedobjectinstance
managedobjecttype
[edit] Configuration examples
There are at least two formats which these rules can exist on the xml level (GUI format to follow).
In this first example, the entire rule is wrapped in "&lt;![CDATA[...]]&gt;" so that ampersands ("&amp;") do not have to be escaped (the CDATA bits are in bold and in blue):
&lt;rule>&lt;![CDATA[(IPADDR != '0.0.0.0') &amp; (IPADDR IPLIKE 192.168.1.1-154) &amp; (isSMTP | isPOP3 ) &amp; (categoryName == 'Production')]]&gt;&lt;/rule>
In this example, instead of using the CDATA construct above, the ampersands are escaped as "&amp;amp;" (in bold and in blue):
&lt;rule>(IPADDR != '0.0.0.0' &amp;amp; (IPADDR IPLIKE 192.168.1.1-154) &amp;amp; (isSMTP | isPOP3 ) &amp;amp; (categoryName == 'Production'))&lt;/rule>
For the GUI you simply drop in the unescaped value into the text field:
(IPADDR != '0.0.0.0' &amp; (IPADDR IPLIKE 192.168.1.1-154) &amp; (isSMTP | isPOP3 ) &amp; (categoryName == 'Production'))
catinc
Sometimes you need to include hosts belonging to more than one Category, via an AND operator. eg. All hosts that belong to BOTH Production and Linux groups need to be included. It is not currently possible to do this using any variation of eg. (categoryName == 'Production') &amp; (categoryName == 'Linux').
The "catinc" function is used the following way. Note Category names can not have spaces for this to work:
&lt;rule> &lt;![CDATA[((IPADDR != '0.0.0.0') &amp; catincProduction &amp; catincLinux)]]&gt; &lt;/rule>
</Text>
        </Document>
        <Document ID="82">
            <Title>LinkD</Title>
            <Text>Linkd!
Linkd is layer 2 iso/osi model network topology discovery daemon.

[edit] Availability
This feature is available in the "trunk" first appears in the 1.3.2 unstable release. This notes apply for 1.3.8 unstable release.
[edit] Quick Start
Opennms by default has linkd disabled, so you should enable it.
To enable linkd follow these simple steps:
	•	Go to service-configuration.xml and uncomment the following lines:
 &lt;service>
        &lt;name>OpenNMS:Name=Linkd&lt;/name>
        &lt;class-name>org.opennms.netmgt.linkd.jmx.Linkd&lt;/class-name>
                &lt;invoke at="start" pass="0" method="init"/>
                &lt;invoke at="start" pass="1" method="start"/>
                &lt;invoke at="status" pass="0" method="status"/>
                &lt;invoke at="stop" pass="0" method="stop"/>
 &lt;/service>
	•	Restart openNMS, you can follow the progress of Linkd in the link.log
The inital Linkd discovery will take a bit as each nodes topology is discovered. The linkd daemon collects a lot of data from nodes to perform the network topology discovery, this should affect performance, so if you are running opennms with default memory allocation remeber to increase JAVA_HEAP_SIZE by 64Mbyte for each 5 threads used. By default linkd uses 5 thread to do the work.
Links and data collected by Linkd can be visualized in maps and in WEB UI element section.

[edit] Concept
Linkd Daemon is controlled by a '"Configuration File linkd-configuration.xml (located in the $OPENNMS_BASE/etc directory.) and do its work in two steps:
A) Performs SNMP data collection from nodes (Java Class SnmpCollection) for each node included in packages for each package
B) Discover layer 2 network topology using SNMP collected data (Java Class DiscoveryLink) for each packages among node included in package.
 The SNMP data collection regards the following MIBS:
.iso.org.dod.internet.mgmt.mib-2.ip.ipRouteTable
Snmp Table that holds node's ip routing tables. These informations are saved by default in Database table ipRouteInterface

.iso.org.dod.internet.mgmt.mib-2.ip.ipNetToMediaTable
Snmp Table that contains node's ip address to mac address binding. These informations are saved by default in Database table atInterface

.iso.org.dod.internet.private.enterprises.cisco.ciscoMgmt.ciscoCdpMIB.ciscoCdpMIBObjects.cdpCache.cdpCacheTable Snmp Table that contains node's Cisco Discovery Protocol cdp Cache Table with all the data about Cisco neighbourgh devices.

For the nodes whose sysoid matches the sysoids supported and listed in the configuration file is collected the Snmp Vlan table that holds information on Vlans.

.iso.org.dod.internet.mgmt.mib-2.dot1dBridge.dot1dBase
.iso.org.dod.internet.mgmt.mib-2.dot1dBridge.dot1dStp
 These MIBS have information about node's Bridge and Spanning Tree Protocol. The data collected regards information about nodes and its interfaces. This data is collected for each active and operative Vlan for Cisco and intel switches to support multi vlan STP.
The node's general data is saved by default in Databasetable StpNode and the node's interface data is saved by default in Database table StpInterface.
 .iso.org.dod.internet.mgmt.mib-2.dot1dBridge.dot1dTp
This MIBS contains information on Bridge forwarding tables. That means how the bridge forward a packet looking at destination mac address. This table mantains informations on which switch port a packet must be forwarded. It's an association among bridge ports and mac addresses. This data is collected for each active and operative Vlan for Cisco and intel switches to support multi vlan STP.
Extreme Network switches put the bridge forwarding table under another tree MIB. Linkd in case the dot1dTp is clear try to download the QBridgeDot1dTp using the following Root OID: .1.3.6.1.2.1.17.7.1.2.2 Collection to collect specific snmp data (that we'll see depends on the package configuration). For each Package is scheduled a thread that uses DiscoveLink to find links among nodes.
 The Discovery Link process find links among nodes using several option and configuration, the default link discovery uses the following steps:
	•	Try to use Cisco Discover Protocol Cache Table to get links among Cisco Nodes. For nodes found on ethernet segments there is a test to be sure that the link is a true link (in fact for non CDP devices CDP acts as multicast so it's broadcasted on all ports). For example: you have a Cisco Router Ethernet port connected with a 3Com Switch that is connected to a Switch Cisco Catalyst, the CDP cache table of the Router and of the switch too should have an entry that says that they are nearest CDP devices. From the CDP point of view this information is correct but from network topology discovery this is wrong becouse the link is between Cisco Router and 3Com and between 3Com and Cisco Switch. So DiscoveryLink verify that the guessed link did not have other devices between. This is done just looking to the intersection of the Bridge Forwarding Table on the two bridge ports, if this intersection doesn,t contains a recognized bridge mac address the link is accepted.
	•	Try to use the Bridge Snmp Info and the IpNetToMediaTable to find links on Ethernet LAN segments. This process starts using Spanning Tree Protocol node tables to find links among switches. Againg STP is like CDP a multicast protocol so if you have two STP enabled connected via a non STP enabled device the information you get from is of a link between the two STP devices. So again verify that the guessed link did not have other devices between. After STP link discovery the bridge forwarding table is used to find links. In such a manner you get all the mac addresses that are "linked" to a bridge port. Using the IpNetToMedia tables you find the ip address that is binded to mac address. From the Ip address you get the nodeid. A lot of special cases appears all related to the specific network configuration. For example you can have unmanaged devices connected to managed Snmp devices, DiscoveryLink finds all the nodes connected on the unmanaged device to the Bridge port on which the device is connected to the managed once.
	•	Try to use Ip Routing Tables to find not ethernet links among nodes
 DiscoveryLink process save discovered links (the topology) into database table datalinkinterface.
 Each db table (atInterface, stpNode, stpInterface,vlan, ipRouteInterface, DataLinkInterface) row has a status tag that signify the status of the row.
"A" means Active (that means that linkd has this information updated)
"N" means Not Polled(that means that linkd has this information but was not lastly updated)
"D" means Deleted (that means that linkd has not more this information)
[edit] TIPS
Network topology discovery on ethernet Lans is done using the bridge and ipnettomedia tables. So if a complete discovery is needed it's necessary to have all these data. The bridge table is achieved just managing and enabling SNMP protocols on each layer 2 device. The information on ipnettomedia is shared by nodes on the network. Usually a almost complete table is found on the lans default gateway.
'To full discovery network topology it's recommended that every network device in the area network works using SNMP.
In an multivlan enrivirroment to resolve ip to mac address it is required that layer 3 switches shold be accessed via SNMP.
It is also suggested to enable SNMP on OpenNMS server to get ip to mac addresses ipnettomedia table.

[edit] Daemon Configuration
The Linkd daemon is controlled by The Linkd Configuration File linkd-configuration.xml (located in the $OPENNMS_BASE/etc directory.)
Here you see the default configuration distributed.
&lt;?xml version="1.0" encoding="ISO-8859-1"?>
&lt;linkd-configuration threads="5" initial_sleep_time="3600000"
                     snmp_poll_interval="18000000" discovery_link_interval="300000">
    
    
    &lt;!--
    
    Here is a list of sys oids and corresponding classes to get 
    SNMP vlan info from nodes.  
    Verify that your switch devices sys oids are in the following lists
    if you want to get informations on Vlans!
    Otherwise:
    Put your own sys oids in the proper vendor tag and specify sys oid
    using &lt;specific> and &lt;include-range> tags 
    
    -->
    
    &lt;vlans>

        &lt;vendor vendor_name="3Com" sysoidRootMask=".1.3.6.1.4.1.43"
                class-name="org.opennms.netmgt.linkd.snmp.ThreeComVlanTable">
            &lt;specific>1.9.13.3.1&lt;/specific>
            &lt;specific>10.27.4.1.2.2&lt;/specific>
            &lt;specific>10.27.4.1.2.4&lt;/specific>
            &lt;specific>10.27.4.1.2.11&lt;/specific>
            &lt;specific>1.16.4.3.5&lt;/specific>
            &lt;specific>1.16.4.3.6&lt;/specific>
        &lt;/vendor>

        &lt;vendor vendor_name="3Com3870" sysoidRootMask=".1.3.6.1.4.1.43.1"
                class-name="org.opennms.netmgt.linkd.snmp.Dot1qStaticVlanTable">
            &lt;specific>8.43&lt;/specific>
        &lt;/vendor>

        &lt;vendor vendor_name="Nortel" sysoidRootMask=".1.3.6.1.4.1.45.3"
                class-name="org.opennms.netmgt.linkd.snmp.RapidCityVlanTable">
            &lt;specific>61.1&lt;/specific>
            &lt;specific>35.1&lt;/specific>
            &lt;specific>53.1&lt;/specific>
        &lt;/vendor>

        &lt;vendor vendor_name="Intel" sysoidRootMask=".1.3.6.1.4.1.343.5"
                class-name="org.opennms.netmgt.linkd.snmp.IntelVlanTable">
            &lt;specific>1.5&lt;/specific>
        &lt;/vendor>

        &lt;vendor vendor_name="HP Networks" sysoidRootMask=".1.3.6.1.4.1.11.2.3.7"
                class-name="org.opennms.netmgt.linkd.snmp.Dot1qStaticVlanTable">
            &lt;specific>11.1&lt;/specific>
            &lt;specific>11.3&lt;/specific>
            &lt;specific>11.13&lt;/specific>
            &lt;specific>11.14&lt;/specific>
            &lt;specific>11.18&lt;/specific>
            &lt;specific>11.19&lt;/specific>
            &lt;specific>11.20&lt;/specific>
            &lt;specific>11.27&lt;/specific>
            &lt;specific>11.23&lt;/specific>
            &lt;specific>11.29&lt;/specific>
            &lt;include-range begin="11.6" end="11.11"/>
        &lt;/vendor>

        &lt;vendor vendor_name="cisco" sysoidRootMask=".1.3.6.1.4.1.9"
                class-name="org.opennms.netmgt.linkd.snmp.CiscoVlanTable">
            &lt;specific>1.217&lt;/specific>
            &lt;specific>1.218&lt;/specific>
            &lt;specific>1.221&lt;/specific>
            &lt;specific>1.247&lt;/specific>
            &lt;specific>1.248&lt;/specific>
            &lt;specific>1.283&lt;/specific>
            &lt;specific>1.300&lt;/specific>
            &lt;specific>1.324&lt;/specific>
            &lt;specific>1.366&lt;/specific>
            &lt;specific>1.367&lt;/specific>
            &lt;specific>5.46&lt;/specific>
        &lt;/vendor>

        &lt;vendor vendor_name="Extreme Networks" sysoidRootMask=".1.3.6.1.4.1.1916"
                class-name="org.opennms.netmgt.linkd.snmp.ExtremeNetworkVlanTable">
            &lt;specific>2.11&lt;/specific>
            &lt;specific>2.14&lt;/specific>
            &lt;specific>2.28&lt;/specific>
            &lt;specific>2.63&lt;/specific>
        &lt;/vendor>

    &lt;/vlans>

    &lt;!--
    It's possible in packages to overwrite all properties - (thread and initial_sleep_time) 
    in the linkd-configuration
    -->
    
    &lt;package name="example1">
		&lt;filter>IPADDR != '0.0.0.0'&lt;/filter>
		&lt;include-range begin="1.1.1.1" end="254.254.254.254"/>
    &lt;/package>
    
    &lt;!-- Use more packages with huge network-->

    &lt;!-- here is a configuration that is valid on ethernet LANs -->
    &lt;!-- 
    &lt;package name="LAN" use-ip-route-discovery="false">
		&lt;filter>IPADDR != '0.0.0.0'&lt;/filter>
		&lt;include-range begin="10.1.1.1" end="10.1.1.254"/>
    &lt;/package>
	-->

	&lt;!-- here is a configuration valid on WAN where linkd try to get
	     links using cisco discovery protolol first and then routers ip route tables -->
	 &lt;!--
    &lt;package name="WAN" use-bridge-discovery="false">
		&lt;filter>IPADDR != '0.0.0.0'&lt;/filter>
		&lt;include-range begin="10.1.1.1" end="10.1.1.254"/>
    &lt;/package>
	-->
	&lt;!-- here is a configuration in which you have to find links when using cross ethernet cables 
		among router. Linkd by default finds links on LAN using bridge forwarding table becouse it 
		guess that the node is linked to a switch port. By the way there are situations in which
		you can have some routers connected via cross ethernet cables. So this is a way to find that links.
	-->
	&lt;!--
    &lt;package name="LINUX-ETHERNET-ROUTERS" force-ip-route-discovery-on-ethernet="true">
		&lt;filter>IPADDR != '0.0.0.0'&lt;/filter>
		&lt;include-range begin="10.1.1.1" end="10.1.1.254"/>
    &lt;/package>
  	-->
    &lt;!-- do you want that linkd send a new suspect event for each ip address found on devices? -->
    &lt;!-- 
    &lt;package name="AUTODISCOVERY" auto-discovery="true">
		&lt;filter>IPADDR != '0.0.0.0'&lt;/filter>
		&lt;include-range begin="10.1.1.1" end="10.1.1.254"/>
    &lt;/package>
    
     -->    
&lt;/linkd-configuration>

Let's take a look at the configuration parameters:
Linkd uses packages as usual opennms daemon like poller and collectd. Each package has its own set of ip addresses and special configuration attributes.
There are four main required configuration attributes:
&lt;attribute name="threads" type="int" use="required">
The max number of threads used for snmp data collection on devices and discovery links.
This means that the linkd daemon has max 5 threads active performing basic operation.
This is only global setting.


&lt;attribute name="initial_sleep_time" type="long" use="required">
The initial sleep time in mill seconds before starting any operation.
This is only global setting.

&lt;attribute name="snmp_poll_interval" type="long" use="required">
This is the interval in mill seconds from two Snmp Collection on a node.
If specified in package overwrites attribute in root tag.

&lt;attribute name="discovery_link_interval" type="long" use="required">
Time interval in mill seconds from last snmp collection and link discovery.
If specified in package overwrites attribute in root tag.
Threads, Snmp_poll_interval and discovery_link_interval values are each other correlated and depends on the number of collection. If you have a lot of Snmp Collection you should increase threads otherwise increase snmp_poll_interval and discovery_link_interval. The last value should be setted according to the fact that a discovery of links should be done when data collection is finished. The snmp_poll_interval indicates how many times is network topology discovery is done.
Use for packages is strongly recommended especially for etherogeneus networks.
For example, becouse WAN topology is substantially static you should calculate topology once a day. In Lans maybe you want to run topology discovery every four hours.
Let's try to do some simple consideration on the definition of the previus attributes.
For example: consider a LAN with 10 switches and a router (default gateway), to get links you need to perform snmp data collection on 11 nodes. Suppose data collection requires 1 minute on each node. And suppose you want to run topology discovery every hour. Let suppose threads=5, then you can collect data for 5 nodes in 1 minutes. Becouse you want to collectd all the data before linkdiscovery starts you have to be sure that the data collection is finished in "discovery_link_interval" you need to increase discovery_link_interval to 300000 (5 minutes). But with our guess we can collect data for 25 nodes.
So following settings should be safe:
&lt;linkd-configuration threads="5" initial_sleep_time="600000"
                     snmp_poll_interval="3600000" discovery_link_interval="300000">

If we consider an envirroment with at least 100 switches and several routers we should increase threads or discovery_link_interval.
So the number of thread is just given by a simple formula: threads = (SnmpNodes) * (mean time to collect snmp data on node) / (discovery_link_interval)
In this case we have: 100 * 60000/300000 = 100/5=20
So a safe value for threads is "20". Otherwise you can increase discovery_link_interval to 600000!
Using a reverse formula we can calculate the number of nodes on which collect data from default configuration:
SnmpNodes= threads * discovery_link_interval / (mean time to collect snmp data on node) Guessing (mean time to collect snmp data on node) = 60000ms (1 minute) a simple calculation gives: nodes = 25
Of course it is recommended that discovery_link_interval &lt; snmp_poll_interval.
If your network is slow you should increase "mean time to collect snmp data on node", If your network is fast you can decrease "mean time to collect snmp data on node".
Remember that increasing threads requires an increment of JAVA_HEAP_SIZE as previously discussed.
Let's see what other option are configurable, everyone is optional in the configuration file but has a default value. If you need to overwrite the default values add the attribute in the configuration file. Here is listed the xsd file with the default value. You can find a brief description of what is done.
&lt;attribute name="auto-discovery" type="boolean" use="optional" default="false">
Whether Linkd should generate newSuspect events for ip addresses unknown to OpenNMS found on nodes snmp tables.

&lt;attribute name="enable-discovery-download" type="boolean" use="optional" default="false">

Whether Discovery Link should try to get 
specific snmp bridge info probably lost in snmp data collection.
Should be enabled only in very complex network
where snmpCollection often fails.
Enabling this flags slows the discovery process a lot.   
(The main fact about this flag is that DiscoveryLink uses SnmpGet against SnmpWalk used by SnmpCollection,
 this is useful when there are some non standard response from Snmp Agent)

&lt;attribute name="enable-vlan-discovery" type="boolean" use="optional" default="true">
Whether SnmpCollection has to collect snmp Vlan tables on devices.
The Vlan Table data collection is made using the classes defined in vlans elements. 

&lt;attribute name="use-cdp-discovery" type="boolean" use="optional" default="true">
Whether links discovery process should use 
cisco discovery protocol snmp tables.

&lt;attribute name="use-ip-route-discovery" type="boolean" use="optional" default="true">
Whether links discovery process should use ip route tables.

&lt;attribute name="use-bridge-discovery" type="boolean" use="optional" default="true">
						Whether links discovery process should use
						snmp bridge and spanning tree tables.

&lt;attribute name="save-route-table" type="boolean" use="optional" default="true">
Whether Linkd has to save snmp ip route tables to database

&lt;attribute name="save-stp-node-table" type="boolean" use="optional" default="true">
Whether Linkd has save bridge base informations to database.

&lt;attribute name="save-stp-interface-table" type="boolean" use="optional" default="true">
Whether Linkd has to save to database bridge interfaces informations to database.

&lt;attribute name="force-ip-route-discovery-on-ethernet" type="boolean" use="optional" default="false" >
Whether Linkd has to find links using ip route tables on ethernet interfaces.
By default discovery process skips ip route table for ethernet interfaces.
Finally we consider the vlans element that maps Vendor Devices Sysoid with the specific class to use to get snmp Vlan table.
In fact each vendor has its own specific snmp vlan table where to get vlan setting.
&lt;element ref="this:vlans" minOccurs="1" maxOccurs="1">
A section to get specific Vendors Vlan
informations that are used to get vlan tables on nodes.

 Once the general settings are in place the only thing left to tell the linkd is for which sysoids try to collects vlan and bridging infos. This is controlled by tags:
	•	vlans: specify a list of sysoids for which download SNMP vlan table.
	•	vendor: Allow you to specify a set of sysoids whose root is set by a vendor sysoidrootmask attribute and using the following tags.
	•	include-range: &lt;begin>start-sub-oid&lt;/begin>&lt;end>end-sub-oi&lt;/end>
	•	specific: sub-oid
	•	exclude-range: &lt;begin>start-sub-oid&lt;/begin>&lt;end>end-ip-address&lt;/end>
The vendor tags in the default configuration file are tested. If you have switches with different sysoid you have to find an appropriate vlan class to get Vlans and add a vendor tag for this.
 For example consider that you have a Cisco catalyst295024 whose sysoid is: 1.3.6.1.4.1.9.1.324
And you want to get Vlan info on that device. You should look under Cisco vendor tag if this sysoid is in place:
  &lt;vendor vendor_name="cisco" sysoidRootMask=".1.3.6.1.4.1.9"
                class-name="org.opennms.netmgt.linkd.snmp.CiscoVlanTable">
            &lt;specific>1.217&lt;/specific>
            &lt;specific>1.218&lt;/specific>
            &lt;specific>1.221&lt;/specific>
            &lt;specific>1.247&lt;/specific>
            &lt;specific>1.248&lt;/specific>
            &lt;specific>1.283&lt;/specific>
            &lt;specific>1.300&lt;/specific>
            &lt;specific>1.324&lt;/specific>
            &lt;specific>1.366&lt;/specific>
            &lt;specific>1.367&lt;/specific>
            &lt;specific>5.46&lt;/specific>
        &lt;/vendor>
and you see is in place.
if you want Vlan on cat6506 whose sysoid is: 1.3.6.1.4.1.9.1.282 you should add to cisco vendor tag:
&lt;specific>1.282&lt;/specific>
[edit] REMARK AutoDiscovery
AutoDiscovery Let linkd send a newSuspectEvent when an unknown ip address is found. For an issue, in 1.3.8 it is enabled by default. So if you don't need autodiscovery enabled you have to set auto-discovery="false" in linkd-configuration.xml. This notes applies only for 1.3.8. By default in following release autodiscovery is disabled.
[edit] WEB User interface
There are some JSP (like box in includes dir and pages in element dir) to take a look at the infos saved in db tables. Here you see the complete list of what is going to do:
A. mac address search in element/index.jsp
B. Include mac address info for element/interface.jsp
C. Include link to element/linkednode.jsp in element/node.jsp
D. Include link to element/routeipnode.jsp in element/node.jsp
E. Include link to element/bridgenode.jsp in element/node.jsp
F. Include box includes/nodebridge-box.jsp in element/node.jsp
G. Include box includes/noderouteInfo-box.jsp in element/node.jsp
H. Include box includes/nodeStpint-box.jsp in element/node.jsp
I.   Include box includes/interfaceLink-box.jsp in element/interface.jsp
L.  Include box includes/interfaceStp-box.jsp in element/interface.jsp
Links should also be visualized on maps
[edit] Troubleshooting
[edit] Link is not discovered
	•	is the sysoid of your device included in the configuration described above? Some devices using non-standard mib tables won't work. If the sysoid is not included but a similar device from the same vendor is referenced in the config try to add the sysoid of your device. If it works open an enhancement bug to get it included in future releases.
	•	is the ip address of your device included in a range in the configuration described above?
	•	is the snmp community configured in opennms? Go to the node page of your device in opennms and check if "SNMP Attributes" is visible on the node page
	•	can you snmp walk your device from your opennms server?
	•	If you can do an snmp walk grep in the output from the snmp walk for the MAC address of the device on the other side of the missing link. If the MAC is in there, and opennms has the snmp values of the device, linkd should discover the link.
	•	What is your intervall for linkd to run? Maybe you just have to wait for the next run of linkd. Check the log files when it is running.
	•	Is the device on the other end of the missing link always busy talking? If it is "quiet" while linkd collects the snmp tables it depends on the timeouts of the device where linkd tries to get the MAC tables - if the timeout for the MAC tables is too short the MAC will be thrown away and linkd can't find anything.
</Text>
        </Document>
        <Document ID="78">
            <Title>Custom Categories (SLA) Rules</Title>
            <Text>Think of the filter (also referred to as a rule) as a shorthand SQL statement that automatically selects ipaddr from the ipinterface table. You are basically building the WHERE clause with the rule that you write. There is an XML file called database-schema.xml in the /opt/opennms/etc directory that informs the filter code of what tables it can use in building the WHERE clause. Here is an explaination of that file:
Each table used by the filter code is in a &lt;table> tag. If a table has an attribute of 'visible=false' then no columns in that table can be used in the WHERE clause and thus can't appear in the rule. You will get some sort of syntax exception if it sees any non-visible columns in the rule. The same applies to a non-visible column in a table.
A &lt;join> tag tells the filter module how to relate this table to the ipinterface table. The ipinterface table is marked with the attribute 'key=primary', meaning that it is going to be the table we select ipaddr from and join all other tables to.
In brief, you can use C / Java-style comparison operators to data types they apply to ( == and != can be used on strings, as can the SQL LIKE keyword).
For LIKE comparisons, the character _ matches any single character and % matches any series of characters (or none at all). For example, 'F_o%' matches 'Foo', 'Foom' and 'Flowers' but not 'Foip'.
For handling NULL values (which includes cases where you've joined across to a table where there is no matching row), the IS NULL and IS NOT NULL operators can be used. It's important to note that comparing a null value to anything with any other operator always returns false, so
categoryName != 'SomeCategory'
will not return you anything with a null categoryName. Instead you probably need to use
categoryName != 'SomeCategory' | categoryName IS NULL
You can group expressions using parentheses and apply boolean operators anywhere in an expression. Note that, in a departure from C / Java convention, boolean operators are single characters rather than double, so they look more like the bitwise arithmetic operators in C:
AND 
&amp;
OR 
|
NOT 
 !
Each comparision is joined together with the &amp; or | operators meaning logical AND, logical OR operations. Anything delimited by an &amp; or | character gets translated into a sub-select that selects the ipaddr based on the comparision for that clause. Here is an example:
Rule:
(nodesysname == 'something') &amp; (snmpifdescr == 'something else')
SQL:
SELECT ipaddr
FROM ipinterface
WHERE ipaddr in (SELECT ipaddr FROM ipinterface, node
                 WHERE nodesysname='something'
                 AND ipinterface.nodeid =node.nodeid)
AND   ipaddr in (SELECT ipaddr FROM ipinterface, snmpInterface
                 WHERE snmpifdescr='something else'
                 AND ipinterface.ipaddr = snmpInterface.ipaddr);
The IPLIKE function is a short hand to call a Postgres function that was written in C to compare ipaddresses using *, lists and ranges. is&lt;Service> is a shorthand to build a complicated join to match on a service name. notis&lt;Service> can be used since 1.2.7 or so.
[edit] Components of openNMS that use filters
notifd 
Notification rules control whether or not a received event triggers a notification. Each event is tested against the rules in notifications.xml looking for a match. When a match is found, the corresponding notification is sent. The default for most notifications is "IPADDR IPLIKE *.*.*.*", however, any valid rule may be used. To do an exclusive filter use "!(IPADDR IPLIKE 169.254.*.*)". This allows you to be very granular when defining what to alert on.
collectd 
Controls which IP interfaces are included in a collection package. Evaluated at startup.
pollerd 
Controls which IP interfaces are included in a polling package. Evaluated at startup.
rtcd 
Controls which IP interfaces are included in the various categories that are shown on the front page in the webUI. Evaluated at startup and when a "uei.opennms.org/nodes/assetInfoChanged" event is received (usually generated when asset information is changed in the webUI).
threshd 
Controls which IP interfaces are included in a thresholding package. Evaluated at startup.
webUI 
The webUI uses filters to validate rules when configuring notifications. This insures that the rule will be syntactically correct, and it allows the administrator to preview the list of matching interfaces before finalizing the notification.
Filters are also used with rules when configuring path outages to select a list of nodes that lie behind the specified outage path. This list may be previewed before finalizing the configuration.
[edit] So How do I use them?
Rules can be much richer than simply filtering by IP, you can use parameters from nodes, categories etc. Parameters not present in database-schema.xml can be added for use in filter expressions, as long as the existing database table hierarchy is followed. Here is a partial list:
Pollers 
dpNumber
dpName
dpIP
dpComment
dpDiscLimit
dpAdminState
dpRunState
Nodes 
nodeID
dpName
nodeCreateTime
nodeParentID
nodeType
nodeSysOID
nodeSysName
nodeSysDescription
nodeSysLocation
nodeSysContact
nodeLabel
Catagories 
categoryID
categoryName
categoryDescription
Applications
name - This field is used for alerting on user defined Applications, that span multiple systems and services.
IpInterface 
ipAddr
ipHostname
isManaged
ipStatus
ipLastCapsdPoll
SNMP Interface
snmpIpAdEntNetMask
snmpPhysAddr
snmpIfIndex
snmpIfDescr
snmpIfType
snmpIfSpeed
snmpIfAlias (from 1.6.6 and 1.7.4)
snmpIfAdminStatus
snmpIfOperStatus
Services 
serviceName
ifServices 
serviceID
lastGood
lastFail
Server Map 
serverName
Service Map 
serviceMapName
Assets 
displayCategory
notifyCategory
pollerCategory
thresholdCategory
category
manufacturer
vendor
modelnumber
serialnumber
description
circuitid
assetnumber
operatingsystem
rack
slot
port
region
division
department
address1
address2
city
state
zip
building
floor
room
vendorphone
vendorfax
vendorassetnumber
lease
leaseexpires
supportphone
maintcontract
maintcontractexpires
comment
managedobjectinstance
managedobjecttype
[edit] Configuration examples
There are at least two formats which these rules can exist on the xml level (GUI format to follow).
In this first example, the entire rule is wrapped in "&lt;![CDATA[...]]&gt;" so that ampersands ("&amp;") do not have to be escaped (the CDATA bits are in bold and in blue):
&lt;rule>&lt;![CDATA[(IPADDR != '0.0.0.0') &amp; (IPADDR IPLIKE 192.168.1.1-154) &amp; (isSMTP | isPOP3 ) &amp; (categoryName == 'Production')]]&gt;&lt;/rule>
In this example, instead of using the CDATA construct above, the ampersands are escaped as "&amp;amp;" (in bold and in blue):
&lt;rule>(IPADDR != '0.0.0.0' &amp;amp; (IPADDR IPLIKE 192.168.1.1-154) &amp;amp; (isSMTP | isPOP3 ) &amp;amp; (categoryName == 'Production'))&lt;/rule>
For the GUI you simply drop in the unescaped value into the text field:
(IPADDR != '0.0.0.0' &amp; (IPADDR IPLIKE 192.168.1.1-154) &amp; (isSMTP | isPOP3 ) &amp; (categoryName == 'Production'))
catinc
Sometimes you need to include hosts belonging to more than one Category, via an AND operator. eg. All hosts that belong to BOTH Production and Linux groups need to be included. It is not currently possible to do this using any variation of eg. (categoryName == 'Production') &amp; (categoryName == 'Linux').
The "catinc" function is used the following way. Note Category names can not have spaces for this to work:
&lt;rule> &lt;![CDATA[((IPADDR != '0.0.0.0') &amp; catincProduction &amp; catincLinux)]]&gt; &lt;/rule>
</Text>
        </Document>
        <Document ID="83">
            <Title>Syslog</Title>
            <Text>Functionality
Syslogd allows OpenNMS to receive syslog datagrams and turn them into events, and then into alarms and/or notifications, much as Trapd does for SNMP traps.
[edit] Availability
This feature is available in stable releases since 1.6.0, and first appeared in the 1.3.2 unstable release. Functionality with substring-based event UEI matching and message hiding (for messages containing sensitive data) was introduced in release 1.3.7. Additional enhancements to enable regular expression-based matching and extraction of regex match-groups as event parameters are available from the 1.3.8 release.
[edit] Cascading
Syslogd up until 1.3.7 supported BSD style forwarding. The 1.3.8 release will attempt to match a nodename to the database, do hostname lookups via the resolver, and is designed to work better with syslog-ng.
	•	The regular expression used to extract the originating hostname and other message metadata is defined in the syslogd-configuration.xml file in the forwarding-regexp attribute of the &lt;configuration> element.
	•	The default setup is for a BSD style syslog. Depending on how messages are cascaded, this can easily be changed by altering the forwarding-regexp.
	•	The example configuration below is from a cascaded syslog-ng + stunnel setup.
[edit] Configuration
&lt;?xml version="1.0"?>
&lt;syslogd-configuration>
    &lt;configuration
            syslog-port="10514"
            new-suspect-on-message="true"
            forwarding-regexp="^((.+?) (.*))\r?\n?$"
            matching-group-host="2"
            matching-group-message="3"
            />


    &lt;ueiList>
        &lt;ueiMatch>
            &lt;match type="substr" expression="CRISCO"/>
            &lt;uei>uei.opennms.org/tests/syslogd/substrUeiRewriteTest&lt;/uei>
        &lt;/ueiMatch>
        &lt;ueiMatch>
            &lt;match type="regex" expression=".*?foo: (\d+) out of (\d+) tests failed for (\S+)$"/>
            &lt;uei>uei.opennms.org/tests/syslogd/regexUeiRewriteTest&lt;/uei>
        &lt;/ueiMatch>
    &lt;/ueiList>

    &lt;hideMessage>
        &lt;hideMatch>
            &lt;match type="substr" expression="TEST"/>
        &lt;/hideMatch>
        &lt;hideMatch>
            &lt;match type="regex" expression="[Dd]ouble secret"/>
        &lt;/hideMatch>
    &lt;/hideMessage>

&lt;/syslogd-configuration>

The &lt;configuration> element specifies global parameters that specify how Syslogd will receive and process syslog messages. This element has the following attributes:
	•	Port, default syslogd port to listen on
	•	New suspect on messages specifies whether Syslogd will inject a newSuspect event when it receives a syslog message originating from a host that cannot be resolved to an existing node managed by OpenNMS. This functionality is equivalent to the same option in Trapd. New suspects will be queued for discovery by Capsd.
	•	The 'Cascading' (forwarding) magic - these parameters define how cascaded or forwarded syslog messages are found and deconstructed
	◦	forwarding-regexp specifies a regular expression used to identify forwarded syslog messages
	◦	matching-group-host specifies which match-group from the forwarding-regexp contains the originating hostname
	◦	matching-group-message specifies which match-group from the forwarding regexp contains the body of the syslog message
	•	A description of regular expressions you can find at http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html
[edit] Discarding Messages
Since stable release 1.6.3 and development release 1.7.1, it is possible to configure Syslogd to discard certain messages without ever creating events. To configure this behavior, simply change the value of the &lt;uei> element inside a &lt;ueiMatch> element to match the value of the discard-uei attribute in the &lt;configuration> element of syslogd-configuration.xml. The default value of this attribute is DISCARD-MATCHING-MESSAGES.
[edit] Integration with syslog-ng
To integrate with syslog-ng, add this to your syslog-ng configuration:
destination opennms {udp("127.0.0.1" port(10514));};
log {source(s_all);destination(opennms);};
If you have problems getting the correct hostname into the opennms events (by example all syslog events are marked as being from "localhost") you might try to add the following option to your syslog-ng configuration:
options { chain_hostnames(no) };
This will create hostnames in the log entry like "routername" instead of "routername/routername". Beware if you have other applications depending on the syslog format!
Another possibility would be to change the OpenNMS syslogd-configuration from the default
...
  matching-group-host="1"
  matching-group-message="2"
...
(which will match the format of the "normal" syslog daemons) to
...
  matching-group-host="2"
  matching-group-message="3"
...
Check the value of your regexp - for syslog-ng, it is recommended to use a basic regexp as follows:
...
            forwarding-regexp="^((.+?) (.*))\n?$"
...
If you still have problems go to Problem solving
[edit] Example Custom Event
    &lt;event>
        &lt;uei>uei.mycompany.net/generic/smartd/DiskLost&lt;/uei>
        &lt;event-label>Smartd Disk could not be inserted&lt;/event-label>
        &lt;descr>
                An event indicating a lost disk
        &lt;/descr>
        &lt;logmsg dest='logndisplay'>Smartd, disk insert failure:%id% (%id%) args(%parm[##]%):%parm[all]%&lt;/logmsg>
        &lt;severity>Critical&lt;/severity>
        &lt;alarm-data reduction-key="%uei%:%dpname%:%nodeid%" alarm-type="1" />
    &lt;/event>
[edit] Activation
To activate this, uncomment the parts in service-configuration.xml where applicable.
        &lt;service>
                &lt;name>OpenNMS:Name=Syslogd&lt;/name>
                &lt;class-name>org.opennms.netmgt.syslogd.jmx.Syslogd&lt;/class-name>
                &lt;invoke at="start" pass="0" method="init"/>
                &lt;invoke at="start" pass="1" method="start"/>
                &lt;invoke at="status" pass="0" method="status"/>
                &lt;invoke at="stop" pass="0" method="stop"/>
        &lt;/service>
[edit] Local Syslog
If you need to run a local syslog as syslogd uses by default port 514 - syslog-ng is a good replacement. It can generate events without having to listen to port 514
[edit] Technical description
The service is built around traditional UDP based syslog messages, and the service is a receiver of messages created by remote syslog services.
[edit] Scalability
In the implementation I tried to look at scalability and as it's a UDP based service the minimization of message loss. The syslogd is implemented as a multithreaded fifo-queue and seems to scale quite well in test scenario
[edit] Status
Currently all RFC Applicable Facilities and Priorities are implemented.
[edit] How does it work
Question - What will happen as a syslog event arrives?
Answer - The message is parsed, the sender compared to known nodes. If the sender is an unknown node, an event is sent to discover the node. (This is configurable but it's the default behaviour). Otherwise the message is tagged to the correct node, broadcast to eventd, and from there on essentially is an openNMS event.
Question - Event how?
Answer - We convert the syslog message to an openNMS Event, the matching event as usual is found in the etc/events directory.
I.e. for each message received, it will get prioritized, categorized and submitted to the eventprocessor for notification or other configured actions.
Question - What if I need to match syslog messages with embedded newlines?
Answer - You can enable DOTALL mode (aka single-line mode) by adding (?s) to the very beginning of your forwarding-regexp, so ^((.+?) (.*)\n?$ becomes (?s)^((.+?) (.*)\n?$. Note that if you add (?s) to your forwarding-regexp, you should also add it to the expression attribute for each &lt;match> element in each of your &lt;ueiMatch> elements.
[edit] Problem solving
Most problems with syslog integration are due to different syslog formats received from different syslog daemons. This results in parsing errors like ip address is not found, ip address contains garbage, host name is not found, hostname seems to be always localhost, hostname contains garbage etc.. In this case you should try to forward all your syslog messages to a syslog file and look out for the different formats you receive like
Nov 19 00:13:47 host1 2540680: Nov 19 00:13:47.123: this is the message from host 1
Nov 19 00:13:48 host2/host3 2540681: Nov 19 00:13:48.523: this is the message from host x forwarded by host y
Nov 19 00:13:53 192.168.1.2 2540682: Nov 19 00:13:53.856: this is the message from ip 192.168.1.2
Nov 19 00:13:55 192.168.1.2/10.2.3.4 2540683: Nov 19 00:13:55.529: this is the message from ip x forwarded by ip y
You should try to get always the same format from the different syslogs, else you have to change the syslogd-configuration and try to match the different formats with your regexp's. Especially take care to match the hostname part correct adjusting
            matching-group-host="..."
            matching-group-message="..."
Note: In version 1.7+, use TRACE as log level in log4j.properties to get DEBUG-like logging.
To limit trial-and-error time you might take a look at the postgres-db of opennms:
Connect to the db:
psql -U userid pwd
(Standard: user=opennms pwd=opennms)
Search for entries coming from syslogd
opennms=# select * from events where eventuei like '%syslog%';
If you have a lot of syslog entries you might want to narrow the search using standard sql parameters.
Now you should see something like
eventid                 | 12159902
eventuei                | uei.opennms.org/syslogd/local7/Error
nodeid                  | 2
eventtime               | 2009-12-26 03:35:33+01
eventhost               | mgtserv1
eventsource             | syslogd
ipaddr                  | 10.99.99.99
eventdpname             | undefined
eventsnmphost           |
serviceid               |
eventsnmp               |
string,text);processid=0(string,text)
eventcreatetime         | 2009-12-26 03:35:33.821+01
ocess: 2664138 &lt;br> PID: 0 &lt;/p>
eventloggroup           |
eventlogmsg             |  &lt;p>An OpenNMS Event has been received as a Syslog Message &lt;/p>
                           Message: Dec 26 03:35:31.813: Late collision message from port 8 &lt;br>
eventseverity           | 5
eventpathoutage         |
eventcorrelation        |
eventsuppressedcount    |
eventoperinstruct       |
eventautoaction         |
eventoperaction         |
eventoperactionmenutext |
eventnotification       |
eventtticket            |
eventtticketstate       |
eventforward            |
eventmouseovertext      |
eventlog                | Y
eventdisplay            | Y
eventackuser            |
eventacktime            |
alarmid                 |

opennms=#
If the eventhost field contains some part of your syslog message or something else recognizable this might give you a clue what you have to change in your regexp or parameter numbering to get the correct values for hostname and messages.
[edit] More examples
Automating alarms and reducing amount of messages Syslogd_Automations
Integration with Hyperic Hyperic_HQ_Syslog_Integration (functionally obsolete, but may contain useful information and examples)
[edit] Version History/Availability
	•	This feature was added in version 1.3.2
	•	This feature was enhanced or modified in version 1.3.8
	•	This feature was enhanced or modified in version 1.6.3
</Text>
        </Document>
        <Document ID="79">
            <Title>Importer (TEC)</Title>
            <Text>Provisioning Integration
[edit] Assumptions
	•	TEC is the name of your Provisioning DB
[edit] Implementation
	•	How to do partial imports?
	•	How to force an import?
Brief description of the integration process Adding/Deleting/Changing sites works by..
[edit] Operational Parameters
	•	Integration is configured in the file: model-importer.properties
#
#http://quartz.sourceforge.net/javadoc/org/quartz/CronTrigger.html
#Field Name             Allowed Values          Allowed Special Characters
#Seconds                0-59                            , - * /
#Minutes                0-59                            , - * /
#Hours                  0-23                            , - * /
#Day-of-month   1-31                            , - * ? / L W C
#Month                  1-12 or JAN-DEC         , - * /
#Day-of-Week    1-7 or SUN-SAT          , - * ? / L C #
#Year (Opt)             empty, 1970-2099        , - * /

importer.importSchedule = 0 0 0 1 1 ? 2023

# Import URL
importer.importURL = file:/tmp/tec_dump.xml

#
# For SNMP scans
importer.scanThreads = 10
#
# Database persistence threads
importer.writeThreads = 8
	•	How Changes are picked up:
OpenNMS recognizes changes to the model-importer.properties file when the OpenNMS service is started. Note that in the next generation of OpenNMS, all services will either dynamically detect their respective config file changes or an event will be used to tell the service to re-read its configuration/add to its configuration.
[edit] Logging
OpenNMS uses the Log4J library for logging. (see the /etc/opennms/log4j.properties file).
[edit] Exported XML
If the export from TEC contains special characters, there needs to be special entity definitions in the XML header. Example:
&lt;?xml version="1.0" encoding="utf-8"?>
  &lt;!DOCTYPE myData [
    &lt;!ENTITY auml    "ä" >
    &lt;!ENTITY ouml    "ö" >
    &lt;!ENTITY uuml    "ü" >
    &lt;!ENTITY szlig   "ß" >
  ]>
[edit] File Format and Location
	•	Integration XML Format
[edit] Performance
The OpenNMS team has worked hard provide this integration with the performance required for very large networks. The current default settings in the model-importer.properties file allowed for an initial import of almost 45,000 devices in less than 1 hour. Subsequent runs are more intensive and take approximately 2 hours to complete. This performance improves tremendously when all devices provisioned with the SNMP service are responding to SNMP. At least half of the update time was spent waiting for SNMP timeouts. There is opportunity for improving performance by increasing the number of SNMP scanning threads in the model-importer.properties file.
#URL
importer.importURL = file:/tmp/tec_dump.xml
#importer.importURL = https://provisioning.mycompany.com/tecadmin/export?Username=XXXXXX&amp;Password=XXXXXX

#Schedule
importer.importSchedule = 0 0 3 * * ?

#Performance Tuning
importer.scanThreads = 50
importer.writeThreads = 8
importer.maxActiveConnections = 10

[edit] Importer Operation
Four new OpenNMS events were added to support operation of the Importer service:
 uei.opennms.org/internal/importer/reloadImport
 uei.opennms.org/internal/importer/importStarted
 uei.opennms.org/internal/importer/importSuccessful
 uei.opennms.org/internal/importer/importFailure
[edit] Reload Import Event
An event that can be used to cause the OpenNMS Importer service to be run immediately. See: How to force an import
[edit] Import Started Event
An event is sent at the beginning of the Import. This can be viewed in the OpenNMS event browser.
[edit] Import Successful Event
An event is sent upon successful completion of the OpenNMS Import integration. This event contains statistics about the performance of the import.
 ImporterService: Finished Importing: Deletes: 0 Updates: 1 Inserts: 855
 Importing: 3m 18s 871ms 
   Loading Resource: URL [file:/tmp/tec_dump.xml]: 873ms
   Auditing: 643ms Preprocessing: 2m 45s 882ms
   Total Scan Time: 326.047s  Avg Scan Time: 380.8960280373832ms per node
   Processing: 3m 17s 355ms
     Total Write Time: 212.918s Avg Write Time: 248.73598130841123ms per node
     Total Event Sending Time: 4.057s Avg Event Sending Time: 4.739485981308412ms per node Avg 0.5312295403954432ms per event
[edit] Import Failure Event
An event is sent upon the failure of the OpenNMS Importer integration. A parameter contains a message for the cause of the failure. This event should be configured to create an alarm.
[edit] JMX MBean
OpenNMS has instrumented the Importer service with JMX. Import statistics can be pulled from the JMX MBean and also viewed here:
http://localhost:8280/mbean?objectname=OpenNMS%3AName%3DImporterd
</Text>
        </Document>
        <Document ID="84">
            <Title>Provisioning</Title>
            <Text>OpenNMS Provisioning
OpenNMS version 1.8.0
The OpenNMS Group, Inc.	220 Chatham Business Drive, Suite 220	Pittsboro, NC 27312	T +1 919 533-0160	F Work Fax Phone david@opennms.com http://www.opennms.com
The OpenNMS Group, Inc.
Provisioning	5
Summary	5
Concepts	5 OpenNMS Provisioning Terminology	5
Entity	6 Foreign Source and Foreign ID	6 Foreign Source Definition	6 Import Requisition	6 Auto Discovery	6 Directed Discovery	6 Enhanced Directed Discovery	7 Policy Based Discovery	7
Addressing Scalability	7 Parallelization and Non-Blocking I/O	7 Provisioning Policies	7 The Default Foreign Source Definition	8
Getting Started	8 Provisioning the SNMP Configuration	8 Automatic Discovery	9
Separation of Concerns	9 Enhanced Directed Discovery	10 Understanding the Process	10
Import Handlers	12 File Handler	12 HTTP Handler	12 DNS Handler	12 DNS Import Examples:	12
Simple	12 Using a Regular Expression Filter	12 DNS Setup	13 Configuration	13
OpenNMS Provisioning	1
The OpenNMS Group, Inc.
Configuration Reload	13 Provisioning Examples	14
Basic Provisioning	14 Defining the Nodes via the Web-UI	14 Import the Nodes	16 Adding a Node	16 Changing a Node	17 Deleting a Node	17 Deleting all the Nodes	18
Advanced Provisioning Example	19 Service Detection	20 Applying a New Foreign Source Definition	20 Provisioning with Policies	21
New Import Capabilities	23 Provisiond Configuration	23 Provisioning Asset Data	24
External Requisition Sources	25 Provisioning Nodes from DNS	25
Adapters	27 DDNS Adapter	27 RANCID Adapter	27 Maps (soon to be moved to Mapd)	27 WiMax-Link (soon to be moved to Linkd)	27
Integrating with Provisiond	28 Provisioning Groups of Nodes	28 Example	28
Step 1 (Create a Foreign Source)	28 Step 2 (Update the SNMP configuration)	29 Step 3 (Create/Update the Requisition)	29 Adding a Node to a Current Requisition	29
Provisioning Single Nodes (Quick Add Node)	30 Fine Grained Provisioning Using “provision.pl”	31
First, Create a new Provisioning Group	31 OpenNMS Provisioning	2
The OpenNMS Group, Inc.
Add a Node to an Existing Provisioning Group	31 Create the Node Element	31 Add a Interface Element to that Node	32 Add a Couple of Services to that Interface	32 Set the Primary SNMP Interface	32 Add a couple Node Categories	32 Setting Asset Fields on a Node	33 Deploy the Import Requisition (Creating the Group)	33 Deleting a Node from an Import Requisition (Provisioning Group)	33
Yet Other API Examples	34 List the Nodes in a Provisioning Group	34
OpenNMS Provisioning	3
The OpenNMS Group, Inc.
Provisioning
Summary
The introduction of OpenNMS version 1.8 empowers enterprises and services providers like never before with a new service daemon for maintaining the managed entity inventory in OpenNMS. This new daemon, Provisiond, unifies all previous entity control mechanisms available in 1.6 (Capsd and the Importer), into a new and improved, massively parallel, policy based provisioning system. System integrators should note, Provisiond comes complete with a RESTFul Web Service API for easy integration with external systems such as CRM or external inventory systems as well as an adapter API for interfacing with other management systems such as configuration management.
OpenNMS 1.0, introduced almost a decade ago now, provided a capabilities scanning daemon, Capsd, as the mechanism for provisioning managed entities. Capsd, deprecated with the release of 1.8.0, provided a rich automatic provisioning mechanism that simply required an IP address to seed its algorithm for creating and maintaining the managed entities (nodes, interfaces, and IP based services). Version 1.2 added and XML-RPC API as a more controlled (directed) strategy for provisioning services that was mainly used by non telco based service providers (i.e. managed hosting companies). Version 1.6 followed this up with yet another and more advanced mechanism called the Importer service daemon. The Importer provided large service providers with the ability to strictly control the OpenNMS entity provisioning with an XML based API for completely defining and controlling the entities where no discovery and service scanning scanning was feasible.
The Importer service improved OpenNMS' scalability for maintaining managed entity databases by an order of magnitude.	This daemon, while very simple in concept and yet extremely powerful and flexible provisioning improvement, has blazed the trail for Provisiond. The Importer service has been in production for 3 years in service provider networks maintaining entity counts of more than 50,000 node level entities on a single instances of OpenNMS. It is a rock solid provisioning tool.
Provisiond begins a new era of managed entity provisioning in OpenNMS.
Concepts
Provisioning is a term that is familiar to service providers (a.k.a. operators, a.k.a. telephone companies) and OSS systems but not so much in the non OSS enterprises.
Provisiond receives "requests" for adding managed entities via 2 basic mechanisms, the OpenNMS traditional “New Suspect” event, typically via the Discovery daemon, and the import requisition (XML definition of node entities) typically via the Provisioning Groups UI. If you are familiar with all previous releases of OpenNMS, you will recognize the New Suspect Event based Discovery to be what was previously the Capsd component of the auto discovery behavior. You will also recognize the import requisition to be of the Model Importer component of OpenNMS. Provisiond now unifies these two separate components into a massively parallel advanced policy based provisioning service.
OpenNMS Provisioning Terminology
The following terms are used with respect to OpenNMS’ provisioning system and are essential for understanding the material presented in this guide.
OpenNMS Provisioning	4
The OpenNMS Group, Inc.
Entity
Entities are managed objects in OpenNMS such as Nodes, IP interfaces, SNMP Interfaces, and Services.
Foreign Source and Foreign ID
The Importer service from 1.6 introduced the idea of foreign sources and foreign IDs. The Foreign Source uniquely identifies a provisioning source and is still a basic attribute of importing node entities into OpenNMS. The concept is to provide an external (foreign) system with a way to uniquely identify itself and any node entities that it is requesting (via a requisition) to be provisioned into OpenNMS. The Foreign ID is the unique node ID maintained in foreign system and the foreign source uniquely identifies the external system in OpenNMS.
OpenNMS uses the combination of the foreign source and foreign ID become the unique foreign key when synchronizing the set of nodes from each source with the nodes in the OpenNMS DB. This way the foreign system doesn’t have to keep track of the OpenNMS node IDs that are assigned when a node is first created. This is how Provisiond can decided if a node entity from an import requisition is new, has been changed, or needs to be deleted.
Foreign Source Definition
Additionally, the foreign source has been extended to also contain specifications for how entities should be discovered and managed on the nodes from each foreign source. The name of the foreign source has become pervasive within the provisioning system and is used to simply some of the complexities by weaving this name into:
• the name of the provisioning group in the Web-UI • the name of the file containing the persisted requisition (as well as the pending requisition if it is in this state) • the foreign-source attribute value inside the requisition (obviously, but, this is pointed out to indicate that the file
name doesn’t necessarily have to equal the value of this attribute but is highly recommended as an OpenNMS
best practice) • the building attribute of the node defined in the requisition (this value is called “site” in the Web-UI and is assigned
to the building column of the node’s asset record by Provisiond and is the default value used in the Site Status View feature)
Import Requisition
Import requisition is the terminology OpenNMS uses to represent the set of nodes, specified in XML, to be provisioned from a foreign source into OpenNMS. The requisition schema (XSD) can be found at the following location.
http://xmlns.opennms.org/xsd/config/model-import
Auto Discovery
Auto discovery is the term used by OpenNMS to characterize the automatic provisioning of nodes entities. Currently, OpenNMS uses an ICMP ping sweep to find IP address on the network. For the IPs that respond and that are not currently in the DB, OpenNMS generates a new suspect event. When this event is received by Provisiond, it creates a node and it begins a node scan based on the default foreign source definition.
Directed Discovery
Provisiond takes over for the Model Importer found in version 1.6 which implemented a unique, first of its kind, controlled mechanism for specifying managed entities directly into OpenNMS from one or more data sources. These data sources often were in the form of an in-housed developed inventory or stand-alone provisioning system or even a set of element
OpenNMS Provisioning	5
The OpenNMS Group, Inc.
management systems. Using this mechanism, OpenNMS is directed to add, update, or delete a node entity exactly as defined by the external source. No discovery process is used for finding more interfaces or services.
Enhanced Directed Discovery
Directed discovery is enhanced with the capability to scan nodes that have been directed nodes for entities (interfaces.
Policy Based Discovery
The phrase, Policy based Directed Discovery, is a term that represents the latest step in OpenNMS’ provisioning evolution and best describes the new provisioning architecture now in OpenNMS for maintaining its inventory of managed entities. This term describes the control that is given over the Provisioning system to OpenNMS users for managing the behavior of the NMS with respect to the new entities that are being discovered. Current behaviors include persistence, data collection, service monitoring, and categorization policies.
Addressing Scalability
The explosive growth and density of the IT systems being deployed today to support not traditional IP services is impacting management systems like never before and is demanding from them tremendous amounts of scalability. The scalability of a management system is defined by its capacity for maintaining large numbers of managing entities coupled with its efficiency of managing the entities.
Today, It is not uncommon for OpenNMS deployments to find node entities with tens of thousands of physical interfaces being reported by SNMP agents due to virtualization (virtual hosts, interfaces, as well as networks). An NMS must be capable of using the full capacity every resource of its computing platform (hardware and OS) as effectively as possible in order to manage these environments. The days of writing scripts or single threaded applications will just no longer be able to do the work required an NMS when dealing with the scalability challenges facing systems and systems administrators working in this domain.
Parallelization and Non-Blocking I/O
Squeezing out every ounce of power from a management system’s platform (hardware and OS) is absolutely required to complete all the work of a fully functional NMS such as OpenNMS. Fortunately, the hardware and CPU architecture of a modern computing platform provides multiple CPUs with multiple cores having instruction sets that include support for atomic operations. While these very powerful resources are being provided by commodity systems, it makes the complexity of developing applications to use them vs. not using them, orders of magnitude more complex. However, because of scalability demands of our complex IT environments, multi-threaded NMS applications are now essential and this has fully exposed the complex issues of concurrency in software development.
OpenNMS has stepped up to this challenge with its new concurrency strategy. This strategy is based on a technique that combines the efficiency of parallel (asynchronous) operations (traditionally used by most effectively by single threaded applications) with the power of a fully current, non-blocking, multi-threaded design. The non-blocking component of this new concurrency strategy added greater complexity but OpenNMS gained orders of magnitude in increased scalability. Note: Java Runtimes, based on the Sun JVM, have provided implementations for processor based atomic operations and is the basis for OpenNMS’ non-blocking concurrency algorithms.
Provisioning Policies
Just because you can, doesn’t mean you should! Because the massively parallel operations being created for Provisiond allows tremendous numbers of nodes, interfaces, and services to be very rapidly discovered and persisted, doesn’t mean it should. A policy API was created for Provisiond that allows implementations to be developed that can be applied to control the behavior of Provisiond. The 1.8 release includes a set of flexible provisioning policies that control the persistence of entities and their attributes constrain monitoring behavior.
OpenNMS Provisioning	6
The OpenNMS Group, Inc.
When nodes are imported or re-scanned, there is, potentially, a set of zero or more provisioning policies that are applied. The policies are defined in the foreign source’s definition. The policies for an auto-discovered node or nodes from provisioning groups that don’t have a foreign source definition, are the policies defined in the default foreign source definition.
The Default Foreign Source Definition
Contained in the libraries of the Provisioning service is the "template" or default foreign source. The template stored in the library is used until the OpenNMS admin user alters the default from the Provisioning Groups WebUI. Upon edit, this template is exported to the OpenNMS etc/ directory with the file name: "default-foreign-source.xml".
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;foreign-source date-stamp="2009-10-16T18:04:12.844-05:00" name="default" xmlns="http://xmlns.opennms.org/ xsd/config/foreign-source">
&lt;scan-interval>1d&lt;/scan-interval> &lt;detectors>
&lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector &lt;detector
&lt;/detectors>
&lt;policies/> &lt;/foreign-source>
Getting Started
class="org.opennms.netmgt.provision.detector.datagram.DnsDetector" name="DNS"/> class="org.opennms.netmgt.provision.detector.simple.FtpDetector" name="FTP"/> class="org.opennms.netmgt.provision.detector.simple.HttpDetector" name="HTTP"/> class="org.opennms.netmgt.provision.detector.simple.HttpsDetector" name="HTTPS"/> class="org.opennms.netmgt.provision.detector.icmp.IcmpDetector" name="ICMP"/> class="org.opennms.netmgt.provision.detector.simple.ImapDetector" name="IMAP"/> class="org.opennms.netmgt.provision.detector.simple.LdapDetector" name="LDAP"/> class="org.opennms.netmgt.provision.detector.simple.NrpeDetector" name="NRPE"/> class="org.opennms.netmgt.provision.detector.simple.Pop3Detector" name="POP3"/> class="org.opennms.netmgt.provision.detector.radius.RadiusAuthDetector" name="Radius"/> class="org.opennms.netmgt.provision.detector.simple.SmtpDetector" name="SMTP"/> class="org.opennms.netmgt.provision.detector.snmp.SnmpDetector" name="SNMP"/> class="org.opennms.netmgt.provision.detector.ssh.SshDetector" name="SSH"/>
Default Foreign Source
An NMS is of no use until it is setup for monitoring and entities are added to the system. OpenNMS installs with a base configuration with a configuration that is sufficient get service level monitoring and performance management quickly up and running. As soon as managed entities are provisioned, the base configuration will automatically begin monitoring and reporting.
Generally speaking, there are two methods of provisioning in OpenNMS: Auto Discovery and Directed Discovery. We'll start with Auto Discovery, but first, we should quickly review the configuration of SNMP so that newly discovered devices can be immediately scanned for entities as well as have reporting and thresholding available.
Provisioning the SNMP Configuration
OpenNMS requires that the SNMP configuration to be properly setup for your network in order to properly understand Network and Node topology as well as to automatically enabled performance data collection. Network topology is updated as nodes (a.k.a. devices or hosts) are provisioned. Navigate to the Admin/Configure SNMP Community Names as shown below.
*Note: Provisiond includes an option to add community information in the "Single Node" provisioning interface. This, is equivalent of entering a single IP address in the screen with the convenience of setting the community string at the same time a node is provisioned. See the “Quick Node Add” feature below for more details about this capability.
This screen sets up SNMP within OpenNMS for agents listening on IP addresses 10.1.1.1 through 10.254.254.254. These settings are optimized into the snmp-configuration.xml file. Optimization means that the minimal configuration
OpenNMS Provisioning	7
The OpenNMS Group, Inc.
possible will be written. Any IP addresses already configured that are eclipsed by this range will be removed. Here is the resulting configuration.
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;snmp-config xmlns="http://xmlns.opennms.org/xsd/config/snmp" port="161"
retry="3" timeout="800" read-community="public"
version="v1" max-vars-per-pdu="10"> &lt;definition retry="1" timeout="2000"
read-community="public" version="v2c"> &lt;specific>10.12.23.32&lt;/specific>
&lt;/definition> &lt;/snmp-config>
Sample snmp-config.xml
However, If an IP address is then configured that is within the range, the range will be split into two separate ranges and a specific entry is added. For example, if a configuration was added through the same UI for the IP: 10.12.23.32 having the community name "public", then the resulting configuration will be:
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;snmp-config xmlns="http://xmlns.opennms.org/xsd/config/snmp" port="161"
retry="3" timeout="800" read-community="public" version="v1" max-vars-per-pdu="10"> &lt;definition retry="1" timeout="2000" read-community="YrusoNoz" version="v2c">
&lt;range begin="10.1.1.1" end="10.12.23.31"/>
&lt;range begin="10.12.23.33" end="10.254.254.254"/> &lt;/definition>
&lt;definition retry="1" timeout="2000" read-community="public" version="v2c"> &lt;specific>10.12.23.32&lt;/specific>
&lt;/definition> &lt;/snmp-config>
*Note: the bold IP addresses show where the range was split and the specific with community name "public" was added.
Now, with SNMP configuration provisioned for our 10 network, we are ready to begin adding nodes. Our first example will be to automatically discovery and add all managed entities (nodes, IP interfaces, SNMP Interfaces, and Monitored IP based Services). We will then give an example of how to be more "directed" and deliberate about your discovery by using Provisioning Groups.
Automatically discovered entities are analyzed, persisted to the relational data store, and then managed based on the policies defined in the default foreign source definition. This is very similar to the way that entities were handled by Capsd by with finer grained sense of control.
Automatic Discovery
Currently in OpenNMS, the ICMP is used to automatically provision node entities into OpenNMS. This functionality has been in OpenNMS since is 1.0 release, however, in 1.8, a few of the use cases have been updated with Provisiond’s replacement of Capsd.
Separation of Concerns
Version 1.8 Provisiond separates what was called Capsd scanning in to 3 distinct phases: entity scanning, service detection, and node merging. These phases are now managed separately by Provisiond. Immediately following the import of a node entity, tasks are created for scanning a node to discover the node entity’s interfaces (SNMP and IP). As interfaces are found, they are persisted and tasks are scheduled for service detection of each IP interface.
For auto discovered nodes, a node merging phase is scheduled. Nodes that have been directly provisioned will not be included in the node process. Only in the case the 2 where nodes that have been automatically discovered that appear to be the same node with the node merging phase be activated.
(Note: the use case and redesign of node merging is still an outstanding issue with the 1.8.0 release)
OpenNMS Provisioning	8
The OpenNMS Group, Inc.
Enhanced Directed Discovery
This new form of provisioning first appears in OpenNMS with version 1.8 and the new Provisiond service. It combines the benefits of the Importer’s strictly controlled methodology of directed provisioning (from version 1.6) with OpenNMS’ robustly flexible auto discovery. Enhanced Directed discovery begins with an enhanced version of the same import requisition used in directed provisioning and completes with a policy influenced persistence phase that sorts though the details of all the entities and services found during the entity and service scanning phase.
If you are planning to use this form of provisioning, it important to understand the conceptual details of how Provisiond manages entities it is “directed” to provision. This knowledge will enable administrators and systems integrators to better plan, implement, and resolve any issues involved with this provisioning strategy.
Understanding the Process
There are 3 phases involved with directing entities to be discovered: import, node scan, and service scan. The import phase also has sub phases: marshal, audit, limited SNMP scan, and re-parent.
Marshal and Audit Phases
It is important to understand that the nodes requisitioned from each foreign source are managed as a complete set. Nodes defined in a requisition from the foreign source “CRM” and “CMDB”, for example, will be managed separately from each other even if they should contain exactly the same node definitions. To OpenNMS, these are individual entities and they are managed as a set.
Requisitions are referenced via a URL. Currently, the URL can be specified as one of the following protocols: FILE, HTTP, HTTPS, and DNS. Each protocol has a protocol handler that is used to stream the XML from a foreign source, i.e. http://inv.corp.org/import.cgi?customer=acme or file:/opt/opennms/etc/imports/acme.xml.	The DNS protocol is a special handler developed for Provisioning sets of nodes as a foreign-source from a corporate DNS server. See DNS Protocol Handler for details.
Upon the import request (either on schedule or on demand via an Event) the requisition is marshaled into Java objects for processing. The nodes defined in the requisition represent what OpenNMS should have as the current set of managed entities from that foreign source. The audit phase determines for each node defined (or not defined) in the requisition which are to be processed as an Add, Update, or Delete operation during the Import Phase. This determination is made by comparing the set foreign IDs of each node in the requisition set with the set of foreign IDs of currently managed entities in OpenNMS.
The intersection of the IDs from each set will become the Update operations, the extra set of foreign IDs that are in the requisition become the Add operations, and the extra set of foreign IDs from the managed entities become the Delete operations. This implies that the foreign IDs from each foreign source must be unique.
Naturally, the first time an import request is processed from a foreign source there will be zero (0) node entities from the set of nodes currently being managed and each node defined in the requisition will become an Add Operation. If a requisition is processed with zero (0) node definitions, all the currently managed nodes from that foreign source will become Delete operations (all the nodes, interfaces, outages, alarms, etc. will be removed from OpenNMS).
When nodes are provisioned using the Provisioning Groups Web-UI, the requisitions are stored on the local file system and the file protocol handler is used to reference the requisition. Each Provisioning Group is a separate foreign source and unique foreign IDs are generated by the Web-UI. An MSP might use Provisioning Groups to define the set of nodes to be managed by customer name where each customer’s set of nodes are maintained in a separate Provisioning Group.
Import Phase
The import phase begins when Provisiond receives a request to import a requisition from a URL. The first step in this phase is to load the requisition and marshal all the node entities defined in the requisition into Java objects. If any
OpenNMS Provisioning	9
The OpenNMS Group, Inc.
syntactical or XML structural problems occur in the requisition, the entire import is abandoned and no import operations are completed.
Once the requisition is marshaled, the requisition nodes are audited against the persisted node entities. The set of requisitioned nodes are compared with a subset of persisted nodes and this subset is generated from a database query using the foreign source defined in the requisition. The audit generates one of three operations for each requisition node: insert, update, delete based on each requisitioned node’s foreign ID.	Delete operations are created for any nodes that are not in the requisition but are in the DB subset, update operations are created for requisition nodes that match a persisted node from the subset (the intersection), and insert operations are created from the remaining requisition nodes (nodes in the requisition that are not in the DB subset).
If a requisition node has an interface defined as the Primary SNMP interface, then during the update and insert operations the node will be scanned for minimal SNMP attribute information. This scan find the required node and SNMP interface details required for complete SNMP support of the node and only the IP interfaces defined in the requisition. Note: this not the same as Provisiond SNMP discovery scan phases: node scan and interface scan.
Node Scan Phase
Where directed discovery leaves off and enhanced directed discovery begins is that after all the operations have completed, directed discovery is finished and enhanced directed discovery takes off. The requisitioned nodes are scheduled for node scans where details about the node are discovered and interfaces that were not directly provisioned are also discovered. All physical (SNMP) and logical (IP) interfaces are discovered and persisted based on any Provisioning Policies that may have defined for the foreign source associated with the import requisition.
Service Scan (detection) Phase
Additionally, the new Provisiond enhanced directed discovery mechanism follows interface discovery with service detection on each IP interface entity. This is very similar to the Capsd plugin scanning found in all former releases of OpenNMS accept that the foreign source definition is used to define what services should be detected on these interfaces found for nodes in the import requisition.
OpenNMS Provisioning	10
The OpenNMS Group, Inc.
Import Handlers
File Handler
HTTP Handler
DNS Handler
The new Provisioning service in OpenNMS is continuously improving and adapting to the needs of the community. One of the most recent enhancements to the system is built upon the very flexible and extensible API of referencing an import requisition's location via a URL. Most commonly, these URLs are files on the file system (i.e. file:/opt/opennms/etc/ imports/&lt;my-provisioning-group.xml>) as requisitions created by the Provisioning Groups UI. However, these same requisitions for adding, updating, and deleting nodes (based on the original model importer) can also come from URLs specifying the HTTP protocol:
http://myinventory.server.org/nodes.cgi
Now, using Java's extensible protocol handling specification, a new protocol handler was created so that a URL can be specified for requesting a Zone Transfer (AXFR) request from a DNS server. The A records are recorded and used to build an import requisition. This is handy for organizations that use DNS (possibly coupled with an IP management tool) as the data base of record for nodes in the network. So, rather than ping sweeping the network or entering the nodes manually into OpenNMS Provisioning UI, nodes can be managed via 1 or more DNS servers.
The format of the URL for this new protocol handler is:
dns://&lt;host>[:port]/&lt;zone>[/&lt;foreign-source>/][?expression=&lt;regex>]
DNS Import Examples: Simple
dns://my-dns-server/myzone.com
This URL will import all A records from the host "my-dns-server" on port 53 (default port) from zone "myzone.com" and since the foreign source (a.k.a. the provisioning group) is not specified it will default to the specified zone.
Using a Regular Expression Filter
dns://my-dns-server/myzone.com/portland/?expression=^por-.*
This URL will import all nodes from the same server and zone but will only manage the nodes in the zone matching the regular expression "^port-.*" and will and they will be assigned a unique foreign source (provisioning group) for managing these nodes as a subset of nodes from within the specified zone.
OpenNMS Provisioning	11
The OpenNMS Group, Inc.
If your expression requires URL encoding (for example you need to use a "?" in the expression) it must be properly encoded.
dns://my-dns-server/myzone.com/portland/?expression=^por[0-9]%3F
DNS Setup
Currently, the DNS server requires to be setup to allow a zone transfer from the OpenNMS server. It is recommended that a secondary DNS server is running on OpenNMS and that the OpenNMS server be allowed to request a zone transfer. A quick way to test if zone transfers are working is:
dig -t AXFR @&lt;dnsServer> &lt;zone>
Configuration
The configuration of the Provisoning system has moved from a properties file (model-importer.properties) to an XML based configuration container. The configuration is now extensible to allow the definition of 0 or more import requisitions each with their own cron based schedule for automatic importing from various sources (intended for integration with external URL such as http and this new dns protocol handler.
A default configuration is provided in the OpenNMS etc/ directory and is called: provisiond-configuration.xml. This default configuration has an example for scheduling an import from a DNS server running on the localhost requesting nodes from the zone, localhost and will be imported once per day at the stroke of midnight. Not very practical but is a good example.
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;provisiond-configuration
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://xmlns.opennms.org/xsd/config/provisiond-configuration" foreign-source-dir="/opt/opennms/etc/foreign-sources" requistion-dir="/opt/opennms/etc/imports"
importThreads="8" scanThreads="10" rescanThreads="10" writeThreads="8" > &lt;!--
http://quartz.sourceforge.net/javadoc/org/quartz/CronTrigger.html
Field Name Seconds Minutes Hours Day-of-month Month Day-of-Week Year (Opt)
Allowed Values 0-59 0-59 0-23
1-31 1-12 or JAN-DEC 1-7 or SUN-SAT empty, 1970-2099
Allowed ,-* / ,-* / ,-* / ,-* ? ,-* / ,-* ? ,-* /
Special Characters
/LW C /LC #
--> &lt;requisition-def import-name="localhost" import-url-resource="dns://localhost/localhost">
&lt;cron-schedule>0 0 0 * * ? *&lt;/cron-schedule> &lt;!-- daily, at midnight --> &lt;/requisition-def>
&lt;/provisiond-configuration>
Configuration Reload
Like many of the daemon configuration in the 1.7 branch, the configurations are reloadable without having to restart OpenNMS, using the reloadDaemonConfig uei:
/opt/opennms/bin/send-event.pl uei.opennms.org/internal/reloadDaemonConfig --parm 'daemonName Provisiond'
This means that you don't have to restart OpenNMS every time you update the configuration.
OpenNMS Provisioning	12
The OpenNMS Group, Inc.
Provisioning Examples
Here are a few practical examples of enhanced directed discovery to help with your understanding of this feature.
Basic Provisioning
This example adds three nodes and requires no OpenNMS configuration other than specifying the node entities to be provisioned and managed in OpenNMS.
Defining the Nodes via the Web-UI
Using the Provisioning Groups Web-UI, three nodes are created given a single IP address. Navigate to the Admin Menu and click Provisioning Groups Menu from the list of Admin options and create the group “Bronze”.
Creating a new Provisioning Group
Clicking the “Add New Group” button will create the group and will redisplay the page including this new group among the list of any group(s) that have already been created.
Note: At this point, the XML structure for holding the new provisioning group (a.k.a. an import requisition) has been persisted to the $OPENNMS_ETC/imports/pending directory.
Clicking the “Edit” link will bring you to the screen where you can begin the process of defining node entities that will be imported into OpenNMS. Click the Add Node button will begin the node entity creation process fill in the node label and click the “Save” button.
Creating a new Node definition in the Provisioning Group
At this point, the provisioning group contains the basic structure of a node entity but it is not complete until the interface(s) and interface service(s) have been defined. After having clicked the “Save” button, as we did above,
OpenNMS Provisioning	13
The OpenNMS Group, Inc.
presents, in the Web-UI, the options “Add Interface”, “Add Node Category”, and “Add Node Asset”. Click the “Add Interface” link to add an interface entity to the node.
Adding an Interface to the node definition
Enter the IP address for this interface entity, a description, and specify the Primary attribute as ‘P’ (Primary), ‘S’ (Secondary), ‘N’ (Not collected), or ‘C’ (Collected) and click the save button. Now the node entity has an interface for which services can be defined for which the Web-UI now presents the “Add Service” link. Add two services (ICMP, SNMP) via this link.
A complete node definition with all “required” elements defined.
Now the node entity definition contains all the “required” elements necessary for importing this requisition into OpenNMS. At this point, all the interfaces that are required for the node should be added. For example, NAT interfaces should be specified there are services that they provide because they will not be discovered during the Scan Phase.
Two more node definitions will be added for the benefit of this example.
The completed requisition for the example Bronze Provisioning Group
This set of nodes represents an import requisition for the “Bronze” provisioning group. As this requisition is being edited via the WebUI, changes are being persisted into the OpenNMS configuration directory $OPENNMS_etc/imports/ pending as an XML file having the name “bronze.xml”.
Note: The name of the XML file containing the import requisition is the same as the provisioning group name. Therefore naming your provisioning group without the use of spaces makes them easier to manage on the file system.
Click the “Done” button to return to the Provisioning Groups list screen. The details of the “Bronze” group now indicates that there are 3 nodes in the requisition and that there are no nodes in the DB from this group (a.k.a. foreign source). Additionally, you can see that time the requisition was last modified and the time it last imported are given (the time stamps are stored as attributes inside the requisition and are not the file system time stamps). These details are indicative of how well the DB represents what is in the requisition.
OpenNMS Provisioning	14
The OpenNMS Group, Inc.
Note: You can tell that this is a pending requisition for 2 reasons: 1) there are 3 nodes defined and 0 nodes in the DB, 2) the requisition has been modified since the last import (in this case “never”).
Import the Nodes
In this example, you see that there are 3 nodes in the pending requisition and 0 in the DB. Click the “Import” button to submit the requisition to the provisioning system (what actually happens is that the Web-UI sends an event to the Provisioner telling it to begin the Import Phase for this group).
Note: Do not refresh this page to check the values of these details. To refresh the details to verify the import, click the “Provisioning Groups” bread crumb item.
You should be able to immediately verify the importation of this provisioning group because the import happens very quickly. Provisiond has several threads ready for processing the import operations of the nodes defined in this requisition. A few SNMP packets are sent and received to get the SNMP details of the node and the interfaces defined in the requisition. Upon receipt of these packets (or not) each node is inserted as a DB transaction.
The nodes are now added to OpenNMS and are under management.
Following the import of a node with thousands of interfaces, you will be able to refresh the Interface table browser on the Node page and see that interfaces and services are being discovered and added in the background. This is the discovery component of directed discovery.
Adding a Node
To direct that another node be added from a foreign source (in this example the Bronze Provisioning Group) simply add a new node definition and re-import. It is important to remember that all the node definitions will be re-imported and the existing managed nodes will be updated, if necessary. See Changing a Node.
OpenNMS Provisioning	15
The OpenNMS Group, Inc.
Changing a Node
To direct changes to an existing node, simply add, change, or delete elements or attributes of the node definition and re- import. This is a great feature of having directed specific elements of a node in the requisition because that attributes will simply be changed. For example, to change the IP address of the Primary SNMP interface for the node, “barbrady.opennms.org”, just change the requisition and re-import.
Each element in the Web-UI has an associated Edit icon. Click this icon to change the IP address for barbrady.opennms.org, click save, and then Click the Done button.
Changing the IP address of “barbrady.opennms.org” from 10.1.1.2 to 192.168.1.1
The Web-UI will return you to the Provisioning Groups screen where you will see that there are the time stamp showing that the requisition’s last modification is more recent that the last import time.
The Provisioning Group must be re-imported
This provides an indication that the group must be re-imported for the changes made to the requisition to take effect. The IP Interface will be simply updated and all the required events (messages) will be sent to communicate this change within OpenNMS.
The IP interface for barbrady.opennms.org is immediately updated
Deleting a Node
Barbrady has not been behaving, as one might expect, so it is time to remove him from the system. Edit the provisioning group, click the delete button next to the node barbrady.opennms.org, click the Done button.
OpenNMS Provisioning	16
The OpenNMS Group, Inc.
Bronze Provisioning Group definition indicates a node has been removed and requires an import to delete the node entity from the OpenNMS system
Click the Import button for the Bronze group and the Barbrady node and its interfaces, services, and any other related data will be immediately deleted from the OpenNMS system. All the required Events (messages) will be sent by Provisiond to provide indication to the OpenNMS system that the node Barbrady has been deleted.
Barbrady has been deleted
Deleting all the Nodes
There is a convenient way to delete all the nodes that have been provided from a specific foreign source. From the main Admin/Provisioning Groups screen in the Web-UI, click the “Delete Nodes” button. This button deletes all the nodes defined in the Bronze requisition. It is very important to note that once this is done, it cannot be undone! Well it can’t be undone from the Web-UI and can only be undone if you’ve been good about keeping a backup copy of your $OPENMS_ETC/ directory tree. If you’ve made a mistake, before you re-import the requisition, restore the Bronze.xml requisition from your backup copy to the $OPENNMS_ETC/imports/ directory.
All node definitions have been removed from the Bronze requisition. The Web-UI indicates an import is now required to remove them from OpenNMS.
Clicking the Import button will cause the Audit Phase of Provisiond to determine that all the nodes from the Bronze group (foreign source) should be deleted from the DB and will create Delete operations. At this point, if you are satisfied that the nodes have been deleted and that you will no longer require nodes to be defined in this Group, you will see that the “Delete Nodes” button has now changed to the “Delete Group” button. The “Delete Group” button is displayed when there are no nodes entities from that group (foreign source) in OpenNMS.
OpenNMS Provisioning	17
The OpenNMS Group, Inc.
When no node entities from the group exist in OpenNMS, then the Delete Group button is displayed.
Advanced Provisioning Example
In the previous example, we provisioned 3 nodes and let Provisiond complete all of its import phases using a default foreign source definition. Each Provisioning Group can have a separate foreign source definition that controls:
• The rescan interval • The services to be detected • The policies to be applied
This example will demonstrate how to create a foreign source definition and how it is used to control the behavior of Provisiond when importing a Provisioning Group/foreign source requisition.
First let’s simply provision the node and let the default foreign source definition apply.
The node definition used for the Advanced Provisioning Example
Following the import, All the IP and SNMP interfaces, in addition to the interface specified in the requisition, have been discovered and added to the node entity. The default foreign source definition has no polices for controlling which interfaces that are discovered either get persisted or managed by OpenNMS.
OpenNMS Provisioning	18
The OpenNMS Group, Inc.
Logical and Physical interface and Service entities directed and discovered by Provisiond.
Service Detection
As IP interfaces are found during the node scan process, service detection tasks are scheduled for each IP interface. The service detections defined in the foreign source determines which services are to be detected and how (i.e. the values of the parameters that parameters control how the service is detected, port, timeout, etc.).
Applying a New Foreign Source Definition
This example node has been provisioned using the Default foreign source definition. By navigating to the Provisioning Groups screen in the OpenNMS Web-UI and clicking the Edit Foreign Source link of a group, you can create a new foreign source definition that defines service detection and policies. The policies determine entity persistence and/or set attributes on the discovered entities that control OpenNMS’ management behaviors.
When creating a new foreign source definition, the default definition is used as a template.
In this UI, new Detectors can be added, changed, and removed. For this example, we will remove detection of all services accept ICMP and DNS, change the timeout of ICMP detection, and a new Service detection for OpenNMS Web-UI.
OpenNMS Provisioning	19
The OpenNMS Group, Inc.
Custom foreign source definition created for NMS Provisioning Group (foreign source).
Click the Done button and re-import the NMS Provisioning Group. During this and any subsequent re-imports or re- scans, the OpenNMS detector will be active, and the detectors that have been removed will no longer test for the related services for the interfaces on nodes managed in the provisioning group (requisition), however, the currently detected services will not be removed. There are 2 ways to delete the previously detected services:
1.	Delete the node in the provisioning group, re-import, define it again, and finally re-import again 2.	Use the ReST API to delete unwanted services. Use this command to remove each unwanted service from each
interface, iteratively:
curl -X DELETE -H "Content-Type: application/xml" -u admin:admin http://localhost:8980/opennms/rest/nodes/ 6/ipinterfaces/172.16.1.1/services/DNS
HINT: There is a sneaky way to do #1. Edit the provisioning group and just change the foreign ID. That will make Provisiond think that a node was deleted and a new node was added in the same requisition! Use this hint with caution and an full understanding of the impact of deleting an existing node.
Provisioning with Policies
The Policy API in Provisiond allow you to control the persistence of discovered IP and SNMP Interface entities and Node Categories during the Scan phase.
Matching IP Interface Policy
The Matching IP Interface policy controls whether discovered IP interfaces are to be persisted and if they are to be persisted, whether or not they will be forced to be Managed or Unmanaged.
Continuing with this example Provisioning Group, we are going to define a few policies that: a)	Prevent discovered 10 network addresses from being persisted b)	Force 192.168 network addresses to be unmanaged
From the foreign source definition screen, click the Add Policy button and you the definition of a new policy will begin with a field for naming the policy and a drop down list of the currently installed policies. Name the policy “no10s”, make sure that the Match IP Interface policy is specified in the class list and click the Save button. This action will automatically add all the parameters required for the policy.
The 2 required parameters for this policy are action and matchBehavior. The action parameter can be set to DO_NOT_PERSIST, Manage, or UnManage.
OpenNMS Provisioning	20
The OpenNMS Group, Inc.
Creating a policy to prevent persistence of 10 network IP interfaces.
The DO_NOT_PERSIST action does just what it indicates, it prevents discovered IP interface entities from being added to OpenNMS when the matchBehavior is satisfied. The Manage and UnManage values for this action allow the IP interface entity to be persisted by control whether or not that interface should be managed by OpenNMS.
The matchBehavior action is a boolean control that determines how the optional parameters will be evaluated. Setting this parameter’s value to ALL_PARAMETERS causes Provisiond to evaluate each optional parameter with boolean “AND” logic and the value ANY_PARAMETERS will cause “OR” logic to be applied.
Now we will add one of the optional parameters to filter the 10 network addresses. The Matching IP Interface policy supports 2 additional parameters, hostName and ipAddress. Click the Add Parameter link and choose ipAddress as the key. The value for either of the optional parameters can be an exact or regular expression match. As in most configurations in OpenNMS where regular expression matching can be optionally applied, prefix the value with the ‘~’ character.
Example Matching IP Interface Policy to not Persist 10 Network addresses
Any subsequent scan of the node or re-imports of NMS provisioning group will force this policy to be applied. IP Interface entities that already exist that match this policy will not be deleted. Existing interfaces can be deleted by recreating the node in the Provisioning Groups screen (simply change the foreign ID and re-import the group) or by using the ReST API:
curl -X DELETE -H "Content-Type: application/xml" -u admin:admin http://localhost:8980/opennms/rest/nodes/ 6/ipinterfaces/10.1.1.1
The next step in this example is to define a policy that sets discovered 192.168 network addresses to be unmanaged (not managed) in OpenNMS. Again, click the Add Policy button and let’s call this policy noMgt192168s. Again, choose the Mach IP Interface policy and this time set the action to UNMANAGE.
Policy to not manage IP interfaces from 192.168 networks Note: The UNMANAGE behavior will be applied to existing interfaces.
Matching SNMP Interface Policy
Like the Matching IP Interface Policy, this policy controls the whether discovered SNMP interface entities are to be persisted and whether or not OpenNMS should collect performance metrics from the SNMP agent for Interface’s index (MIB2 IfIndex).
In this example, we are going to create a policy that doesn’t persist interfaces that are AAL5 over ATM or type 49 (ifType). Following the same steps as when creating an IP Management Policy, edit the foreign source definition and create a new policy. Let’s call it: “noAAL5s”. We’ll use Match SNMP Interface class for each policy and add a parameter with ifType as the key and “49” as the value.
OpenNMS Provisioning	21
The OpenNMS Group, Inc.
Matching SNMP Interface Policy example for Persistence and Data Collection
Note: At the appropriate time during the scanning phase, Provisiond will evaluate the policies in the foreign source definition and take appropriate action. If during the policy evaluation process any policy matches for a “DO_NOT_PERSIST” action, no further policy evaluations will happen for that particular entity (IP Interface, SNMP Interface).
Node Categorization Policy
With this policy, nodes entities will automatically be assigned categories. The policy is defined in the same manner as the IP and SNMP interface polices. Click the Add Policy button and give the policy name, “cisco” and choose the “Set Node Category” class. Edit the required “category” key and set the value to “Cisco”. Add a policy parameter and choose the “sysObjectId” key with a value “~^\.1\.3\.6\.1\.4\.1\.9\..*”.
Example: Node Category setting policy
New Import Capabilities
Several new XML entities have been added to the import requisition since the introduction of the OpenNMS Importer service in version 1.6. So, in addition to provisioning the basic node, interface, service, and node categories, you can now also provision asset data.
Provisiond Configuration
The configuration of the Provisioning system has moved from a properties file (model-importer.properties) to an XML based configuration container. The configuration is now extensible to allow the definition of 0 or more import requisitions each with their own Cron based schedule for automatic importing from various sources (intended for integration with external URL such as HTTP and this new DNS protocol handler.
A default configuration is provided in the OpenNMS etc/ directory and is called: provisiond-configuration.xml. This default configuration has an example for scheduling an import from a DNS server running on the localhost requesting nodes from the zone, localhost and will be imported once per day at the stroke of midnight.
Not very practical but is a good example.
&lt;?xml version="1.0" encoding="UTF-8"?> &lt;provisiond-configuration
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://xmlns.opennms.org/xsd/config/provisiond-configuration"
OpenNMS Provisioning	22
The OpenNMS Group, Inc.
foreign-source-dir="/opt/opennms/etc/foreign-sources" requistion-dir="/opt/opennms/etc/imports" importThreads="8" scanThreads="10" rescanThreads="10" writeThreads="8" >
&lt;!--
http://quartz.sourceforge.net/javadoc/org/quartz/CronTrigger.html
Field Name Seconds Minutes Hours Day-of-month Month Day-of-Week Year (Opt)
Allowed Values 0-59 0-59 0-23
1-31 1-12 or JAN-DEC 1-7 or SUN-SAT empty, 1970-2099
Allowed ,-* / ,-* / ,-* / ,-* ? ,-* / ,-* ? ,-* /
Special Characters
/LW C /LC #
--> &lt;requisition-def import-name="NMS" import-url-resource="file://opt/opennms/etc/imports/NMS.xml">
&lt;cron-schedule>0 0 0 * * ? *&lt;/cron-schedule> &lt;!-- daily, at midnight --> &lt;/requisition-def>
&lt;/provisiond-configuration>
Configuration Reload
Like many of the daemon configurations in the 1.7 branch, Provisiond’s configuration is re-loadable without having to restart OpenNMS. Use the reloadDaemonConfig uei:
/opt/opennms/bin/send-event.pl uei.opennms.org/internal/reloadDaemonConfig --parm 'daemonName Provisiond'
This means that you don't have to restart OpenNMS every time you update the configuration!
Provisioning Asset Data
The Provisioning Groups Web-UI had been updated to expose the ability to add Node Asset data in an import requisition. Click the “Add Node Asset” link and you can select from a drop down list all the possible node asset attributes that can be defined.
After an import, you can navigate to the Node Page and click the “Asset Info” link and see the asset data that was just provided in the requisition.
OpenNMS Provisioning	23
The OpenNMS Group, Inc.
External Requisition Sources
Because Provisiond takes a “URL” as the location service for import requisitions, OpenNMS can be easily extended to support sources in addition to the native URL handling provided by Java: “file://”, “http://”, and “https://”. When you configure Provisiond to import requisitions on a schedule you specify using a URL Resource. For requisitions created by the Provisioning Groups WebUI, you can specify a file based URL.
&lt;need further documentation>
Provisioning Nodes from DNS
The new Provisioning service in OpenNMS is continuously improving and adapting to the needs of the community. One of the most recent enhancements to the system is built upon the very flexible and extensible API of referencing an import requisition's location via a URL. Most commmonly, these URLs are files on the file system (i.e. file:/opt/opennms/etc/ imports/&lt;my-provisioning-group.xml>) as requisitions created by the Provisioning Groups UI. However, these same requistions for adding, updating, and deleting nodes (based on the original model importer) can also come from URLs specifying the HTTP protocol:
http://myinventory.server.org/nodes.cgi)
Now, using Java's extensible protocol handling specification, a new protocol handler was created so that a URL can be specified for requesting a Zone Transfer (AXFR) request from a DNS server. The A records are recorded and used to build an import requisition. This is handy for organizations that use DNS (possibly coupled with an IP management tool) as the data base of record for nodes in the network. So, rather than ping sweeping the network or entering the nodes manually into OpenNMS Provisioning UI, nodes can be managed via 1 or more DNS servers.
The format of the URL for this new protocol handler is:
dns://&lt;host>[:port]/&lt;zone>[/&lt;foreign-source>/][?expression=&lt;regex>]
Simple Example
dns://my-dns-server/myzone.com
This will import all A records from the host "my-dns-server" on port 53 (default port) from zone "myzone.com" and since the foreign source (a.k.a. the provisioning group) is not specified it will default to the specified zone.
Using a Regular Expression Filter
You can also specify a subset of the A records from the zone transfer using a regular expression:
dns://my-dns-server/myzone.com/portland/?expression=^por-.*
This will import all nodes from the same server and zone but will only manage the nodes in the zone matching the regular expression "^port-.*" and will and they will be assigned a unique foreign source (provisioning group) for managing these nodes as a subset of nodes from within the specified zone.
URL Encoding
If your expression requires URL encoding (for example you need to use a "?" in the expression) it must be properly encoded.
dns://my-dns-server/myzone.com/portland/?expression=^por[0-9]%3F
OpenNMS Provisioning	24
The OpenNMS Group, Inc.
DNS Setup
Currently, the DNS server requires to be setup to allow a zone transfer from the OpenNMS server. It is recommended that a secondary DNS server is running on OpenNMS and that the OpenNMS server be allowed to request a zone transfer. A quick way to test if zone transfers are working is:
dig -t AXFR @&lt;dnsServer> &lt;zone>
OpenNMS Provisioning	25
The OpenNMS Group, Inc.
Adapters
The OpenNMS Provisiond API also supports Provisioning Adapters (plugins) for integration with external systems during the provisioning Import phase. When node entities are added, updated, deleted, or receive a configuration management change event, OpenNMS will call the adapter for the provisioning activities with integrated systems.
Currently, OpenNMS supports the following adapters:
DDNS Adapter
The Opposite end of Provisiond integration from the DNS Requisition Import, is the DDNS adapter. This adapter uses the dynamic DNS protocol to update a DNS system as nodes are provisioned into OpenNMS. To configure this adapter, edit the opennms.properties file and set the importer.adapter.dns.server property:
importer.adapter.dns.server=192.168.1.1
RANCID Adapter
Integration has been integrated with RANCID though this new API. &lt;More documentation coming>
Maps (soon to be moved to Mapd)
&lt;documentation required>
WiMax-Link (soon to be moved to Linkd)
&lt;documentation required>
OpenNMS Provisioning	26
The OpenNMS Group, Inc.
Integrating with Provisiond
The ReST API should be used for integration from other provisioning systems with OpenNMS. The ReST API provides an interface for defining foreign sources and requisitions.
Provisioning Groups of Nodes
Just as with the WebUI, groups of nodes can be managed via the ReST API from an external system. The steps are: 1) Create a Foreign Source (if not using the default) for the group 2) Update the SNMP configuration for each node in the group 3) Create/Update the group of nodes
Example Step 1 (Create a Foreign Source)
If policies for this group of nodes are going to be specified differently than the default policy, then a foreign source should be created for the group. Using the ReST API, a foreign source can be provided. Here is an example:
Note: The XML can be imbedded in the curl command option -d or be referenced from a file if the ‘@’ prefix is used with the file name as in this case.
The XML file: customer-a.foreign-source.xml:
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;foreign-source date-stamp="2009-10-12T17:26:11.616-04:00" name="customer-a" xmlns="http:// xmlns.opennms.org/xsd/config/foreign-source">
&lt;scan-interval>1d&lt;/scan-interval> &lt;detectors>
&lt;detector class="org.opennms.netmgt.provision.detector.icmp.IcmpDetector" name="ICMP"/>
&lt;detector class="org.opennms.netmgt.provision.detector.snmp.SnmpDetector" name="SNMP"/> &lt;/detectors> &lt;policies>
&lt;policy class="org.opennms.netmgt.provision.persist.policies.MatchingIpInterfacePolicy" name="no-192-168">
&lt;parameter value="UNMANAGE" key="action"/> &lt;parameter value="ALL_PARAMETERS" key="matchBehavior"/> &lt;parameter value="~^192\.168\..*" key="ipAddress"/>
&lt;/policy> &lt;/policies>
&lt;/foreign-source>
Here is an example curl command used to create the foreign source with the above foreign source specification above:
curl -v -u admin:admin -X POST -H 'Content-type: application/xml' \ -d '@customer-a.foreign-source.xml' \ http://localhost:8980/opennms/rest/foreignSources
Now that you’ve created the foreign source, it needs to be deployed by Provisiond. Here an the example using the curl command to deploy the foreign source:
curl -v -u admin:admin \ -X PUT http://localhost:8980/opennms/rest/foreignSources/pending/customer-a/deploy
Note: The current API doesn’t strictly follow the ReST design guidelines and will be updated in a later release.
OpenNMS Provisioning	27
The OpenNMS Group, Inc.
Step 2 (Update the SNMP configuration)
The implementation only supports a PUT request because it is an implied "Update" of the configuration since it requires an IP address and all IPs have a default configuration. This request is is passed to the SNMP configuration factory in OpenNMS for optimization of the configuration store snmp-config.xml. This example changes the community string for the IP address 10.1.1.1 to yRuSonoZ.
Note: Community string is the only required element
curl -v -X PUT -H "Content-Type: application/xml" \ -H "Accept: application/xml" \ -d "&lt;snmp-info>
&lt;community>yRuSonoZ&lt;/community> &lt;port>161&lt;/port> &lt;retries>1&lt;/retries> &lt;timeout>2000&lt;/timeout> &lt;version>v2c&lt;/version>
&lt;/snmp-info>" \ -u admin:admin http://localhost:8980/opennms/rest/snmpConfig/10.1.1.1
Step 3 (Create/Update the Requisition)
This example adds 2 nodes to the Provisioning Group, “customer-a”. Note that the foreign-source attribute typically has a 1 to 1 relationship to the name of the Provisioning Group requisition. There is a direct relationship between the foreign- source attribute in the requisition and the foreign source policy specification. Also, typically, the name of the provisioning group will also be the same. In the following example, the ReST API will automatically create a provisioning group based on the value foreign-source attribute specified in the XML requisition.
curl -X POST \ -H "Content-Type: application/xml" \ -d "&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;model-import xmlns="http://xmlns.opennms.org/xsd/config/model-import" date-stamp="2009-03-07T17:56:53.123-05:00" last-import="2009-03-07T17:56:53.117-05:00" foreign-source="customer-a">
&lt;node node-label="p-brane" foreign-id="1" > &lt;interface ip-addr="10.0.1.3" descr="en1" status="1" snmp-primary="P">
&lt;monitored-service service-name="ICMP"/>
&lt;monitored-service service-name="SNMP"/> &lt;/interface> &lt;category name="Production"/> &lt;category name="Routers"/>
&lt;/node> &lt;node node-label="m-brane" foreign-id="1" >
&lt;interface ip-addr="10.0.1.4" descr="en1" status="1" snmp-primary="P"> &lt;monitored-service service-name="ICMP"/> &lt;monitored-service service-name="SNMP"/>
&lt;/interface> &lt;category name="Production"/> &lt;category name="Routers"/>
&lt;/node> &lt;/model-import>" \
-u admin:admin \ http://localhost:8980/opennms/rest/requisitions
A provisioning group file called “etc/imports/customer-a.xml” will be found on the OpenNMS system following the successful completion of this curl command and will also be visible via the WebUI.
Note: Add, Update, Delete operations are handled via the ReST API in the same manner as described in detailed specification.
Adding a Node to a Current Requisition
OpenNMS Provisioning	28
The OpenNMS Group, Inc.
Provisioning Single Nodes (Quick Add Node)
Often, it is requested that a single node add/update be completed for an already defined provisioning group. There is a ReST API for the “Add Node” implementation found in the OpenNMS Web-UI. For this to work, the provisioning group must already exist in the system even if there are no nodes defined in the group.
1) Create a foreign source (if required) 2) Specify SNMP configuration 3) Provide a single node with the following specification
OpenNMS Provisioning	29
The OpenNMS Group, Inc.
Fine Grained Provisioning Using “provision.pl”
We have created a perl script to help your team with this provisioning. It is in the /opt/opennms/bin/ directory when you install from our SNAPSHOT builds. The script has most all the operations you need for interfacing from WAVE and you should be able to use it or duplicate the code in WAVE. The options that are not available can be added to the script if you need them but everything is fully available in the REST interface. The script provides an easy interface to the REST API and should help a lot but making the examples easier to read and having code to inspect sometimes makes understanding the API much easier, as well.
The script /opt/opennms/bin/provision.pl, has many options but the first 3 optional parameters are described here: (note: you can use --help to the script to see all the available options).
--username (default: admin) --password (default: admin) --url (default: http://localhost:8980/opennms/rest)
We stand-by to help with any questions they may have. Additionally, we should get the latest software installed so that they can start testing. It would be good to have installs from the nightly SNAPSHOT builds so that we can keep it easily and quickly updated if there are any changes we have to make for you.
First, Create a new Provisioning Group
Provisioning Groups are created with import requisitions. The script provides an easy access to the REST API using the “requisition” option:
/opt/opennms/bin/provision.pl requisition customer1
This command will create a new requisition (provisioning group) in the /opt/opennms/etc/imports/pending/ directory. It will be an empty requisition (provisioning group). Empty meaning there will be the import definition only with no nodes. IMPORTANT NOTE: Notice that the group is in the "pending" directory. This allows you to iteratively create the group and then later actually import/provide the nodes in the group into OpenNMS. This hands all adds/changes/deletes at once. So, you could be making changes all day and then at night either have a schedule in OpenNMS that imports the group automatically or you can send a command through the REST service from WAVE to have the pending group imported/reimported. This is defined in the docs.
$ cat /opt/opennms/etc/imports/pending/customer1.xml &lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;model-import foreign-source="customer1"
date-stamp="2010-01-12T09:29:23.104-05:00"
xmlns="http://xmlns.opennms.org/xsd/config/model-import"> &lt;/model-import>
You can also get a list of all existing provisioning groups (import requisitions) with the “list” option of the provision.pl script:
/opt/opennms/bin/provision.pl list
Add a Node to an Existing Provisioning Group
Okay, the script we provided helps one to managed provisioning group elements at a very fine grained level. This example shows you how to handle adding a node and all the node elements with fine grained requests. Note, that you could create the resulting XML in WAVE and send the entire group as an XML document to the REST API as I've attempted to document in the docs. I will be including this example in a updated version of the docs, ASAP.
Create the Node Element
/opt/opennms/bin/provision.pl node add customer1 1 node-a
OpenNMS Provisioning	30
The OpenNMS Group, Inc.
This command creates a node element in the provisioning group (a.k.a requisition) "customer1" called "node-a" using the scripts “node” option. Note it has no interfaces or services, yet.
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;model-import foreign-source="customer1" date-stamp="2010-01-12T09:29:23.104-05:00" xmlns="http:// xmlns.opennms.org/xsd/config/model-import">
&lt;node node-label="node-a" foreign-id="1"/>
&lt;/model-import>
Add a Interface Element to that Node
/opt/opennms/bin/provision.pl interface add customer1 1 127.0.0.1
This command adds an interface element to the node element using the “interface” option to the provision.pl command and it can now be seen in the pending requisition:
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;model-import foreign-source="customer1" date-stamp="2010-01-12T09:31:21.029-05:00" xmlns="http:// xmlns.opennms.org/xsd/config/model-import">
&lt;node node-label="node-a" foreign-id="1">
&lt;interface ip-addr="127.0.0.1"/>
&lt;/node> &lt;/model-import>
Add a Couple of Services to that Interface
/opt/opennms/bin/provision.pl service add customer1 1 127.0.0.1 ICMP /opt/opennms/bin/provision.pl service add customer1 1 127.0.0.1 SNMP
This adds the 2 services to the specified 127.0.0.1 interface and is now in the pending XML document. NOTE: These Services must already be defined in the foreign-source definition for this "group". There is a default foreign source definition, btw. This is covered in the docs we provided.
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;model-import foreign-source="customer1" date-stamp="2010-01-12T09:32:14.885-05:00" xmlns="http:// xmlns.opennms.org/xsd/config/model-import">
&lt;node node-label="node-a" foreign-id="1"> &lt;interface ip-addr="127.0.0.1">
&lt;monitored-service service-name="ICMP"/>
&lt;monitored-service service-name="SNMP"/>
&lt;/interface> &lt;/node>
&lt;/model-import>
Set the Primary SNMP Interface
/opt/opennms/bin/provision.pl interface set customer1 1 127.0.0.1 snmp-primary P
This sets the 127.0.0.1 interface to be the Primary SNMP interface:
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;model-import last-import="2010-01-12T09:37:27.373-05:00" foreign-source="customer1" date- stamp="2010-01-12T11:12:23.738-05:00" xmlns="http://xmlns.opennms.org/xsd/config/model-import">
&lt;node node-label="node-a" foreign-id="1"> &lt;interface snmp-primary="P" ip-addr="127.0.0.1">
&lt;monitored-service service-name="ICMP"/>
&lt;monitored-service service-name="SNMP"/> &lt;/interface>
&lt;/node> &lt;/model-import>
Add a couple Node Categories
/opt/opennms/bin/provision.pl category add customer1 1 Routers /opt/opennms/bin/provision.pl category add customer1 1 Production
This adds the 2 categories to the node and is now in the pending XML document. NOTE: These categories are: a) case sensitive and b) do not have to already be defined in OpenNMS. They will be created on the fly during the import if they do not already exist.
OpenNMS Provisioning	31
The OpenNMS Group, Inc.
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;model-import foreign-source="customer1" date-stamp="2010-01-12T09:33:57.740-05:00" xmlns="http:// xmlns.opennms.org/xsd/config/model-import">
&lt;node node-label="node-a" foreign-id="1"> &lt;interface ip-addr="127.0.0.1">
&lt;monitored-service service-name="ICMP"/>
&lt;monitored-service service-name="SNMP"/> &lt;/interface>
&lt;category name="Servers"/>
&lt;category name="Production"/>
&lt;/node> &lt;/model-import>
Setting Asset Fields on a Node
/opt/opennms/bin/provision.pl asset add customer1 1 serialnumber 9999
This will add value of 9999 to the asset field: serialnumber:
&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?> &lt;model-import foreign-source="customer1" date-stamp="2010-01-12T09:35:48.343-05:00" xmlns="http:// xmlns.opennms.org/xsd/config/model-import">
&lt;node node-label="node-a" foreign-id="1"> &lt;interface ip-addr="127.0.0.1">
&lt;monitored-service service-name="ICMP"/>
&lt;monitored-service service-name="SNMP"/> &lt;/interface>
&lt;category name="Servers"/> &lt;category name="Production"/> &lt;asset value="9999" name="serialnumber"/>
&lt;/node> &lt;/model-import>
Deploy the Import Requisition (Creating the Group)
/opt/opennms/bin/provision.pl requisition import customer1
This will cause OpenNMS Provisiond to import the pending requisition. The XML document will moved from the /opt/ opennms/imports/pending directory to the /opt/opennms/imports directory. The philosophy is that the XML document in the imports/ directory should be reflective of what is actually supposed to be in the DB.
Deleting a Node from an Import Requisition (Provisioning Group)
Very much the same as the add, accept, a single delete command and a re-import is required. What happens is that the audit phase is run by Provisiond (this is detailed in the docs we sent) and it will be determined that a node has been removed from the group (requisition) and the node will be deleted from the DB and all services will stop activities related to it.
/opt/opennms/bin/provision.pl node delete customer1 1 node-a /opt/opennms/bin/provision.pl requisition import customer1
This, also, will create a copy of the currently deployed requisition, remove the node-a node element, and place it in the pending directory, so it too must be deployed so that the node is removed from the provisioning group.
/opt/opennms/bin/provision.pl requisition import customer1
This completes the life cycle of managing a node element, iteratively, in a import requisition.
OpenNMS Provisioning	32
The OpenNMS Group, Inc.
Yet Other API Examples List the Nodes in a Provisioning Group
The provision.pl script doesn't supply this feature but you can get it via the REST API. Here is an example using curl:
#!/bin/bash REQ=$1
curl -X -H -u
GET \ "Content-Type: application/xml" \ admin:admin \ http://localhost:8980/opennms/rest/requisitions/$REQ \ 2>/dev/null \ |xmllint --format -
OpenNMS Provisioning	33
</Text>
        </Document>
        <Document ID="85">
            <Title>Trapd</Title>
            <Text>Functionality
Trapd allows OpenNMS to receive snmp traps and turn them into events, and then into alarms and/or notifications, much as Syslogd does for syslog entries.
[edit] Availability
"Unknown" - Probably it's been there from the very beginning as it is a central function.
[edit] Configuration
trapd-configuration.xml
&lt;?xml version="1.0"?>
    &lt;trapd-configuration snmp-trap-port="162" new-suspect-on-trap="false"/>
The &lt;configuration> element specifies global parameters that specify how Trapd will receive and process snmp traps. This element has the following attributes:
	•	Port, Trapd port (udp) to listen on (Default 162)
	•	New suspect on messages specifies whether Trapd will inject a newSuspect event when it receives an snmp trap originating from a host that cannot be resolved to an existing node managed by OpenNMS. This functionality is equivalent to the same option in Syslogd. New suspects will be queued for discovery by Capsd.
[edit] Technical description
The service is built around traditional UDP based snmp traps, and the service is a receiver of traps created by remote devices.
[edit] How does it work
#
￼
Admiral Ackbar: SNMP Specialist
Question - What will happen as a snmp trap arrives?
Answer - The message is parsed, the sender compared to known nodes. If the sender is an unknown node and new-suspect-on-trap="true" is configured an event is sent to discover the node. Otherwise the trap is tagged to the correct node, broadcast to eventd, and from there on essentially is an openNMS event.
Question - Event how?
Answer - We convert the snmp trap to an openNMS Event, the matching event as usual is found in the etc/events directory.
I.e. for each trap received, it will get prioritized, categorized and submitted to the eventprocessor for notification or other configured actions.
[edit] More examples
Automating alarms and reducing amount of messages Syslogd_Automations
</Text>
        </Document>
        <Document ID="86">
            <Title>SyslogD Automations</Title>
            <Text>Introduction
Syslog messages and SNMP Traps, unfortunately, can get rather noisy when things go wrong. It is the nature of these monitoring protocols to provide system administrators with as much information as possible as quickly as possible. Thus, sending notifications on each, while effective, will often create a sea of angry on call staff! What follows in this document is a simple way (workflow) of turning a slew of Syslog messages into:
	•	one alarm
	•	one notification
	•	a trouble ticket with alarm data for the follow-up
[edit] Building Alarms
Alarms are based on one criteria in the event.
&lt;alarm-data reduction-key="%uei%:%dpname%:%nodeid%" alarm-type="1"  />
You need to add this to the events that you want to build alarms from. You can easily do this with a search / replace in your favorite editor.
[edit] Defining the Automation
&lt;VacuumdConfiguration period="86400000" >
  &lt;statement>
    &lt;!-- this deletes all the nodes that have been marked as deleted - it relies on cascading deletes -->
    DELETE FROM node WHERE node.nodeType = 'D';
  &lt;/statement>
   
  &lt;statement>
    &lt;!-- this deletes all the interfaces that have been marked as deleted - it relies on cascading deletes -->
    DELETE FROM ipInterface WHERE ipInterface.isManaged = 'D';
  &lt;/statement>
   
  &lt;statement>
    &lt;!-- this deletes all the services that have been marked as deleted - it relies on cascading deletes -->
    DELETE FROM ifServices WHERE ifServices.status = 'D';
  &lt;/statement>
   
  &lt;statement>
    &lt;!-- this deletes any events that are not associated with outages - Thanks to Chris Fedde for this -->
    DELETE FROM events WHERE NOT EXISTS 
      (SELECT svclosteventid FROM outages WHERE svclosteventid = events.eventid  
    UNION 
      SELECT svcregainedeventid FROM outages WHERE svcregainedeventid = events.eventid 
    UNION 
      SELECT eventid FROM notifications WHERE eventid = events.eventid) 
    AND eventtime &lt; now() - interval '2 weeks';
  &lt;/statement>
    
  &lt;automations>
    &lt;automation name="cosmicClear" interval="30000" trigger-name="selectResolvers" action-name="clearProblems" active="true" />
    &lt;automation name="cleanUp" interval="30000" action-name="deletePastClearedAlarms" active="true" />
    &lt;automation name="cleanUpAcks" interval="30000" action-name="deleteAckedAlarms" active="true" />
    &lt;automation name="cleanUpSyslog" interval="30000" action-name="deleteNonInterestingSyslogAlarms" active="true" />
    &lt;automation name="cleanUpUnknown" interval="30000" action-name="deleteUnknownSyslogAlarms" active="true" />
    &lt;automation name="GC" interval="300000" action-name="garbageCollect" active="true" />
    &lt;automation name="unclear" interval="30000" trigger-name="selectClearedAlarms" action-name="resetSeverity" active="true" />
    &lt;automation name="escalation" interval="30000" trigger-name="selectSuspectAlarms" action-name="escalateAlarm" action-event="eventEscalated" active="false" />
    &lt;automation name="criticalSyslogMessages" interval="30000" trigger-name="selectSyslogMessages" action-name="doNothing" action-event="syslogCritical" active="true" />
    &lt;automation name="criticallog4jMessages" interval="30000" trigger-name="selectlog4jMessages" action-name="doNothing" action-event="log4jCritical" active="true" />
    &lt;automation name="purgeStatisticsReports" interval="3600000" action-name="deletePurgeableStatisticsReports" active="true" />
  &lt;/automations>
  
  &lt;triggers>


	&lt;trigger name="selectlog4jMessages" operator=">=" row-count="1" >

		&lt;statement>
                select a.alarmid as _id,
                 a.eventuei as _eventuei,
                 a.nodeid as _nodeid,
                 a.ipaddr as _ipaddr,
                 a.serviceid as _serviceid,
                 s.servicename as _servicename,
                 a.severity as _sev,
                 a.logmsg as _logmsg,
                 now() as _ts
                FROM alarms a
                 LEFT OUTER JOIN service s
              ON s.serviceid = a.serviceid
                   WHERE alarmType = 1
             AND alarmacktime IS NULL
             AND firstautomationtime is NULL
             AND logmsg like '%log4j:%'
	&lt;/statement>
	&lt;/trigger>

	&lt;trigger name="selectSyslogMessages" operator=">=" row-count="1" >

	&lt;statement>
		select a.alarmid as _id,
                 a.eventuei as _eventuei,
                 a.nodeid as _nodeid,
                 a.ipaddr as _ipaddr,
                 a.serviceid as _serviceid,
                 s.servicename as _servicename,
                 a.severity as _sev,
                 a.logmsg as _logmsg,
                 now() as _ts
                FROM alarms a
                 LEFT OUTER JOIN service s
              ON s.serviceid = a.serviceid
                   WHERE alarmType = 1
             AND alarmacktime IS NULL
             AND firstautomationtime is NULL 
	     AND logmsg not like '%snort:%'
	     AND logmsg not like '%portscan%'
	     AND logmsg not like '%pam_ldap%'
	     AND logmsg not like '%Authentication failure%'
	     AND logmsg not like '%password authentication failed for user%'
	     AND logmsg not like '%AF_INET%
             AND eventuei &lt;> 'uei.opennms.org/syslogd/alarmCreated'
             AND eventuei not like 'uei.opennms.org/syslogd/unknown%'
             AND eventuei not like 'uei.opennms.org/syslogd/unknown%'
             AND (eventuei like 'uei.opennms.org/syslogd/%/Emergency'
             OR  eventuei like 'uei.opennms.org/syslogd/%/Alert'
             OR  eventuei like 'uei.opennms.org/syslogd/%/Error'
             OR  eventuei like 'uei.opennms.org/syslogd/%/Critical');
	&lt;/statement>
	&lt;/trigger>
  
    &lt;trigger name="selectSuspectAlarms" operator=">=" row-count="1" >
      &lt;statement>
          SELECT a.alarmid as _alarmid, 
                 a.eventuei as _eventuei, 
                 a.nodeid as _nodeid, 
                 a.ipaddr as _ipaddr, 
                 a.serviceid as _serviceid,
                 s.servicename as _servicename,
                 now() as _ts
            FROM alarms a
 LEFT OUTER JOIN service s
              ON s.serviceid = a.serviceid
           WHERE alarmType = 1
             AND severity > 3
             AND severity &lt; 7
             AND alarmacktime IS NULL
             AND COALESCE(lastautomationtime, lasteventtime) &lt; now() - interval '60 minutes'
      &lt;/statement>
    &lt;/trigger>
  
    &lt;trigger name="selectClearedAlarms" operator=">=" row-count="1" >
      &lt;statement>
        SELECT a.alarmid as _id, e.eventseverity AS _sev, now() as _ts
          FROM alarms a
          JOIN events e 
            ON e.eventid = a.lasteventid
         WHERE severity = 2
           AND alarmtype = 1
           AND a.lasteventtime > a.lastautomationtime
      &lt;/statement>
    &lt;/trigger>
  
    &lt;trigger name="selectResolvers" operator=">=" row-count="1" >
      &lt;statement>
        SELECT *, now() as _ts 
          FROM alarms 
         WHERE alarmType=2
      &lt;/statement>
    &lt;/trigger>
  &lt;/triggers>
    
  &lt;actions>
  
    &lt;action name="escalateAlarm" >
      &lt;statement>
        UPDATE alarms
           SET severity = severity +1, firstautomationtime = COALESCE(firstautomationtime, ${_ts}), lastautomationtime = ${_ts}
         WHERE alarmid = ${_alarmid}
      &lt;/statement>
    &lt;/action>
  
    &lt;action name="resetSeverity" >
      &lt;statement>
        UPDATE alarms
           SET severity = ${_sev}, firstautomationtime = COALESCE(firstautomationtime, ${_ts}), lastautomationtime = ${_ts}
         WHERE alarmid = ${_id}
      &lt;/statement>
    &lt;/action>

    &lt;action name="doNothing" >
      &lt;statement>
        UPDATE alarms
           SET severity = ${_sev}, firstautomationtime = COALESCE(firstautomationtime, ${_ts}), lastautomationtime = ${_ts}
         WHERE alarmid = ${_id}
      &lt;/statement>
    &lt;/action>
    &lt;!-- action used for postgres 7.4 compatibility -->
    &lt;action name="garbageCollect" >
      &lt;statement>
        DELETE FROM alarms
         WHERE COALESCE(lastautomationtime, lasteventtime) &lt; now() - interval '5 days'
           AND alarmacktime IS NULL
      &lt;/statement>
    &lt;/action>
  
    &lt;!-- a better action when using postgres 8.1 
    &lt;action name="garbageCollect" >
      &lt;statement>
        DELETE FROM alarms
         WHERE GREATEST(lastautomationtime, lasteventtime) &lt; now() - interval '5 days'
           AND alarmacktime IS NULL
      &lt;/statement>
    &lt;/action>
    -->
  
    &lt;action name="deletePastClearedAlarms" >
      &lt;statement>
        DELETE from alarms
         WHERE severity &lt;= 3
           AND COALESCE(lastautomationtime, lasteventtime) &lt; now() - interval '2 minutes'
           AND alarmacktime IS NULL
      &lt;/statement>
    &lt;/action>

    &lt;action name="deleteAckedAlarms" >
      &lt;statement>
        DELETE from alarms
         WHERE severity &lt;= 7
           AND COALESCE(lastautomationtime, lasteventtime) &lt; now() - interval '1 minute'
           AND alarmacktime IS not NULL
      &lt;/statement>
    &lt;/action>

    &lt;action name="deleteUnknownSyslogAlarms">
	&lt;statement>
		DELETE from alarms
 		 WHERE alarmType = 1
			AND alarmacktime IS NULL
			AND  eventuei like 'uei.opennms.org/syslogd/unknown%'
	&lt;/statement>
	&lt;/action>

    &lt;action name="deleteNonInterestingSyslogAlarms">
	&lt;statement>
		DELETE from alarms
 		 WHERE alarmType = 1
			AND alarmacktime IS NULL
             		AND eventuei not like 'uei.opennms.org/syslogd/%/Emergency'
			AND  eventuei not like 'uei.opennms.org/syslogd/%/Alert'
			AND  eventuei not like 'uei.opennms.org/syslogd/%/Error'
			AND  eventuei not like 'uei.opennms.org/syslogd/%/Critical'
			AND  eventuei like 'uei.opennms.org/syslogd%'
	&lt;/statement>
	&lt;/action>
 
&lt;!--
    &lt;action name="clearProblems" >
      &lt;statement>
        UPDATE alarms 
           SET severity=2, firstautomationtime = COALESCE(firstautomationtime, ${_ts}), lastautomationtime = ${_ts}
         WHERE alarmType=1 
           AND severity > 2 
           AND lastEventTime &lt;  ${lastEventTime} 
           AND eventUei = ${clearUei} 
           AND COALESCE(dpName, '') = COALESCE(${dpName}, '') 
           AND COALESCE(nodeID, 0) = COALESCE(${nodeID}, 0) 
           AND COALESCE(ipaddr, '') = COALESCE(${ipaddr}, '') 
           AND COALESCE(serviceID, 0) = COALESCE(${serviceID}, 0)
      &lt;/statement> 
    &lt;/action>
-->

    &lt;!--  New and optimized version of clearing problems -->
    &lt;action name="clearProblems" >
      &lt;statement>
        UPDATE alarms 
           SET severity=2, firstautomationtime = COALESCE(firstautomationtime, ${_ts}), lastautomationtime = ${_ts}
         WHERE alarmType=1 
           AND severity > 2 
           AND lastEventTime &lt;  ${lastEventTime} 
           AND reductionKey = ${clearKey} 
      &lt;/statement> 
    &lt;/action>
    
    &lt;action name="deletePurgeableStatisticsReports" >
      &lt;statement>
        DELETE from statisticsReport
         WHERE purgeDate &lt; now()
      &lt;/statement>
    &lt;/action>

  &lt;/actions>

  &lt;!--  Deprecating this element... see the new &lt;action-events> element -->    
  &lt;auto-events>
    &lt;auto-event name="escalationEvent" >
      &lt;uei>uei.opennms.org/vacuumd/alarmEscalated&lt;/uei>
    &lt;/auto-event>
  &lt;/auto-events>

  &lt;!-- Note: action events that have org.apache.maven.project.MavenProject@61bd1a3c tokens require the for-each-result attribute to be set to true -->
  &lt;action-events>
    &lt;action-event name="eventEscalated" for-each-result="true" >
      &lt;assignment type="field" name="uei" value="uei.opennms.org/vacuumd/alarmEscalated" />
      &lt;assignment type="field" name="nodeid" value="${_nodeid}" />
      &lt;assignment type="field" name="interface" value="${_ipaddr}" />
      &lt;assignment type="field" name="service" value="${_servicename}" />
      &lt;assignment type="parameter" name="alarmId" value="${_alarmid}" />
      &lt;assignment type="parameter" name="alarmEventUei" value="${_eventUei}" />
    &lt;/action-event>

  &lt;action-event name="syslogCritical" for-each-result="true" >
      &lt;assignment type="field" name="uei" value="uei.opennms.org/syslogd/alarmCreated" />
      &lt;assignment type="field" name="nodeid" value="${_nodeid}" />
      &lt;assignment type="field" name="interface" value="${_ipaddr}" />
      &lt;assignment type="field" name="service" value="${_servicename}" />
      &lt;assignment type="parameter" name="logmsg" value="${_logmsg}" />
      &lt;assignment type="parameter" name="alarmId" value="${_id}" />
      &lt;assignment type="parameter" name="alarmEventUei" value="${_eventUei}" />
    &lt;/action-event>
  &lt;action-event name="log4jCritical" for-each-result="true" >
      &lt;assignment type="field" name="uei" value="uei.opennms.org/syslogd/log4jCreated" />
      &lt;assignment type="field" name="nodeid" value="${_nodeid}" />
      &lt;assignment type="field" name="interface" value="${_ipaddr}" />
      &lt;assignment type="field" name="service" value="${_servicename}" />
      &lt;assignment type="parameter" name="logmsg" value="${_logmsg}" />
      &lt;assignment type="parameter" name="alarmId" value="${_id}" />
      &lt;assignment type="parameter" name="alarmEventUei" value="${_eventUei}" />
    &lt;/action-event>
  &lt;/action-events>

&lt;/VacuumdConfiguration>

[edit] The events
  &lt;event-label>Syslog Alarm event: alarmCreated&lt;/event-label>
      &lt;descr>
		Message: %parm[syslogmessage]% &lt;br>
        &lt;p>Alarm #&lt;a href="/opennms/alarm/detail.jsp?id=%parm[alarmId]%">%parm[alarmId]%&lt;/a>
        was created&lt;/p>
	%parm[all]%
      &lt;/descr>
      &lt;logmsg dest='logndisplay'>
	Message: %parm[syslogmessage]% &lt;br>
        &lt;p>Alarm #&lt;a href="/opennms/alarm/detail.jsp?id=%parm[alarmId]%">%parm[alarmId]%&lt;/a>
        for node:%nodelabel%; interface:%interface%; was created&lt;/p>
	%parm[logmsg]%
	&lt;br>
	The whole event: 
	%parm[all]%
      &lt;/logmsg>
 	     &lt;severity>Major&lt;/severity>
    &lt;/event>


 &lt;event>
      &lt;uei>uei.opennms.org/syslogd/log4jCreated&lt;/uei>
      &lt;event-label>Log4j Alarm event: log4jCreated&lt;/event-label>
      &lt;descr>
		Message: %parm[syslogmessage]% &lt;br>
        &lt;p>Alarm #&lt;a href="/opennms/alarm/detail.jsp?id=%parm[alarmId]%">%parm[alarmId]%&lt;/a>
        was created&lt;/p>
	%parm[all]%
      &lt;/descr>
      &lt;logmsg dest='logndisplay'>
	Message: %parm[syslogmessage]% &lt;br>
        &lt;p>Alarm #&lt;a href="/opennms/alarm/detail.jsp?id=%parm[alarmId]%">%parm[alarmId]%&lt;/a>
        for node:%nodelabel%; interface:%interface%; was created&lt;/p>
	%parm[logmsg]%
	&lt;br>
	The whole event: 
	%parm[all]%
      &lt;/logmsg>
 	     &lt;severity>Major&lt;/severity>
  &lt;/event>

[edit] The Notifications
    &lt;notification name="Jabber SyslogAlarm Created" status="on" writeable="yes">
        &lt;uei xmlns="">uei.opennms.org/syslogd/alarmCreated&lt;/uei>
        &lt;description xmlns="">Jabber SyslogAlarmCreated&lt;/description>
        &lt;rule xmlns="">(IPADDR != '0.0.0.0')&lt;/rule>
        &lt;destinationPath xmlns="">Jabber&lt;/destinationPath>
        &lt;text-message xmlns="">Alarm - a compound syslog message has been generated.
%severity% %nodelabel% : %noticeid% %parm[all]%
Any new messages will be added to this alarm.
Please investigate this issue and create a Jira.&lt;/text-message>
        &lt;subject xmlns="">Notice #%noticeid%&lt;/subject>
    &lt;/notification>

&lt;notification name="Jabber log4j Alarm" status="on" writeable="yes">
        &lt;uei xmlns="">uei.opennms.org/syslogd/log4jCreated&lt;/uei>
        &lt;description xmlns="">Jabber SyslogAlarmCreated&lt;/description>
	&lt;rule xmlns="">(IPADDR != '0.0.0.0') &amp; (notifyCategory != 'qa')&lt;/rule>
        &lt;destinationPath xmlns="">Jabber&lt;/destinationPath>
	&lt;text-message xmlns="">log4j Alarm - a compound log4j message has been generated.
%severity% %nodelabel% : %noticeid% %parm[all]%
Any new messages will be added to this alarm.
Please investigate this issue and create a Jira.&lt;/text-message>
        &lt;subject xmlns="">Notice #%noticeid%&lt;/subject>
    &lt;/notification>

</Text>
        </Document>
        <Document ID="90">
            <Title>SNMP Asset provisioning</Title>
            <Text>Summary
OpenNMS can collect SNMP values and use them to populate asset records for systems that you are managing. This can be very useful for automatically populating asset fields in a large number of managed systems. Depending on the SNMP MIBs that your devices support, you could either fill in some of the existing asset fields (manufacturer, vendor, modelnumber, serialnumber, etc.) or construct a custom-formatted message for the longer comment field.
[edit] Configuration
To start using the SNMP asset adapter, you must install the opennms-snmp-asset-provisioning-adapter-[version].jar into OpenNMS's lib directory. After you do that, it is just a matter of defining the fields in the snmp-asset-adapter-configuration.xml configuration file.
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;snmp-asset-adapter-configuration>
  &lt;package name="ciscoWireless">
    &lt;!-- A Cisco wireless sysoid, for example purposes only :) -->
    &lt;sysoid>.1.3.6.1.4.1.9.1.379&lt;/sysoid>
    &lt;assetField name="region" formatString="${gsmCountry}">
      &lt;mibObjs>
        &lt;!-- A Cisco 3G WAN adapter OID -->
        &lt;mibObj oid=".1.3.6.1.4.1.9.9.661.1.3.2.1.8" alias="gsmCountry"/>
      &lt;/mibObjs>
    &lt;/assetField>
  &lt;/package>
&lt;/snmp-asset-adapter-configuration>
In this configuration example, every node whose sysoid equals .1.3.6.1.4.1.9.1.379 will be matched by the "ciscoWireless" package. This package is configured to set the value of the region asset field. The value that is set is defined by a formatted string. Tokens in the string using the ${token} syntax will be replaced by values that are defined by the list of mibObj parameters.
In this example, if an SNMP get of the OID value .1.3.6.1.4.1.9.9.661.1.3.2.1.8 returns something like "UK", then the "region" field in the asset database will end up being set to "UK" since there is just a single token in the formatting string.
It's that simple! You may want to snmpwalk your target nodes and see if there are any interesting identifiers or string values that you would like to see in an asset field first. Then just define them under a package that matches based on sysoid and you will be set to start collecting data.
[edit] Operation
The SNMP asset provisioning adapter is invoked every time a node is added or updated by provisiond. With the default provisiond configuration, the update will take place once every 24 hours.
[edit] Bugs Addressed By This Feature
Bug 2132: Auto assign asset columns
Bug 3337: Display collected info on Node page
Bug 3492: Automatically populating asset fields
[edit] Version History/Availability
	•	This feature was added in version 1.8.2
</Text>
        </Document>
        <Document ID="87">
            <Title>Provisioner</Title>
            <Text>OpenNMS has a number of ways to perform automated discovery, but sometimes it is better to just tell OpenNMS what to monitor. A case in point would be a remote router accessed via DSL where the IP address is set via DHCP. If that address changes, OpenNMS may have no way of knowing that automatically.
Provisioning Groups is an administrative GUI option that provides a front end to the Importer Service. It works by allow a "group" to be created that consists of "nodes". For each node, the IP address and services on that IP address can be set manually. Access this GUI via Admin -> Manage Provisioning Groups.
#
Add a new group, in this case "Wiki Example Group" and press the "Add New Group" button.
At this point it will be added to the list of groups. Click on the group name (Wiki Example Group) to be able to add devices to it.
Add the Node (setting the name, etc.), add interfaces, and add services to those interfaces. The "ForeignId" key will be randomly generated and can be left alone, but it is very important since it indicates the unique key that will identify the device in the system. Usually when using the Importer this key will relate the OpenNMS node to the device in the remote database.
￼
Press the Done button when finished and it will return to the main Provisioning Groups menu.
The remaining step is to hit the "Synchronize" link which will cause all of the devices in the group to be added to OpenNMS. From this point on they are managed just like any other device in the system.
If anything should change on these devices, such as an IP address, simply return to this screen and click on the group name to edit it. When finished, re-import the file and the changes will take effect immediately.
</Text>
        </Document>
        <Document ID="91">
            <Title>Poller Configuration Howto</Title>
            <Text>Introduction
[edit] Purpose
This How-To is one in a series designed to serve as a reference for getting started with OpenNMS. Eventually, these documents will cover everything necessary to get OpenNMS installed and running in your environment.
[edit] Copyright
Content is available under a Creative Commons Attribution-NonCommercial-ShareAlike2.5 License.
[edit] Corrections and Omissions
Please submit any corrections and omissions to the author.
[edit] Overview
OpenNMS is an enterprise-grade network management platform developed under the open-source model. Unlike traditional network management products which are very focused on network elements such as interfaces on switches and routers, OpenNMS focuses on the services network resources provide: web pages, database access, DNS, DHCP, etc. (although information on network elements is also available).
There are two major ways that OpenNMS gathers data about the network. The first is through polling. Processes called monitors connect to a network resource and perform a simple test to see if the resource is responding correctly. If not, events are generated. The second is through data collection using collectors. Currently, the only collector is for SNMP data, and it will be covered in another How-To.
The basic idea behind the poller starts with grouping network devices into packages. Each package will consist of various services and how they are to be polled (i.e. frequency). In addition, should an outage be detected, each package can have its own downtime model which controls how the poller will dynamically adjust its polling on services that are down. Finally, each package has an outage calendar that schedules times when the poller is not to poll (i.e. scheduled downtime).
The poller will only operate on interfaces and services that have been previously discovered by capsd (see the Discovery How-To for information on configuring that process).
[edit] Polling
[edit] The Poller Configuration File Header
Polling in OpenNMS is controlled by the poller-configuration.xml file (located in the /opt/OpenNMS/etc directory).
Let's look at that file:
&lt;poller-configuration threads="30" serviceUnresponsiveEnabled="false">
        &lt;node-outage status="on"
                     pollAllIfNoCriticalServiceDefined="true">
                &lt;critical-service name="ICMP"/>
        &lt;/node-outage>
There are three basic behaviors that are configured in the header of this file:
poller-configuration threads 
This determines the maximum number of threads that will be used for polling, and can be adjusted up or down depending on the size of your network and the power of your server. (See Performance tuning#Poller threads.)
serviceUnresponsiveEnabled 
A poll consists of a connection to a particular port on a remote interface, and then a test to see if the service on that port returns an expected response. If the response is not received within the timeout, the service is considered down. In some networks, however, short, intermittent failures are common. This will result in what is known as a "30 second outage". Due to the default downtime model, a failed service will be polled again in 30 seconds. Note that this is a real problem: a user attempting to access that resource would also have experienced a timeout. But in some networks these 30 second outages can be annoying yet hard to correct. So the option was added to denote a failure as when the port connection fails and not the response. In this case, an unresponsive service does not generate an outage, but only a "service unresponsive" event. To enable this behavior, set this value to "true".
node-outage 
The basic event that is generated when a poll fails is called "NodeLostService". If more than one service is lost, multiple NodeLostService events will be generated. If all the services on an interface are down, instead of a NodeLostService event, an "InterfaceDown" event will be generated. If all the interfaces on a node are down, the node itself can be considered down, and this section of the configuration file controls the poller behavior should that occur. If a "NodeDown" event occurs and node-outage status="on" then all of the InterfaceDown and NodeLostService events will be suppressed and only a NodeDown event will be generated. Instead of attempting to poll all the services on the down node, the poller will attempt to poll only the critical-service, by default ICMP. Once the critical service returns, the poller will then resume polling the other services. If the critical service is not available on a node, the pollAllIfNoCriticalServiceDefined parameter controls the behavior. If set to "true" then all services will be polled. If set to "false" then the first service in the package that exists on the node will be polled until service is restored, and then polling will resume for all services.
Note that any changes to this file will not take affect until OpenNMS is restarted.
[edit] Poller Packages
A poller package consists of a name, a group of interfaces to poll, and the services to be polled on those interfaces. Multiple packages can be configured, and an interface can exist in more than one package (although the value of that is questionable). This gives great flexibility to how the service levels will be determined for a given device.
For example, you could build packages based upon different polling rates, say, "gold", "silver" and "bronze". In the gold package, services would be polled every minute, the silver package every five minutes and the bronze package every fifteen. Or, you could build different poller packages based upon levels of monitoring. A "basic" package may just poll ICMP and HTTP, whereas a "deluxe" package would include databases, etc..
In addition to a list of services, each package can have a "downtime" model and an "outage calendar", both discussed below.
The definition of a package starts with a package tag:
&lt;package name="example1">
This is followed by a list of tags that define what interfaces will be included in the package. There are five of these tags:
filter 

IPADDR IPLIKE *.*.*.*
Each package must have a filter tag that performs the initial test to see if an interface should be included in a package. Filters operate on interfaces (not nodes) and are discussed on the filters page. Only one filter statement can exist per package.
specific 

&lt;specific>192.168.1.59&lt;/specific>
This specifies a particular IP address to include in a package.
include-range 

&lt;include-range begin="192.168.0.1" end="192.168.0.254"/>
This specifies a particular range of IP addresses to include in a package.
exclude-range 

&lt;exclude-range begin="192.168.0.100" end="192.168.0.104"/>
This specifies a particular range of IP addresses to exclude in a package. This will override an include-range tag.
include-url 

&lt;include-url>file:/opt/OpenNMS/etc/include&lt;/include-url>
This tag will point to a file that consists of a list of IP addresses, one to a line, that will be included in the package. Comments can be imbedded in this file. Any line that begins with a "#" character will be ignored, as will the remainder of any line that includes a space followed by "#".
All of the above tags, except for filter, are optional and unbounded.
[edit] Poller Services
Once the IP addresses to include in a package are defined, the services to be polled are listed. For example:
&lt;service name="DNS" interval="300000" user-defined="false" status="on">
  &lt;parameter key="retry" value="3"/>
  &lt;parameter key="timeout" value="5000"/>
  &lt;parameter key="port" value="53"/>
  &lt;parameter key="lookup" value="localhost"/>
&lt;/service>
The common parameters for the poller service are as follows:
retry 
The number of attempts that will be made to connect to the service. Default is
timeout 
The amount of time, in milliseconds, that OpenNMS will wait for a response from the service. Default is
port 

lookup 

This will poll the DNS service once every five minutes (300,000 ms). The rest of the block is similar to the corresponding block in the capsd configuration. Since users can define new services to be polled, the user-defined attribute indicates this for a particular service. Polling can also be universally stopped for a particular service, indicated by the status tag. Note that the service as defined in the poller can be different from the one defined in capsd. You may want a longer timeout during discovery, for example. Also, in this example, a DNS request will be made to look up "localhost". This should return an error (as localhost is usually not listed in a DNS) but if that error is returned, DNS is functioning properly and the test passes. Microsoft's implementation of DNS, however, sometimes has problems with this, so you may want to put a real host for the lookup value (and in capsd as well).
There must be at least one service defined per package.
[edit] Poller Outage Calendar
(In versions newer than 1.5.x this feature is called "scheduled outages")
In order to keep servers operating properly, it is often necessary to bring them down for scheduled maintenance. Instead of having these maintenance outages reflected as a true service outage, they can be included in an "Poller Outage Calendar" and then referenced by the poller package using the outage-calendar tag. This tag contains the name of a valid outage in the poll-outages.xml file.
The outage-calendar tag is optional and unbounded (i.e. you can reference more than one outage).
Since version 1.5.91 you can configure scheduled outages from the GUI, got to Admin -> Scheduled Outages.
Before version 1.5.91, there were three types of outages: weekly, monthly and specific. Since 1.5.91 there is also the possibility to configure daily outages.
If you have the problem that nodes are reported to be down thought they are within a daily outage which goes past midnight try to define two timespans within the outage, one until midnight and the other one starting after midnight, e.g. instead of outage 22:00:00-01:00:00 define 22:00:00-23:59:59 and 00:00:00-01:00:00.
 Examples from the poll-outages file:
&lt;outage name="global" type="weekly">
  &lt;time day="sunday" begins="12:30:00" ends="12:45:00"/>
  &lt;time day="sunday" begins="13:30:00" ends="14:45:00"/>
  &lt;time day="monday" begins="13:30:00" ends="14:45:00"/>
  &lt;time day="tuesday" begins="13:00:00" ends="14:45:00"/>
  &lt;interface address="192.168.0.1"/>
  &lt;interface address="192.168.0.36"/>
  &lt;interface address="192.168.0.38"/>
&lt;/outage>
This defines an outage calendar called "global" that is run every week. It specifies four outage times: Sunday starting at 12:30 pm and lasting 15 minutes, Sunday starting at 1:30 pm and lasting an hour and fifteen minutes, the same outage on Monday, and one on Tuesday from 1:00 pm to 2:45 pm. This is to demonstrate that you can have multiple outages on a given day and the same outage on different days. Three interfaces will be affected.
&lt;outage name="hub maintenance" type="monthly">
  &lt;time day="1" begins="23:30:00" ends="23:45:00"/>
  &lt;time day="15" begins="21:30:00" ends="21:45:00"/>
  &lt;time day="15" begins="23:30:00" ends="23:45:00"/>
  &lt;interface address="192.168.100.254"/>
  &lt;interface address="192.168.101.254"/>
  &lt;interface address="192.168.102.254"/>
  &lt;interface address="192.168.103.254"/>
  &lt;interface address="192.168.104.254"/>
  &lt;interface address="192.168.105.254"/>
  &lt;interface address="192.168.106.254"/>
  &lt;interface address="192.168.107.254"/>
&lt;/outage>
This outage calendar is called "hub maintenance" that is run every month. On the first of the month the outage begins at 11:30 pm and lasts 15 minutes. The same outage occurs on the 15th of the month in addition to another outage from 9:30 pm to 9:45 pm. Thus you can have the same outage on different dates as well as more than one outage on a particular date. Eight interfaces are affected by this outage.
&lt;outage name="proxy server tuning" type="specific">
  &lt;time begins="10-Nov-2001 17:30:00" ends="11-Nov-2001 08:00:00"/>
  &lt;interface address="192.168.0.1"/>
&lt;/outage>
It is also possible to include an outage on a specific date and time. This outage named "proxy server tuning" began on November 10th, 2001 at 5:30 pm and lasted until 8:00 am the next day. This affected one interface. You can have more than one "time" entry per specific outage.
If a particular outage calendar is included in a poller package, then polling will not occur during this time. Note that this does not mean that the service will be considered "up" during this time. If the maintenance is started a minute too soon and an outage is detected, then no poll will be made to restore the service until after the outage window has closed.
[edit] Downtime Models
One of the most powerful features of the OpenNMS poller is its downtime models. The goal of the poller is to verify service levels, and everyone involved would like to see those be as high as possible. By default, the poller will poll every five minutes. If that polling rate was static, then the shortest an outage could be would be five minutes: one poll to note the outage and the next to note it was restored. In these days of service levels in the "99.99%" range, a five minute outage can be devastating. You might as well guarantee "100%" availability since any outage will break your service level agreement.
To help combat this, OpenNMS uses adaptive polling. Once an outage is detected, polling is temporarily increased to try and detect, as soon as possible, when the service is restored.
&lt;downtime interval="30000" begin="0" end="300000"/>             &lt;!-- 30s, 0, 5m -->
&lt;downtime interval="300000" begin="300000" end="43200000"/>     &lt;!-- 5m, 5m, 12h -->
&lt;downtime interval="600000" begin="43200000" end="432000000"/>  &lt;!-- 10m, 12h, 5d -->
&lt;downtime begin="432000000" delete="true"/>                     &lt;!-- anything after 5 days delete -->
What this downtime model will do is the following: from the moment the outage begins (time 0) until five minutes later (time 300,000 ms), the poller will poll every 30 seconds (30,000 ms). After five minutes, it is assumed that any service level that would be greatly affected by a five minute outage has been broken, so from five minutes (300,000 ms) into the outage until the first 12 hours of the outage (43,200,000 ms) polling resumes its five minute (300,000 ms) interval.
If the outage is older than 12 hours, it must not be important and/or it is difficult to fix, so from when the outage is 12 hours old until it is 5 days (432,000,000 ms) old, the interval is reduced to poll once every ten minutes (600,000 ms).
If a service has been down for longer than five days, it is deleted (well, marked as "forced unmanaged") and no longer polled. Note that this is optional, you can continue to poll a down service for as long as you would like. For the last downtime interval in the model, just leave the "end" time off in order to extend polling indefinitely.
[edit] Poller Monitors
For each service in a poller package, there must be a corresponding monitor. In the capsd configuration, this was included on the service line itself, but since there is the potential for a particular service to exist many times in the poller configuration file, this bit of bookkeeping was put, once, at the end of the file.
&lt;monitor service="DominoIIOP"   class-name="org.opennms.netmgt.poller.DominoIIOPMonitor"/>
&lt;monitor service="ICMP"         class-name="org.opennms.netmgt.poller.IcmpMonitor"/>
&lt;monitor service="Citrix"       class-name="org.opennms.netmgt.poller.CitrixMonitor"/>
&lt;monitor service="LDAP"         class-name="org.opennms.netmgt.poller.LdapMonitor"/>
&lt;monitor service="HTTP"         class-name="org.opennms.netmgt.poller.HttpMonitor"/>
&lt;monitor service="HTTP-8080"    class-name="org.opennms.netmgt.poller.HttpMonitor"/>
&lt;monitor service="HTTP-8000"    class-name="org.opennms.netmgt.poller.HttpMonitor"/>
&lt;monitor service="HTTPS"        class-name="org.opennms.netmgt.poller.HttpsMonitor"/>
&lt;monitor service="SMTP"         class-name="org.opennms.netmgt.poller.SmtpMonitor"/>
&lt;monitor service="DHCP"         class-name="org.opennms.netmgt.poller.DhcpMonitor"/>
&lt;monitor service="DNS"          class-name="org.opennms.netmgt.poller.DnsMonitor" />
&lt;monitor service="FTP"          class-name="org.opennms.netmgt.poller.FtpMonitor"/>
&lt;monitor service="SNMP"         class-name="org.opennms.netmgt.poller.SnmpMonitor"/>
&lt;monitor service="Oracle"       class-name="org.opennms.netmgt.poller.TcpMonitor"/>
&lt;monitor service="Postgres"     class-name="org.opennms.netmgt.poller.TcpMonitor"/>
&lt;monitor service="MySQL"        class-name="org.opennms.netmgt.poller.TcpMonitor"/>
&lt;monitor service="Sybase"       class-name="org.opennms.netmgt.poller.TcpMonitor"/>
&lt;monitor service="Informix"     class-name="org.opennms.netmgt.poller.TcpMonitor"/>
&lt;monitor service="SQLServer"    class-name="org.opennms.netmgt.poller.TcpMonitor"/>
&lt;monitor service="SSH"          class-name="org.opennms.netmgt.poller.TcpMonitor"/>
&lt;monitor service="IMAP"         class-name="org.opennms.netmgt.poller.ImapMonitor"/>
&lt;monitor service="POP3"         class-name="org.opennms.netmgt.poller.Pop3Monitor"/>
You should not need to modify this section unless you manually add your own pollers.
[edit] Documentation for Specific Pollers
Some documentation has been written for:
	•	HTTP.
	•	Radius.
[edit] Conclusion
It is hoped that this How-To has proved useful. Please direct corrections and comments to the author.
[edit] Examples
Monitoring a Dell PowerEdge Expandable RAID Controller 3/Di
</Text>
        </Document>
        <Document ID="88">
            <Title>Bug reporting, Information Sources</Title>
            <Synopsis>IRC etc.</Synopsis>
        </Document>
        <Document ID="92">
            <Title>Service Pollers</Title>
            <Text>C
	•	Citrix Service Poller
D
	•	DHCP Service Poller
	•	DNS Service Poller
	•	DominoHTTP Service Poller
F
	•	FTP Service Poller
G
	•	GeneralPurposePoller
H
	•	HTTP Service Poller
	•	HTTP-8000 Service Poller
	•	HTTP-8080 Service Poller
	•	HTTP-MGMT Service Poller
	•	HTTPS Service Poller
I
	•	ICMP Service Poller
I cont.
	•	IMAP Service Poller
	•	Informix Service Poller
L
	•	LDAP Service Poller
M
	•	MSExchange Service Poller
	•	MySQL Service Poller
N
	•	NTP Service Poller
	•	NotesHTTP Service Poller
O
	•	Oracle Service Poller
P
	•	POP3 Service Poller
	•	Passive Status Poller
	•	Poller package
	•	Postgres Service Poller
R
	•	RadiusAuth Service Poller
	•	RadiusPoller
S
	•	SMB Service Poller
	•	SMTP Service Poller
	•	SNMP Interface Poller
	•	SNMPv2 Service Poller
	•	SQLServer Service Poller
	•	SSH Service Poller
	•	Service pollers
	•	Sybase Service Poller
T
	•	TCP Port Service Poller
	•	Telnet Service Poller
</Text>
        </Document>
        <Document ID="10">
            <Title>License Information</Title>
            <Text>OpenNMS

$OpenNMS_License

This Book

OpenNMS is free open source software. Software is knowledge and experience expressed in a programming language. The knowledge and experience which is inside OpenNMS is free for you to use. We thought it consequent to make this book as “free” as a book can be, too. That means that you are permitted to share the content of this book. You can download it as a PDF file from our software repository. You can share that, too. In fact - the more you share it, the better for OpenNMS - the more experienced users we have, the better our project will become. 

If you want to express your appreciation for the work which went inside this book, buy a paperback or donate. You can find information about how to donate on the website for our book: http://www.opennms.org/book


You are free:

to Share — to copy, distribute and transmit the work
to Remix — to adapt the work
	
Under the following conditions:

Attribution — You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work).  
What does "Attribute this work" mean?  The page you came from contained embedded licensing metadata, including how the creator wishes to be attributed for re-use. You can use the HTML here to cite the work. Doing so will also include metadata on your page so that others can find the original work as well.  
Noncommercial — You may not use this work for commercial purposes. 

Share Alike — If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. 

With the understanding that:

Waiver — Any of the above conditions can be waived if you get permission from the copyright holder.

Public Domain — Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.

Other Rights — In no way are any of the following rights affected by the license:

	◦	Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations;
	◦	The author's moral rights;
	◦	Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.

Notice — For any reuse or distribution, you must make clear to others the license terms of this work. The best way to do this is with a link to this web page: http://www.opennms.org/book
</Text>
        </Document>
        <Document ID="93">
            <Title>Poller Package</Title>
            <Text>A "Poller Package" is OpenNMS' Idea of grouping IP addresses which are polled for service assurance. E.g. if you have 10 hosts whose interfaces you want to be polled for SSH, you can create a poller package to explain that to OpenNMS.
Let's call the package "tenforssh":
	•	poller-configuration.xml
&lt;package name="tenforssh">
	•	Any address on any node may be a candidate for polling, so make the filter wide open
  &lt;filter>IPADDR != '0.0.0.0'&lt;/filter>
	•	Load the interface addresses we want:
  &lt;include-url>file:/etc/opennms/include/tenforssh.cfg&lt;/include-url>
tenforssh.cfg holds a list of IP Addresses (one per line) with the 10 hosts.
	•	The info for RRD (..)
  &lt;rrd step="300">
      &lt;rra>RRA:AVERAGE:0.5:1:2016&lt;/rra>
      &lt;rra>RRA:AVERAGE:0.5:12:1488&lt;/rra>
      &lt;rra>RRA:AVERAGE:0.5:288:366&lt;/rra>
      &lt;rra>RRA:MAX:0.5:288:366&lt;/rra>
      &lt;rra>RRA:MIN:0.5:288:366&lt;/rra>
   &lt;/rrd>
	•	And now the SSH definition
  &lt;service name="SSH" interval="300000" user-defined="false" status="on">
    &lt;parameter key="retry" value="1"/>
    &lt;parameter key="timeout" value="3000"/>
    &lt;parameter key="rrd-repository" value="/var/lib/opennms/rrd/response"/>
    &lt;parameter key="rrd-base-name" value="ssh"/>
    &lt;parameter key="ds-name" value="ssh"/>
  &lt;/service>
&lt;/package>
</Text>
        </Document>
        <Document ID="11">
            <Title>Reading SLA Category Data</Title>
        </Document>
        <Document ID="94">
            <Title>Passive Status Keeper</Title>
            <Text>Passive Nodes
Passive nodes in OpenNMS, are nodes that OpenNMS is unable to directly communicate with because either they are 'not real' or because the only communication avaliable to them is done by someone else. They are termed 'Passive' because we cannot actively poll them but must instead passively rely on information sent to us form somewhere else. This document's intention is to explain Passive Nodes and how to configure and use this exciting new feature of OpenNMS.
[edit] Passive Status Keeper Revised
In OpenNMS version 1.3.2, the Passive Status Keeper was revised to support greater functionality. The impetus for this change stems from an additional requirement of the passive status keeper custom development project. This final requirement specified the ability to re-assign the node associated with a non "passive status" event to a passive node.
In the OpenNMS version 1.3.1, events that set the status of a passive node were identified in the passive-status-configuration.xml file. These events became "passive status" events only in application and were never associated with the passive node, they were associated with the node that sent the event (or in most cases the SNMP Trap). I say only in application because no actual passive status event was ever actually generated, these events ended up performing the same functionality as a passive status event because they're attributes were translated into the parameters required to set the status of the passive node:
	•	passiveNodeLabel
	•	passiveIpAddr
	•	passiveServiceName
	•	passiveStatus
Optionally, one can set passiveReasonCode as well, otherwise e.g. SNMP traps will be displayed as "Reason: Unknown".
In version 1.3.1, the passive-status-configuration.xml file had sophisticated parsing and formatting functionality that was able to derive the above parameters to control the status of passive services. In version 1.3.2, this functionality was enhanced and moved to a new OpenNMS service called the EventTranslator. The enhancement also changed the configuration, as well. The following documentation has been updated to support the new functionality. Since this documentation started in the OpenNMS WIKI, you can view the history for OpenNMS version 1.3.1 support.
[edit] Overview
OpenNMS has been enhanced with a new feature. This features provides OpenNMS with the ability to monitor the availability of what we call passive nodes. The term passive node was derived during the creation of use cases for a requested feature that initially used the term virtual node. Passive nodes, with respect to this feature, are nodes for which OpenNMS maintains the status of its interfaces and services, yet typically, is not an actual IP based service, hence the the original term "virtual" node. (Note: in practice, the status of an IP based service could be tracked with the passive monitor, we'll get to that later). Basically, the status of passive nodes are determined by a non-IP based protocol which are interpreted by an intermediary (proprietary management console) then reported (sent "Northbound") to OpenNMS via an SNMP trap or an OpenNMS XML event. This trap or event is how that status is tracked by the Passive Status Keeper inside OpenNMS.
The best way to get an understanding of this new feature is with a practical example, and there is no better example than one from the our original request.
[edit] Requirement
	•	Monitor availability of satellite communication devices
	•	Satellite communication devices do not have IP interfaces; however, they do have an IP based management station that monitors their status and can report their status via an SNMP trap. Many satellites use contact closures to indicate alarm state.
	•	Enable OpenNMS to track the status of these satellite communication devices using SNMP traps.
[edit] Challenges and constraints
Design this feature using the current Poller architecture so that all the current facilities (availability reports, outages and outage calendars, notifications, alarms, etc.) behave as if these service were being monitored directly by implementing an OpenNMS Poller monitor Interface class.
[edit] The Solution
[edit] PassiveStatusKeeper.java, PassiveServiceMonitor.java, LoopPlugin.java and EventTranslator.java
Four new classes were created for monitoring the status of Passive nodes (a passive node is any node that has a passive service). These classes are the Passive Status Keeper, PSK, and the Passive Service Monitor, PSM, the Loopback Plugin (LP), and the Event Translator (ET).
[edit] Passive Status Keeper
It is the job of the PSK to simply maintain the status of passive (virtual) services in a hash table. The PSM is called by the Poller, just as the other IP based monitors, yet, its behavior is to report the status currently represented in the PSK's hash table. No polling on the network is performed. This hash table will either contain the latest status reported by a passive status event (PSE). If no status messages have been received, the PSK defaults to status "Up" a.k.a. "Available". (The UEI for the PSE is: "uei.opennms.org/services/passiveServiceStatus") During OpenNMS initialization, the current Outages queried to set the initial state of any passive service with an outage condition when OpenNMS was shutdown.
The hash table key is derived using the combination of the nodelabel, ipaddr, and serviceName. When a PSE is received by the PSK, it derives this key from 3 parameters:
	•	passiveNodeLabel
	•	passiveIpAddr
	•	passiveServiceName
The "passiveNodeLabel" needs to be exactly the same as node name in OpenNMS and is case sensitive. If a device has multiple PSK services they can be grouped together under the same IP address.
[edit] Configuration
As of 1.3.2, there is no longer any configuration required for the PSK. The PSK simply listens for PSEs and tracks that status of the passive service reported by a PSE. The configuration of PSK in version 1.3.1, mainly involved translating any event into a PSE and registering those events with the PSK. This translation has moved to the new and even more sophisticated OpenNMS daemon called, oddly enough, EventTranslator (ET) and the PSK only listens for PSEs. Use ET to create PSEs for PSK (grin).
ET provides the ability to translate any event into a completely new event. (We chose creating a new event rather than changing the original event to preserve the integrity of the original event) This functionality can be used to create PSEs deriving the four PSE required parameters using regular expressions, SQL queries, and formatting statements.
Okay, before we get into the dirty details of the syntax of ET's configuration to create PSEs, here is a sample used to set that status of a passive service based on a trap from a Pixel Metrics device.
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;event-translator-configuration
xmlns="http://xmlns.opennms.org/xsd/translator-configuration"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" >
  &lt;translation>
    &lt;!-- Create a new event and translate a few fields/ parameters
         for the PixelMetrics Trap tspEventPCRRepetitionError
         this translation creates a passiveServiceStatus event and sets
         the service status to Down -->
    &lt;event-translation-spec uei="uei.opennms.org/mib2opennms/tspEventPCRRepetitionError">
      &lt;mappings>
        &lt;mapping>
          &lt;!-- Create a parameter -->
          &lt;assignment type="parameter" name="passiveNodeLabel">
            &lt;value type="parameter" name=".1.3.6.1.4.1.6768.6.2.2.5.0" matches="^([A-z]+) ([0-9]+).*" result="${1}-${2}" />
          &lt;/assignment>
          &lt;!-- Create a parameter -->
          &lt;assignment type="parameter" name="passiveIpAddr">
            &lt;value type="constant" result="169.254.1.1" />
          &lt;/assignment>
          &lt;!-- Create a parameter -->
          &lt;assignment type="parameter" name="passiveServiceName">
            &lt;value type="parameter" name=".1.3.6.1.4.1.6768.6.2.2.7.0" matches="^([A-z]+): .*" result="${1}" />
          &lt;/assignment>
          &lt;!-- Create a parameter -->
          &lt;assignment type="parameter" name="passiveStatus" >
            &lt;value type="parameter" name=".1.3.6.1.4.1.6768.6.2.2.7.0" matches=".*exceeded.*" result="Down" />
          &lt;/assignment>
          &lt;!-- Change the UEI to be a passive status event-->
          &lt;assignment type="field" name="uei">
            &lt;value type="constant" result="uei.opennms.org/services/passiveServiceStatus" />
          &lt;/assignment>
        &lt;/mapping>
      &lt;/mappings>
    &lt;/event-translation-spec>
  &lt;/translation>
&lt;/event-translator-configuration>
[edit] The Tricky Part
The derivation of the 4 required parameters of a PSE is the single most important component to using the passive node/service feature. From the fields and parameters in any event, one must be able to piece together these 4 parameters for the translated PSE. As discussed earlier, the first 3 parameters (passiveNodeLabel, passiveIpAddr, passiveServiceName) combine to make a hash table key for the status hash table maintained by PSK. Using the above example, ET will:
1) clone the original tspEventPCRRepetitionError event 2) Set the 4 PSE parameters with values derived from fields and parameters of the event 3) publish the new PSE event
Note: Since the original event is cloned, all the fields and parameters
will remain intact other than those changed by the assignments
within each mapping.
The trickiest part of all is the handling of the result attribute to the value element. The result attribute can function as a literal or as a formatted string with access to the back-references of the regular expression specified in the matches attribute. Whew, that is a mouthful.
There are 2 ways to literally set the value. The first way is to use the matches attribute. If the regular expression specified in the matches attribute returns true the result attribute can be a literal string, such as with the "Down" result in the above example's setting of passiveStatus:
&lt;assignment type="parameter" name="passiveStatus" >
  &lt;value type="parameter" name=".1.3.6.1.4.1.6768.6.2.2.7.0" matches=".*exceeded.*" result="Down" />
&lt;/assignment>
The other way to set a literal is to set the type attribute equal to "constant" as in the above example's change of the UEI:
&lt;!-- Change the UEI to be a passive status event-->
&lt;assignment type="field" name="uei">
  &lt;value type="constant" result="uei.opennms.org/services/passiveServiceStatus" />
&lt;/assignment>
. The result attribute will then be set directly without any expression matching.
For more in-depth examples of using ET, see EventTranslator.
[edit] Sample Implementation
Okay, now that we know how status for a passive node's services are maintained, now we need a way to get the passive nodes, interfaces, and their services defined into OpenNMS. As of release 1.3.1, there are two methods to accomplish this:
	1.	send-event.pl method
	2.	Loop plug-in method.
	3.	Provistioning Group method.
[edit] The send-event.pl Method
OpenNMS has two not very well-known events (not very published either) called the "Update Server" and "Update Service" events. These events have the UEIs: "uei.opennms.org/internal/capsd/updateServer" and "uei.opennms.org/internal/capsd/updateService", respectively. Use the events to add or delete nodes and services; for our purposes here, passive nodes and services.
Here is an example to add a passive node to OpenNMS:
send-event.pl uei.opennms.org/internal/capsd/updateServer localhost \
        --interface 169.254.1.1 \
        --parm 'nodelabel Channel-9' \
        --parm 'action ADD'
With this event, a node will be created with the node label "Channel-9" and an IP interface of "169.254.1.1". Note that the node label must be unique in OpenNMS. Also note the different UEI.
To add a passive service:
send-event.pl uei.opennms.org/internal/capsd/updateService localhost \
	--interface 169.254.1.1 \
	--service PCR \
	--parm 'nodelabel Channel-9' \
	--parm 'action ADD'
With this event, the service PCR is added to the interface 169.254.1.1. This service must already be defined in the Capsd configuration. Note: if this service is not defined to use the Loop Plugin or some custom plugin that can determine the existence of this service, set the scan attribute to "off". (More on the Loop Plugin later).
[edit] Loop Plugin Method
The Loop Plugin (LoopPlugin.java) was created to give OpenNMS users the ability to force a service on to an interface. Using this method, a passive node, interface, and service can be added with one event, the New Suspect event (uei.opennms.org/internal/newSuspect). This is the process to add a passive service using this method:
First, add the new service to capsd-configuration.xml:
&lt;protocol-plugin protocol="PCR"
               class-name="org.opennms.netmgt.capsd.plugins.LoopPlugin"
               scan="on" user-defined="false" >
    &lt;property key="ip-match" value="169.254.*.*" />
    &lt;property key="is-supported" value="true" />
&lt;/protocol-plugin>
This configuration will automatically add the PCR service to any interface beginning with "169.254".
[edit] The Provisioning Group Method
It is now recommended that you add Nodes, Interfaces, and Services, that are not OpenNMS discovered, with the Admin/Provisioing Groups WebUI enhancement added in version 1.3.2. This feature gives you the ability to define nodes, interfaces, and services and manage them (add, change, delete) visually and more completely. The only down side is that you need to add the servcie to the DB manually. Say your new service name is 'davidService':
 psql -U opennms -c "insert into service values (nextval('serviceNxtId'), 'davidService');"
This will make the service name show up in the provisioning WebUI drop down list. (Admin | Manage Provisioning Groups). When adding nodes you can add multiple entries. For example a Telephone switch that has minor/major/critical contact closures. After you add them they will not automatically be added to the node list. The "Nodes in Group/Nodes in DB" field will show how many still need to be added. Don't forget to click on "Import" to add them to the database.
 Now, using either method, you are ready to start passively monitoring this service. This calls for our new Passive Service Monitor class (PassiveServiceMonitor.java) in the Poller configuration. You may use the same default polling package for passive monitors, however, I recommend you create a new polling package that polls much more frequently. This allows are more real-time experience for your outages. I recommend a 30 second polling interval (there isn't any network activity here and accessing a hash table should be in the sub-milliseconds). Enough explanation, here's a sample config:
poller-configuration.xml
&lt;package name="passive-services">
    &lt;filter>IPADDR IPLIKE *.*.*.*&lt;/filter>
    &lt;include-range begin="1.1.1.1" end="254.254.254.254"/>
    &lt;rrd step = "300">
        &lt;rra>RRA:AVERAGE:0.5:1:2016&lt;/rra>
        &lt;rra>RRA:AVERAGE:0.5:12:4464&lt;/rra>
        &lt;rra>RRA:MIN:0.5:12:4464&lt;/rra>
        &lt;rra>RRA:MAX:0.5:12:4464&lt;/rra>
    &lt;/rrd>
    &lt;service name="PCR" interval="30000" user-defined="false" status="on" />
    &lt;downtime interval="15000" begin="0" end="300000"/>             &lt;!-- 15s, 0, 5m -->
    &lt;downtime interval="30000" begin="300000" end="43200000"/>      &lt;!-- 30s, 5m, 12h -->
    &lt;downtime interval="300000" begin="43200000" end="432000000"/>  &lt;!-- 5m, 12h, 5d -->
    &lt;downtime begin="432000000" delete="true"/>                     &lt;!-- anything after 5 days delete -->
&lt;/package>
&lt;monitor service="PCR" class-name="org.opennms.netmgt.poller.monitors.PassiveServiceMonitor" />
This polling package tells the poller to schedule polling for the PCR service every 30 seconds. When the monitor is called, the monitor will compute the key ("nodelabel:ipaddr:serviceName") and request the status from the PSK.
That's not so bad, huh?
[edit] Making it Happen
As stated earlier, the PSK subscribes for passive events and sets that status that the above monitor will poll. Events can be generated in a number of ways, however, for the purposes of this feature, we'll look at 2: send-event.pl and SNMP traps.
[edit] The send-event.pl Method
Even if you are going to be using SNMP Traps to set the status of a passive nodes services, you can quickly test your setup for passive status monitoring using the send-event.pl script (or your own version) as shown in the sample here:
send-event.pl uei.opennms.org/services/passiveServiceStatus localhost \
	--interface 169.254.1.1 \
	--service PCR \
	--parm 'passiveNodeLabel Channel-9' \
	--parm 'passiveIpAddr 169.254.1.1' \
	--parm 'passiveServiceName PCR' \
	--parm 'passiveReasonCode just went down the sink' \
	--parm 'passiveStatus Down'
Using the send-event.pl script, you can always just use the default passiveServiceStatus event already configured in OpenNMS version 1.3.1. This event uses parameters (not fields) and conveniently uses the sample configuration defined in passive-status-configuration.xml shipped in OpenNMS version 1.3.1 and a UEI already defined in eventconf.xml. This event is a handy way to quickly test your configuration.
The PSK will add/update an entry in its hash table. A visual representation looks like this (which sql table is this from? Probably it's no SQL table at all but a list kept in memory, so it's not accessible via the database):
Key 				  | Value
----------------------------------|-------
"Channel-9:169.254.1.1:PCR"       | Down
 Now that the status is set in the PSK, how does OpenNMS monitor that status? This is where the PSM comes in. This Monitor is scheduled for polling just as any of the IP based Service Monitors; however, instead of using network protocols, it requests the status from the PSK. The monitor returns the status of this passive service to the Poller and the behavior is identical and the Outage is recorded, notifications are triggered, the WebUI reflects status and availability, and availability reports can be created. Magic!
To restore the outage, the process is the same using the following event:
send-event.pl uei.opennms.org/services/passiveServiceStatus localhost \
	--interface 169.254.1.1 \
	--service PCR \
	--parm 'passiveNodeLabel Channel-9' \
	--parm 'passiveIpAddr 169.254.1.1' \
	--parm 'passiveServiceName PCR' \
	--parm 'passiveStatus Up'
Now the PSK hash table will reflect:
Key 				  | Value
----------------------------------|-------
"Channel-9:169.254.1.1:PCR"       | Up
The PSM will poll and see the outage is restored.
[edit] Using SNMP Traps
For convenience, let's continue with this example and use one of the traps that came with the request for this new feature. This trap is defined in the PIXMET-DVSTATION-MIB and was converted into the OpenNMS event configuration format (eventconf.xml):
&lt;event>
  &lt;mask>
    &lt;maskelement>
      &lt;mename>id&lt;/mename> 
      &lt;mevalue>.1.3.6.1.4.1.6768.3.4.5&lt;/mevalue>
    &lt;/maskelement>
    &lt;maskelement>
      &lt;mename>generic&lt;/mename>
      &lt;mevalue>6&lt;/mevalue>
    &lt;/maskelement>
    &lt;maskelement>
      &lt;mename>specific&lt;/mename>
      &lt;mevalue>1240&lt;/mevalue>
    &lt;/maskelement>
  &lt;/mask>
  &lt;uei>uei.opennms.org/mib2opennms/tspEventPCRRepetitionError&lt;/uei>
  &lt;event-label>PIXMET-TSP-MIB defined trap event: tspEventPCRRepetitionError&lt;/event-label>
  &lt;descr>
&lt;p>The time interval between two consecutive Program Clock Reference
(PCR) values exceeded the maximum allowed.&lt;/p>&lt;table>
        &lt;tr>&lt;td>&lt;b>

        pmEventClockTimestamp&lt;/b>&lt;/td>&lt;td>
        %parm[#1]%;&lt;/td>&lt;td>&lt;p;>&lt;/p>&lt;/td;>&lt;/tr>
        &lt;tr>&lt;td>&lt;b>

        pmEventDeviceTimestamp&lt;/b>&lt;/td>&lt;td>
        %parm[#2]%;&lt;/td>&lt;td>&lt;p;>&lt;/p>&lt;/td;>&lt;/tr>
        &lt;tr>&lt;td>&lt;b>

        pmEventSeverity&lt;/b>&lt;/td>&lt;td>
        %parm[#3]%;&lt;/td>&lt;td>&lt;p;>
                cleared(1)
                indeterminate(2)
                critical(3)
                major(4)
                minor(5)
                warning(6)
                info(7)
                none(8)
        &lt;/p>&lt;/td;>&lt;/tr>
        &lt;tr>&lt;td>&lt;b>

        pmEventSource&lt;/b>&lt;/td>&lt;td>
        %parm[#4]%;&lt;/td>&lt;td>&lt;p;>&lt;/p>&lt;/td;>&lt;/tr>
        &lt;tr>&lt;td>&lt;b>

        pmEventCurrentProfileName&lt;/b>&lt;/td>&lt;td>
        %parm[#5]%;&lt;/td>&lt;td>&lt;p;>&lt;/p>&lt;/td;>&lt;/tr>
        &lt;tr>&lt;td>&lt;b>

        pmEventSourceName&lt;/b>&lt;/td>&lt;td>
        %parm[#6]%;&lt;/td>&lt;td>&lt;p;>&lt;/p>&lt;/td;>&lt;/tr>
        &lt;tr>&lt;td>&lt;b>

        pmEventDescription&lt;/b>&lt;/td>&lt;td>
        %parm[#7]%;&lt;/td>&lt;td>&lt;p;>&lt;/p>&lt;/td;>&lt;/tr>
        &lt;tr>&lt;td>&lt;b>

        pidNumber&lt;/b>&lt;/td>&lt;td>
        %parm[#8]%;&lt;/td>&lt;td>&lt;p;>&lt;/p>&lt;/td;>&lt;/tr>&lt;/table>
  &lt;/descr>
  &lt;logmsg dest='logndisplay'>&lt;p>
                        tspEventPCRRepetitionError trap received
                        pmEventClockTimestamp=%parm[#1]%
                        pmEventDeviceTimestamp=%parm[#2]%
                        pmEventSeverity=%parm[#3]%
                        pmEventSource=%parm[#4]%
                        pmEventCurrentProfileName=%parm[#5]%
                        pmEventSourceName=%parm[#6]%
                        pmEventDescription=%parm[#7]%
                        pidNumber=%parm[#8]%&lt;/p>
  &lt;/logmsg>
  &lt;severity>Indeterminate&lt;/severity>
&lt;/event>
The following step isn't necessary in version 1.6 (maybe earlier?). All configuration is in translator-configuration.xml.
Now that the trap has been registered as an event, register this event in the passive-status-configuration.xml file as we discussed above:
&lt;passive-event uei="uei.opennms.org/mib2opennms/tspEventPCRRepetitionError">
    &lt;status-key>
        &lt;node-label>
            &lt;event-token is-parm="true" name=".1.3.6.1.4.1.6768.6.2.2.5.0" value="~^([A-z]+) ([0-9]+).*" pattern="$1-$2"/>
        &lt;/node-label>
        &lt;ipaddr>
            &lt;event-token is-parm="true" name=".1.3.6.1.4.1.6768.6.2.2.5.0" value="169.254.1.1"/>
        &lt;/ipaddr>
        &lt;service-name>
            &lt;event-token is-parm="true" name=".1.3.6.1.4.1.6768.6.2.2.7.0" value="~^([A-z]+): .*" pattern="$1"/>
        &lt;/service-name>
        &lt;status>
            &lt;event-token is-parm="true" name=".1.3.6.1.4.1.6768.6.2.2.5.0" value="Down"/>
        &lt;/status>
    &lt;/status-key>
&lt;/passive-event>
Notice the names of the parameters. When a trap is received by OpenNMS the varbinds are converted to event parameters and the name of the parameter is the SNMP Object Identifier. Using your new found knowledge of this feature you gained by mastering the "TRICKY PART" above, you can see how this will set the status of the PCR service of the Channel-9 node for interface 169.254.1.1. Can't you? Oh, here, it might help if you have a look at some data from an actual trap:
Trap signature:
	.1.3.6.1.4.1.6768.3.4.5,undefined,v2,1240,6,public
Varbinds:
	.1.3.6.1.2.1.1.3.0=963472433(TimeTicks,text);
	.1.3.6.1.6.3.1.1.4.1.0=.1.3.6.1.4.1.6768.3.4.5.0.1240(ObjectIdentifier,text);
	.1.3.6.1.4.1.6768.6.2.2.1.0=4026458540(TimeTicks,text);
	.1.3.6.1.4.1.6768.6.2.2.2.0=07 D5 0B 15 09 05 36 09 2B 0B 00(OctetString,text);
	.1.3.6.1.4.1.6768.6.2.2.3.0=4(Int32,text);
	.1.3.6.1.4.1.6768.6.2.2.4.0=20(Gauge32,text);
	.1.3.6.1.4.1.6768.6.2.2.5.0=Channel 9(OctetString,text);
	.1.3.6.1.4.1.6768.6.2.2.6.0=Channel 9(OctetString,text);
	.1.3.6.1.4.1.6768.6.2.2.7.0=PCR: PID 0x0902 (2306) (PCR only): 61.171 ms interval between PCR values exceeded max threshold of 60 ms.(OctetString,text);
	.1.3.6.1.4.1.6768.3.4.2.2.1.1.0=2306(Gauge32,text)
This passive-status-configuration will derive the key "Channel-9:169.254.1.1:PCR" and a status of "Down" from the contents of this trap. Note that the &lt;ipaddr> and the &lt;status> elements are literals and the name of the parameter could have been any valid event field or paramenter.
And last but not least: Here is a sample trap command you can use to test this config: (Note: that I only send in the varbinds need to derive the key and status tokens)
snmptrap -v 2c -c public localhost '' .1.3.6.1.4.1.6768.3.4.5.1240 \
        .1.3.6.1.6.3.1.1.4.1.0 o .1.3.6.1.4.1.6768.3.4.5.0.1240 \
        .1.3.6.1.4.1.6768.6.2.2.1.0 t 4026458540 \
        .1.3.6.1.4.1.6768.6.2.2.5.0 s "Channel 9" \
        .1.3.6.1.4.1.6768.6.2.2.7.0 s "PCR: PID 0x0902 ... exceeded max threshold of 60ms."
[edit] Summary
To attack this feature for you own passive events, take the following steps and use the instructions in this document to complete each task. Looking at the following steps compared.
	1.	Define additional passive status events (if using traps) in an event configuration file.
	2.	Register these events with the PSK in that passive-status-configuration.xml file.
	3.	Create the Passive Nodes/Interfaces/Services (use one of the 2 methods discussed above... the XML-RPC API can be used, too, but that is another set of documentation)
	4.	Prosper.
[edit] Version History/Availability
	•	This feature was added in version 1.3.1
	•	This feature was enhanced or modified in version 1.3.2
</Text>
        </Document>
        <Document ID="12">
            <Title>Reading Graphs</Title>
        </Document>
        <Document ID="95">
            <Title>HTTP Monitor</Title>
            <Text>HTTP Monitor Parameters
ds-name
user-agent
basic-authentication
host-name
response-text 
Text to look for in the response body. This will be matched against every line, and it will be considered a success at the first match. If there is a "~" at the beginning of the parameter, the rest of the string will be used as a regular expression pattern match, otherwise the match will be a substring match. Note: the regular expression match is anchored at the beginning and end of the line, so you will likely need to put a ".*" on both sides of your pattern unless you are going to be matching on the entire line.
response
rrd-repository
port
timeout
retry
url
verbose
[edit] Example config
&lt;!-- Test for virtual host opennms.com running -->
&lt;service name="HTTP" interval="300000" user-defined="false" status="on">
  &lt;parameter key="retry" value="1"/>
  &lt;parameter key="timeout" value="3000"/>
  &lt;parameter key="port" value="80"/>
  &lt;parameter key="host-name" value="opennms.com"/>
  &lt;parameter key="url" value="/solutions"/>
  &lt;parameter key="response" value="200-202,299"/>
  &lt;parameter key="response-text" value="~.*[Cc]onsulting.*"/>
&lt;/service>

&lt;!-- Test for instance of OpenNMS 1.2.9 running -->
&lt;service name="OpenNMS" interval="300000" user-defined="false" status="on">
  &lt;parameter key="retry" value="1"/>
  &lt;parameter key="timeout" value="3000"/>
  &lt;parameter key="port" value="8080"/>
  &lt;parameter key="url" value="/opennms/event/list"/>
  &lt;parameter key="basic-authentication" value="admin:admin"/>
  &lt;parameter key="response" value="200"/>
&lt;/service>
Note that the default acceptable responses are the following: When monitoring the "/" URL path: 100-499 When monitoring any other URL path: 100-399
[edit] Non-root URLs
In order to configure a poller that tests a non-root URL, you need to configure capsd to discover the service and pollerd to poll the discovered service (two different things). Below is an example using ColdFusion.
[edit] capsd-configuration.xml
It's perfectly acceptable to have multiple instances of the same protocol-plugin class-name, but they will need different protocol attributes. For example, you may have several protocol-plugins defined for HTTP, all using the same HttpPlugin class. You will, however, need to make sure that they have distinct protocol attributes. Be advised that the HttpPlugin for capsd only uses HTTP/1.0. So tests that require HTTP/1.1(i.e. virtual hosts) will not be discovered.
&lt;protocol-plugin protocol="ColdFusion"
                 class-name="org.opennms.netmgt.capsd.plugins.HttpPlugin"
                 scan="on" user-defined="false">
  &lt;property key="port" value="80"/>
  &lt;property key="timeout" value="3000"/>
  &lt;property key="retry" value="2"/>
  &lt;property key="url" value="/cfide/administrator/index.cfm"/>
&lt;/protocol-plugin>
[edit] poller-configuration.xml
now that you've discovered the service, you'll need to poll it. Note that, just as for capsd discovery, it's prefectly accpetable to have multiple instances of the same poller class, but they will need different names. For example, you may have several pollers testing HTTP services, all using the same HttpMonitor class. You will, however need to make sure that they have distinct service names.
&lt;service name="ColdFusion" interval="300000"
         user-defined="false" status="on">
  &lt;parameter key="retry" value="3"/>
  &lt;parameter key="timeout" value="10000"/>
  &lt;parameter key="port" value="80"/>
  &lt;parameter key="url" value="/cfide/administrator/index.cfm"/>
&lt;/service>
Important Notes:
	•	The name attribute of the service in poller-configuration.xml needs to match the protocol attribute of the protocol-plugin in capsd-configuration.xml.
	•	The ds-name attribute (not shown in this example) also needs to be unique for each service, or you'll find response time from one service overwriting response time from another.
	•	You'll also need a line to map the new service to a monitor class (see at the end of the file):
&lt;monitor service="ColdFusion"         class-name="org.opennms.netmgt.poller.monitors.HttpMonitor"/>
Note: This only works in 1.2.4 or later.
[edit] Stupid HttpMonitor Tricks
Testing_Filtering_Proxies_With_HTTPMonitor
[edit] Do not just test for "OK"
Some (including me) may think of writing a special webapp that returns "OK" on a website if the program succeeds and use HttpMonitor's response-test configuration to check for "OK" on that website. Since we have Bug 2702, testing for "OK" will never work. So better make "OK" something like "Seems like it's working".
[edit] Links
	•	http://technocrat.watson-wilson.ca/blosxom/computer/onmsreview.html
</Text>
        </Document>
        <Document ID="13">
            <Title>Acknowledging Notifications</Title>
        </Document>
        <Document ID="96">
            <Title>Page Sequence Monitor</Title>
            <Text>PSM
PSM stands for "Page Sequence Monitor". The PSM allows to monitor beyond the complexity of only requesting a single url.
A typical use case for this is:
	•	You want to login to a certain application
	•	Execute an action while being logged in
	•	Log off again
If this is all working ok, your application works. If there's an error somewhere, your application will need attention.
Before the PSM, it was necessary to implement this using a Plugin. By the means of the GPPlugin you had to call a script to execute the queries and give feedback to OpenNMS about the result. Webinject is to be mentioned here.
With the PSM you can now run "simple" sequences from within OpenNMS without calling an external script. This has some advantages (better resource allocation, higher reliability, configuration for the full process in one place).
"Simple" sequences because they must be programmable in a sequence. A forward (30x Response) for example is right now not supported.

[edit] Capability Scan
You can use the regular HttpPlugin or HttpsPlugin to detect a webapp.
    &lt;protocol-plugin protocol="OpenNMSLogin"
        class-name="org.opennms.netmgt.capsd.plugins.HttpPlugin"
        scan="on" user-defined="false">
        &lt;!-- this needs to be configured to detect the opennms webapp on port 8980.
             I don't remember if capsd will let you do a match for opennms or something
             it would probably be sufficent to just check /opennms returning a valid return code at port 8080
        -->
        &lt;property key="timeout" value="5000"/>
        &lt;property key="retry" value="1"/>
        &lt;property key="port" value="8980"/>
    &lt;/protocol-plugin>

[edit] Poller
To configure a service using the PSM, the poller must be configured to know what page sequence it should use. With this monitor, the page sequence is defined as an XML element within a parameter! The parameter key attribute is required to be "page-sequence" and the value attribute is not specified. The value is actually an XML page-sequence element itself and is embedded within the definition of this special parameter.
(Note: this was done so that the page-sequence monitor could be used in the distributed monitor for the distributed monitor doesn't have access to the configuration files on the filesystem of the server. With the page-sequence configuration in-lined, it can be serialized and passed to the remote poller through the RMI interface)"
        &lt;service name="OpenNMSLogin" interval="300000" user-defined="true" status="off">
            &lt;parameter key="retry" value="1"/>
            &lt;parameter key="timeout" value="5000"/>
            &lt;parameter key="rrd-repository" value="/opt/opennms/share/rrd/response"/>
            &lt;parameter key="ds-name" value="opennmslogin"/>
            &lt;parameter key="page-sequence">
              &lt;!-- this uses a newly implemented feature of poller-configuration.xml
                   (it has not yet been added to other daemons like capsd, collected, etc)
                   this feature allows you to insert xml content inside of the parameter tags
                   rather than as a string in the value attribute -->
              &lt;page-sequence>
                &lt;page path="/opennms" port="8980" successMatch="Password" />
                &lt;page path="/opennms/j_spring_security_check"  port="8980" method="POST" 
                      failureMatch="(?s)Your log-in attempt failed.*Reason: ([^&lt;]*)" 
                      failureMessage="Login Failed: ${1}" 
                      successMatch="Log out">
                  &lt;parameter key="j_username" value="admin"/>
                  &lt;parameter key="j_password" value="admin"/>
                &lt;/page>
                &lt;page path="/opennms/event/index.jsp" port="8980" successMatch="Event Queries" />
                &lt;page path="/opennms/j_spring_security_logout" port="8980" successMatch="logged out" />
              &lt;/page-sequence>
            &lt;/parameter>
        &lt;/service>


        ...


        &lt;monitor service="OpenNMSLogin" class-name="org.opennms.netmgt.poller.monitors.PageSequenceMonitor"/>
(Note: The XML definition of a page follows the same design as the HTTP Collector's URL in that it has the same attributes and child elements. See the HTTP Collector URL page fragment to learn about all these attributes)
(Note: In 1.8.7+, changes were made to how redirect behavior is handled. See NMS-3827 Page Sequence Monitor uses configured method instead of GET when following redirects for more detail.)
Page sequences work by attempting to retrieve each page defined in the sequence and verifying that it meets the specified expectations. If any page fails, then the entire sequence fails and the monitor returns 'UNAVAILABLE' with a reason code indicating the reason for failure. If all of the pages succeed, then the monitor returns 'AVAILABLE' with a response time indicating the time it takes to run the entire sequence.
The above page sequence works as follows:
	1.	First do an HTTP GET of ${ipaddr}/opennms (following redirects as a browser would) and then checks to ensure that the resulting page has the phrase 'Password' on it. Each page is checked to ensure it HTTP response code fits into the response-range which defaults 100-399. This is done before any sucess or failure matches occur and out of range indicated a failing page.
	2.	Next a login is attempted.
	1.	The URL specified in the path attribute, is the relative URL used for submitting form data. The parameters for the page indicate the form's data and values to be submitted. In this example, the monitor will be sending the user and password as values for the form's j_username and j_password parameters.
	2.	After getting the resulting page, first the expression specified in the page's failureMatch attribute is verified. If the failure match expression is found anywhere on the page, then page has failed. In this example, it indicates that the login failed. The failureMessage is then used to construct the reason code. ${n} values are used to pull information from matching groups in the failureMatch regular expression.
	3.	If the failureMatch expression is not found in the resulting page, then the expression specified in the page's successMatch attribute is next checked to ensure it matches the resulting page. If successMatch expression is not found on the page, then the page fails. Both the failureMatch and the successMatch attributes are optional.
	3.	If the monitor was able to successfully login, then the next page is processed. In this example, the monitor navicates to Event page to ensure that the text "Event Queries" is found on the page.
	4.	The final page in the sequence is a logout page.
[edit] Another Example Using HTTP and HTTPS mixed
                &lt;page-sequence>
                    &lt;page scheme="http" host="ecomm.example.com" port="80"
                        path="/ecomm/jsp/Login.jsp"
                        virtual-host="ecomm.example.com"
                        successMatch="eComm Login" timeout="10000" http-version="1.1"/>
                    &lt;page scheme="https" method="POST"
                        host="ecomm.example.com" port="443"
                        path="/ecomm/controller"
                        virtual-host="ecomm.example.com"
                        successMatch="requesttab_select.gif"
                        failureMessage="Login failed: ${1}"
                        timeout="10000" http-version="1.1">
                        &lt;parameter key="action_name" value="XbtnLogin"/>
                        &lt;parameter key="session_timeout" value=""/>
                        &lt;parameter key="userid" value="EXAMPLE"/>                        
                        &lt;parameter key="password" value="econ"/>
                    &lt;/page>                    
                    &lt;page scheme="http" host="ecomm.example.com" port="80"
                        path="/econsult/controller"                        
                        virtual-host="ecomm.example.com"
                        successMatch="You have successfully logged out of eComm"
                        timeout="10000" http-version="1.1">
                        &lt;parameter key="action_name" value="XbtnLogout"/>
                    &lt;/page>
                &lt;/page-sequence>
[edit] Session Variables
From OpenNMS 1.6.10 onward, the PSM supports session variables. This facility allows the assignment of strings from a retrieved page to variables that can be used in page parameters later in the same sequence. This example shows how to log in to the web UI of demo.opennms.org without knowing ahead of time what username and password to use.
    &lt;page-sequence name="opennms-login-seq-dynamic-credentials">
      &lt;page path="/opennms" port="80" virtual-host="demo.opennms.org"
            successMatch="(?s)User:.*&lt;strong>(.*?)&lt;/strong>.*?Password:.*?&lt;strong>(.*?)&lt;/strong>">
        &lt;session-variable name="username" match-group="1" />
        &lt;session-variable name="password" match-group="2" />
      &lt;/page>      &lt;page path="/opennms/j_acegi_security_check"  port="80" virtual-host="demo.opennms.org" method="POST"
            failureMatch="(?s)Your log-in attempt failed.*Reason: ([^&lt;]*)" failureMessage="Login Failed: ${1}"
            successMatch="Log out">"
        &lt;parameter key="j_username" value="${username}" />
        &lt;parameter key="j_password" value="${password}" />
      &lt;/page>
      &lt;page path="/opennms/event/index.jsp" port="80" virtual-host="demo.opennms.org" successMatch="Event Queries" />
      &lt;page path="/opennms/j_acegi_logout" port="80" virtual-host="demo.opennms.org" successMatch="logged off" />
    &lt;/page-sequence>
The &lt;session-variable> tags tell the PSM to assign match groups 1 and 2 from the successMatch regular expression of the first page to the variables username and password, respectively. The match groups correspond to the parenthesized sub-expressions in the expression, highlighted here:
(?s)User:.*&amp;lt;strong&amp;gt;(.*?)&amp;lt;/strong&amp;gt;.*?Password:.*?&amp;lt;strong&amp;gt;(.*?)&amp;lt;/strong&amp;gt;
(Note that the (?s) at the beginning of the regular expression is a modifier and not a subexpression.)
The variables are referenced in &lt;parameter> child tags of the second page using the ${varName} convention:
&lt;parameter key="j_username" value="${username}" />
&lt;parameter key="j_password" value="${password}" />
[edit] Per-Page Response Times
From OpenNMS 1.6.10 onwards, the PSM can store the response times for individual pages in a sequence. To use this functionality, just add a ds-name attribute to each page whose load time you want to record. For example:
&lt;page path="/opennms/event/index.jsp" ds-name="event-page" successMatch="Event Queries" />
The response time for each such page will be stored in the same RRD file specified for the service via the rrd-base-name parameter under the specified datasource name. Note that you will need to delete existing RRD files and let them be recreated with the new list of datasources when you add a ds-name attribute to a page in a sequence that is already storing response time data.
[edit] Per-Page Response Graph Configuration
Graph configuration for per-page response collection isn't entirely intuitive. RRD data of total session and each page is stored within a single file. Given the following poller configuration (removed unrelated bits):
    &lt;service name="OpenNMSLogin" interval="300000" user-defined="false" status="on">
       &lt;parameter key="rrd-repository" value="/opt/opennms/share/rrd/response"/>
       &lt;parameter key="rrd-base-name" value="opennmslogin"/>
       &lt;parameter key="ds-name" value="opennmslogin"/>
       &lt;parameter key="page-sequence">
           &lt;page path="/opennms/acegilogin.jsp" port="8980" ds-name="login-page" successMatch="Login" />
           &lt;page path="/opennms/event/index.jsp" port="8980" ds-name="event-page" successMatch="Event Queries" />
         &lt;/page-sequence>
       &lt;/parameter>
   &lt;/service>
Graph definition (response-graph.properties) would end up as:
report.opennmslogin.name=OpenNMS Login
report.opennmslogin.columns=opennmslogin
report.opennmslogin.type=responseTime, distributedStatus
report.opennmslogin.command=--title="OpenNMS Login Response" \
 --vertical-label="Seconds" \
 DEF:totalrtMills={rrd1}:opennmslogin:AVERAGE \
 DEF:totalminRtMills={rrd1}:opennmslogin:MIN \
 DEF:totalmaxRtMills={rrd1}:opennmslogin:MAX \
 DEF:page1rtMills={rrd1}:login-page:AVERAGE \
 DEF:page1minRtMills={rrd1}:login-page:MIN \
 DEF:page1maxRtMills={rrd1}:login-page:MAX \
 DEF:page2rtMills={rrd1}:event-page:AVERAGE \
 DEF:page2minRtMills={rrd1}:event-page:MIN \
 DEF:page2maxRtMills={rrd1}:event-page:MAX \
 CDEF:totalrt=totalrtMills,1000,/ \
 CDEF:totalminRt=totalminRtMills,1000,/ \
 CDEF:totalmaxRt=totalmaxRtMills,1000,/ \
 CDEF:page1rt=page1rtMills,1000,/ \
 CDEF:page1minRt=page1minRtMills,1000,/ \
 CDEF:page1maxRt=page1maxRtMills,1000,/ \
 CDEF:page2rt=page2rtMills,1000,/ \
 CDEF:page2minRt=page2minRtMills,1000,/ \
 CDEF:page2maxRt=page2maxRtMills,1000,/ \
 LINE2:totalrt#00ff00:"Total Session" \
 GPRINT:totalrt:AVERAGE:" Avg\\: %6.2lf %s" \
 GPRINT:totalrt:MIN:"Min\\: %6.2lf %s" \
 GPRINT:totalrt:MAX:"Max\\: %6.2lf %s\\n" \
 AREA:page1rt#0000ff:"Login Page   " \
 GPRINT:page1rt:AVERAGE:" Avg\\: %6.2lf %s" \
 GPRINT:page1rt:MIN:"Min\\: %6.2lf %s" \
 GPRINT:page1rt:MAX:"Max\\: %6.2lf %s\\n" \
 STACK:page2rt#ff00ff:"Event Page   " \
 GPRINT:page2rt:AVERAGE:" Avg\\: %6.2lf %s" \
 GPRINT:page2rt:MIN:"Min\\: %6.2lf %s" \
 GPRINT:page2rt:MAX:"Max\\: %6.2lf %s\\n"
[edit] Importing a custom SSL-certificate
To import a SSL-certificate into the default keystore used by Java you should perform the following steps.
[edit] Obtain a Copy of the SSL Certificate
First, copy the SSL certificate to a file, preferably in PKCS#7 format. You can do this easily by using openssl:
$ openssl s_client -connect www.opennms.com:443
Then copy the certificate (the text in between "-----BEGIN CERTIFICATE---" and "-----END CERTIFICATE-----", including those tags) into a text file. This block of text is the PKCS#7-formatted certificate data.
You can also save the certificate using Firefox. In Firefox, visit the desired URL. Click on the SSL lock icon on the status bar once you reach the page. Click "View Certificate", then go to the "Details" tab, then click "Export...". This will allow you to export the certificate (or the entire certificate chain) in PKCS#7 format.
Another option is to simply contact the administrator of the service and have them give you a copy of the certificate file.
Note that the certificate file does not contain confidential or cryptographically insecure data: it is the public portion of the encrypted content. You are not compromising the security of the server by copying its certificate file.
[edit] Import the Certificate into the Keystore Using Java Keytool
Run the following command. If the JAVA_HOME environment variable is undefined, replace it with the full path where your Java Developer Kit (JDK) is installed.
$JAVA_HOME/bin/keytool -import -keystore $JAVA_HOME/jre/lib/security/cacerts -file &lt;pkcs7_filename>
[edit] Debugging
Logs are kept in a couple files. There are some standard poller-type entries and a verbose http page log. They are enabled by setting appropriate DEBUG levels in the logging properties file. A restart is NOT needed.
etc/log4j.properties
# Pollers
log4j.category.OpenNMS.Poller=DEBUG, POLLERS
# Miscellaneous
...
log4j.category.httpclient=DEBUG, MISC

[edit] Version History/Availability
	•	This feature was added in version 1.3.2
	•	This feature was enhanced or modified in version 1.3.7
	•	This feature was enhanced or modified in version 1.6.1
	•	This feature was enhanced or modified in version 1.6.10
</Text>
        </Document>
        <Document ID="14">
            <Title>Creating Tickets in a Ticket System</Title>
        </Document>
        <Document ID="97">
            <Title>Template</Title>
            <Text>Description
Context
Content
Version
Author</Text>
        </Document>
        <Document ID="15">
            <Title>One chapter per way to provision with provisiond</Title>
        </Document>
        <Document ID="98">
            <Title>Remote Monitor</Title>
            <Text>Did you mean Remote Monitoring?
Create a process that can be installed  a remote machine, perhaps with access to a network that the main OpenNMS instance cannot reach, but with the goal to achieve visibility of a service defined in OpenNMS from the user's perspective. It will be able to perform all of the monitoring checks that the current service monitor does (ICMP, HTTP, etc.) and send events such as nodeDown or nodeLostService back to the main OpenNMS instance.
It will not do any discovery on its own, rather, it will take direction from the main OpenNMS instance as to what to monitor.
This Milestone is dependent on the first sub-task of the Correlation milestone.
Distributed polling is an enhancement that is part of a custom development project. In order to distribute the poller code the correlation logic currently contained in the OpenNMS Poller daemon will be removed and will become a seperate service allowing different correlation services to be added. For example, topology based correlation logic from the Italian Adventures branch in CVS and business logic correlation using a rules engine such as Drools.
Contents
[hide]
	•	1 Sub Tasks
	•	2 OpenNMS Remote Monitoring Use Case
	◦	2.1 Definitions:
	◦	2.2 Use Case
	▪	2.2.1 Detailed Configuration Explanation:
	▪	2.2.2 Detailed Operational Explanation:
[edit] Sub Tasks
	•	Build standalone asynchronous poller
	•	Build Web Start framework for on demand distribution of poller
	•	Enhance object model to allow multipoint service monitoring
[edit] OpenNMS Remote Monitoring Use Case
[edit] Definitions:
OpenNMS-MOM
For lack of a better term, this is the central OpenNMS system running all the OpenNMS daemons and manages communication with the OpenNMS-DMs.
OpenNMS-DP
A light weight polling process that communicates with the OpenNMS-MOM. An OpenNMS-DP can operate in the role of a distributed polling or a distributed monitoring application.
Remote Polling
An OpenNMS polling process that discovers, schedules, monitors, and correlates services reporting discovery of new entities and correlated status messages to the OpenNMS-MOM. (This project has not been scoped)
Remote Monitoring
An OpenNMS polling process that simply receives a polling schedule from the OpenNMS-MOM and monitors a list of services that have been identified by the OpenNMS administrator for distributed monitoring. Each poll status is reported back to the OpenNMS-MOM and aggregated such that the distributed pollers are monitoring the same application from multiple perspectives.
OpenNMS Entity
An abstraction in the OpenNMS object model from which persisted objects are extended and can be represented with status and/or performance metrics (i.e. network service, interface, node, virtual node, application, etc.)
Polling configuration
An extremely flexible XML based configuration that defines for an OpenNMS poller the services to be monitored on OpenNMS Entities, the schedule, a downtime polling model, and the tunable parameters for each service monitor.
[edit] Use Case
Typically, resources on the network are monitored by a network management system from a central location. Often times, these resources are accessible via multiple paths on a network and perhaps over various WAN and VPN technologies where outages and performance degradation may occur and not observed by a central NMS; sitting perhaps in the same location as the resources it’s monitoring. These resources need to be monitored from multiple remote locations so that their status can be seen from the prospective of the users accessing these resources.
The status and performance measurements (i.e. latency) of these services can be monitored from multiple locations and can be viewed at the OpenNMS-MOM, collectively.
An OpenNMS administrator defines an entity (an Application Entity for example) in the OpenNMS WebUI that is composed of services to be monitored by one or more distributed pollers. (Note: The situation could be that those services are only reachable by distributed pollers and not by the OpenNMS-MOM’s central polling services)
[edit] Detailed Configuration Explanation:
The OpenNMS administrator recognizes the requirement to monitor an application from multiple locations.
Steps:
	1.	The OpenNMS-MOM administrator creates a new OpenNMS Application entity that will be used to aggregate the status of services provided by that application and monitored remotely by an OpenNMS-DP.
	2.	The OpenNMS-MOM administrator defines remote polling locations.
	3.	The OpenNMS-MOM administrator creates or modifies a polling configuration for remote polling location that will be used to monitor services defined for the Application entity created in step 1.
	4.	A system administrator installs the distributed polling code on one or more remote systems in the required locations. They modify the distributed monitoring properties file, on each instance, to define the remote location (using the location name provided by the OpenNMS-MOM administrator from step 2) and the IP address of the OpenNMS-MOM.
[edit] Detailed Operational Explanation:
	1.	Following the configuration steps above, the remote system administrator starts the OpenNMS-DP and verifies its connection to the OpenNMS-MOM by either a) looking at the poller.log file or via the optional OpenNMS-DP GUI.
	2.	The OpenNMS-MOM receives the initial communication from the OpenNMS-DP and registers it as active.
	3.	The OpenNMS-MOM sends the OpenNMS-DP the polling configuration defined by the OpenNMS-MOM administrator and the OpenNMS-DP begins monitoring services and reporting poll status information.
	4.	An aggregated status view of each of the OpenNMS Entities being remotely monitored can be seen in the OpenNMS WebUI.
	5.	The status of monitors themselves is represented in the OpenNMS WebUI as determined by successful communication and execution of the remote polling schedule. Alarms and notifications are initiated when failures occur. Distributed pollers can be configured to cease all monitoring activity when communication with OpenNMS-MOM is lost (the lysine contingency).
	6.	Distributed monitors have a separate thread that continuously checks for updates to the remote location’s polling configuration and immediately adapts the new configuration.
	7.	Poll status messages reported to the OpenNMS-MOM contain:
	1.	status of services
	2.	latency of services
	3.	with some monitors (such as HTTP) bandwidth utilization. Bandwidth utilization is calculated by requesting a static HTML page and determining the size and the time required to make the transfer.
	8.	List of Distributable Monitors
	1.	CitrixMonitor
	2.	DnsMonitor
	3.	DominoIIOPMonitor
	4.	FtpMonitor
	5.	HttpMonitor
	6.	HttpsMonitor
	7.	ImapMonitor
	8.	JDBCMonitor
	9.	JMXMonitor
	10.	LdapMonitor
	11.	LoopMonitor
	12.	NrpeMonitor
	13.	NsclientMonitor
	14.	NtpMonitor
	15.	PageSequenceMonitor
	16.	Pop3Monitor
	17.	RadiusAuthMonitor
	18.	SmtpMonitor
	19.	SshMonitor
	20.	TcpMonitor
	9.	list of monitors in 1.3.7
	1.	AvailabilityMonitor
	2.	CitrixMonitor
	3.	DnsMonitor
	4.	DominoIIOPMonitor
	5.	FtpMonitor
	6.	HttpMonitor
	7.	HttpsMonitor
	8.	ImapMonitor
	9.	JDBCMonitor
	10.	JMXMonitor
	11.	LdapMonitor
	12.	LoopMonitor
	13.	NrpeMonitor
	14.	NsclientMonitor
	15.	NtpMonitor
	16.	PageSequenceMonitor
	17.	Pop3Monitor
	18.	RadiusAuthMonitor
	19.	SmtpMonitor
	20.	SshMonitor
	21.	TcpMonitor
</Text>
        </Document>
        <Document ID="16">
            <Title>Installing</Title>
        </Document>
        <Document ID="20">
            <Title>SNMP Datacollection</Title>
        </Document>
        <Document ID="99">
            <Title>Remote Polling</Title>
            <Text>Overview
Remote polling allows you to poll devices from disparate locations, allowing you to get an aggregate of availability across sites. The remote polling architecture utilizes the same interfaces and services that OpenNMS's traditional centralized polling uses, but has it's own availability data tuned towards keeping track of multiple pollers originating from the same site, as well as combining that data with other site data for an overall view of your monitored devices. This data can be viewed either through a tabular report page, or a GUI map.
[edit] Architecture
The OpenNMS remote polling facility uses a client-server model, with clients checking in at regular intervals with updated poll data and a heartbeat, and the server considering a remote poller down if that client doesn't "check in" in a certain amount of time.
The remote poller client is designed to be a standalone jar file which can be run from the command-line, or through Java webstart.
[edit] Getting Started
[edit] Server-Side (Your Central OpenNMS Server)
[edit] Before You Start
Make sure OpenNMS is working first! Before you work on trying to add remote pollers to the make, be sure that the central OpenNMS server is doing what you want. Able to poll, send notifications, get graphs, etc. You don't want to be debugging server issues while trying to figure out why a remote poller might not be reporting in.
[edit] poller-configuration.xml
The first step is to decide what interfaces and services you wish to poll remotely. The default $OPENNMS_HOME/etc/poller-configuration.xml comes with an "example1" &lt;package> section, which is used by the central OpenNMS server for polling. You will now want to add another package to define which services should be polled remotely.
Define a new &lt;package>, giving it a unique name, and making sure that it contains the attribute remote="true". Then, include any services you wish to poll remotely. Note that some services (like ICMP and SNMP) are not distributable because they rely on configuration outside of the &lt;service> definition, or on native code.
Here is an example that matches all IP addresses, and enables polling HTTP remotely:
&lt;package name="raleigh" remote="true">
  &lt;filter>IPADDR IPLIKE *.*.*.*&lt;/filter>
  &lt;include-range begin="1.1.1.1" end="254.254.254.254"/>
  &lt;rrd step = "300">
    &lt;rra>RRA:AVERAGE:0.5:1:2016&lt;/rra>
    &lt;rra>RRA:AVERAGE:0.5:12:4464&lt;/rra>
    &lt;rra>RRA:MIN:0.5:12:4464&lt;/rra>
    &lt;rra>RRA:MAX:0.5:12:4464&lt;/rra>
  &lt;/rrd>
  &lt;service name="HTTP" interval="30000" user-defined="false" status="on">
    &lt;parameter key="retry" value="1"/>
    &lt;parameter key="timeout" value="3000"/>
    &lt;parameter key="port" value="80"/>
    &lt;parameter key="url" value="/"/>
    &lt;parameter key="rrd-repository" value="/var/log/opennms/rrd/response"/>
    &lt;parameter key="ds-name" value="http"/>
  &lt;/service>
  &lt;outage-calendar>zzz from poll-outages.xml zzz&lt;/outage-calendar>

  &lt;!-- 30s, 0, 5m -->
  &lt;downtime interval="30000" begin="0" end="300000"/>
  &lt;!-- 5m, 5m, 12h -->
  &lt;downtime interval="300000" begin="300000" end="43200000"/>
  &lt;!-- 10m, 12h, 5d -->
  &lt;downtime interval="600000" begin="43200000" end="432000000"/>
  &lt;!-- anything after 5 days delete -->
  &lt;downtime begin="432000000" delete="true"/>
&lt;/package>
[edit] monitoring-locations.xml
The next step is to decide what locations your remote pollers will be able to check in from. These will generally be site-specific, and will have a GPS location associated with them (so they can be displayed on the maps).
The $OPENNMS_HOME/etc/monitoring-locations.xml file defines the different locations from which remote poller monitoring instances will be running. Inside the &lt;locations> tag, create one or more &lt;location> entries with a set of attributes that uniquely identify it:
location-name 
The short name of the location, used on the remote-poller startup command-line.
monitoring-area 
Used to group multiple locations together.
polling-package-name 
The package in poller-configuration.xml that the monitor will use to determine the services to poll.
geolocation 
(As of OpenNMS 1.7.11) The geographical location of the monitor. This should be a street address or similar. If none is specified or Google can't resolve the address to a latitude and longitude, the marker will be placed on the map at OpenNMS World HQ in Pittsboro, NC.  :)
coordinates 
(As of OpenNMS 1.7.11) The geographical location of the monitor in the format "latitude,longitude".
priority 
(As of OpenNMS 1.7.11) The sort priority of this location for the UI (1 is lowest, 100 is highest).
It can also be optionally associated with 0 or more tags that identify a location. Generally these will be arbitrary metadata associated with that monitoring location.
For example, here's a monitoring-locations.xml that defines a location for The OpenNMS Group, Inc. headquarters in Pittsboro, NC:
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;monitoring-locations-configuration
  xmlns="http://www.opennms.org/xsd/config/monitoring-locations"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://xmlns.opennms.org/xsd/config/monitoring-locations
    http://www.opennms.org/xsd/config/monitoring-locations.xsd ">
  &lt;locations>
    &lt;location-def location-name="RDU" 
        monitoring-area="raleigh" polling-package-name="raleigh"
        geolocation="The OpenNMS Group, Pittsboro, NC"
        coordinates="35.7174,-79.1619" priority="50">
      &lt;tags>
        &lt;tag name="store" />
        &lt;tag name="production" />
      &lt;/tags>
    &lt;/location-def>
  &lt;/locations>
&lt;/monitoring-locations-configuration>

[edit] users.xml and magic-users.properties
By default, only administrators have the rights to send remote poller data to the central server, so you will want to create a user, or users, with remote polling rights to avoid using an admin username and password.
The easiest way to do so is to go to the OpenNMS admin UI, and add a new user there:
 http://192.168.0.1:8980/opennms/admin/userGroupView/users/list.jsp
Once you've created the user, edit $OPENNMS_HOME/etc/magic-users.properties and add that user's ID to the role.remoting.users property.
[edit] Restart OpenNMS
Now that you've finished the server-side configuration, restart OpenNMS.
[edit] Client Side (The Remote Pollers)
[edit] Installing the Remote Poller
If you are on an RPM or Debian-based install, you should be able to just install the "opennms-remote-poller" package through yum or apt.
If not, you can download the latest remote poller standalone distribution at the OpenNMS SourceForge project page.
These instructions will assume you are using one of the pre-packaged remote pollers, which provides the shell wrapper script in $OPENNMS_HOME/bin/remote-poller.sh.
[edit] Running the Remote Poller
To run the remote poller and make sure everything's working, the easiest way to do so is just to start it on the command-line. No real configuration is necessary, but you will need to know the following information:
	•	the location you wish to use (the "location-name" tag in monitoring-locations.xml)
	•	how to reach your OpenNMS server from the remote poller system
	•	the username and password you created above
If you run it without options, it gives examples on what options are available:
$ $OPENNMS_HOME/bin/remote-poller.sh 
usage:
 -d,--debug            write debug messages to the log
 -g,--gui              start a GUI (default: false)
 -h,--help             this help
 -l,--location &lt;arg>   the location name of this remote poller
 -n,--name &lt;arg>       the name of the user to connect as
 -p,--password &lt;arg>   the password to use when connecting
 -u,--url &lt;arg>        the URL for OpenNMS (default: rmi://server-name/)
If your OpenNMS server is reachable at http://192.168.0.1:8980/opennms, then you will want to use the following command:
 $OPENNMS_HOME/bin/remote-poller.sh -l RDU -n remoteuser -p remotepass \
   -u http://192.168.0.1:8980/opennms-remoting
You should start to see data in the distributed status page within your polling interval (usually 5 minutes).
 http://192.168.0.1:8980/opennms/distributedStatusSummary.htm
[edit] Advanced Topics
[edit] Configuring Applications
Applications allow you to create a collection of arbitrary services and treat them as a single unit with its own availability calculation. This is useful for creating an overall "service" that represents a number of different things.
For example, if you have a public-facing web application which uses tomcat, retrieves files from a SAN, and reads data from a database on another machine, you could create a single application which contains the HTTP service from the tomcat system and the SAN machine, and a JDBCStoredProcedureMonitor service from the database machine.
Applications can be configured in the applications UI:
 http://192.168.0.1:8980/opennms/admin/applications.htm
[edit] Configuring Maps
As of OpenNMS 1.7.11, support for distributed maps was added, which lets you visualize locations and applications on a world map, based on the geolocation data in your monitoring-locations.xml.
All distributed map configuration is done in the $OPENNMS_HOME/etc/opennms.properties file on the central server.
[edit] Configure Map Type
First, configure the type of map API you wish to use. If you have a Google Maps API key, or a MapQuest API key, you can choose "GoogleMaps" or "Mapquest" as the implementation, otherwise, OpenLayers uses OpenStreetMaps, an open-data project for providing map data in an open source manner, and should work for any user.
[edit] Configure Geocoding
Geocoding is what converts addresses into coordinates, and can be necessary to look up your addresses in monitoring-locations.xml if you did not provide exact coordinates. If you choose the Google or MapQuest geocoders, they will use the API key you configured earlier in opennms.properties. If you choose the Nominatim geocoder, you will have to configure your email address by setting:
 gwt.geocoder.email=&lt;your-address>
It is a requirement from the OpenStreetMaps geocoder that you provide a contact address so they can contact you if you are making too many queries, since their server is run by a volunteer organization.
[edit] About Using OpenStreetMaps
As of 1.8.7, our default OpenStreetMaps implementation uses MapQuest's servers for data. MapQuest has contributed a large amount of resources to the OpenStreetMaps community, including servers which are free for all to use. If you see references to MapQuest relating to OpenLayers in our configuration, it is because we are using the open MapQuest resources, not the paid enterprise ones.
[edit] Accessing the Maps
Just click on the "Distributed Maps" link in the menu bar in your OpenNMS server's web UI.
[edit] Starting the Remote Poller
While the most common way to run the remote poller is as a console-only, command-line tool, it also provides a GUI version which you can start either from the command-line, or through Java webstart.
[edit] Command-Line GUI
To start the GUI from the command-line, add the "-g" option to your remote-poller command, like so:
 $OPENNMS_HOME/bin/remote-poller.sh -g -l RDU \
   -n remoteuser -p remotepass \
   -u http://192.168.0.1:8980/opennms-remoting
[edit] HTTP Proxies
If you need to use an HTTP proxy to communicate with the OpenNMS server, add the http.proxyHost and http.proxyPort options to the java command-line:
 $OPENNMS_HOME/bin/remote-poller.sh \
   -Dhttp.proxyHost=proxy.mydomain.net  \
   -Dhttp.proxyPort=8080 -g -l RDU \
   -u http://192.168.0.1:8980/opennms-remoting
[edit] Design Overview
For a complete overview of the remote poller architecture, see the Remote Poller Design Overview page.
[edit] Version History/Availability
	•	This feature was added in version 1.3.2
	•	This feature was enhanced or modified in version 1.5.94
	•	This feature was enhanced or modified in version 1.7.11
	•	This feature was enhanced or modified in version 1.8.7
</Text>
        </Document>
        <Document ID="17">
            <Title>Sizing</Title>
        </Document>
        <Document ID="21">
            <Title>Monitoring</Title>
        </Document>
        <Document ID="18">
            <Title>Thresholding</Title>
        </Document>
        <Document ID="22">
            <Title>Maps</Title>
        </Document>
        <Document ID="19">
            <Title>Notifications</Title>
        </Document>
        <Document ID="23">
            <Title>Graphing</Title>
        </Document>
        <Document ID="24">
            <Title>Notifications</Title>
        </Document>
        <Document ID="25">
            <Title>Syslog</Title>
        </Document>
        <Document ID="30">
            <Title>System Architecture</Title>
        </Document>
        <Document ID="26">
            <Title>Debugging</Title>
        </Document>
        <Document ID="31">
            <Title>KSC Reports</Title>
        </Document>
        <Document ID="27">
            <Title>LinkD</Title>
        </Document>
        <Document ID="32">
            <Title>User Management and external Authentication</Title>
        </Document>
        <Document ID="28">
            <Title>Eventd</Title>
        </Document>
        <Document ID="100">
            <Title>Event Config</Title>
            <Text>Introduction
[edit] Purpose
This How-To is one in a series designed to serve as a reference for getting started with OpenNMS. Eventually, these documents will cover everything necessary to get OpenNMS installed and running in your environment.
[edit] Copyright
Content is available under a Creative Commons Attribution-NonCommercial-ShareAlike2.5 License.
[edit] Corrections and Omissions
Please submit any corrections and omissions to the author.
[edit] Overview
OpenNMS has three main functional areas:
	•	Determining Availability of Network Services (discussed in part 3 of this series)
	•	Gathering Performance Data via SNMP (discussed in part 4 of this series)
	•	Event Management and Notifications
The last area: Events, is the subject of this How-To. Notifications will be handled in the next part.
Events are a staple of any network management system (NMS). In fact, the main function of an NMS can be described as detecting changes within the network, and every change can be thought of as an event.
OpenNMS manages events through a process called eventd. There are two main types of events: those generated internally by the OpenNMS software and those generated via external SNMP traps. Processes can generate events, such as when the discovery process generates a newSuspect event when an interface responds to a ping, and processes can "subscribe" to events, as when the capsd process asks to be notified whenever a newSuspect event occurs so it can begin its capabilities scan.
When an event is generated, various parameters can be set, such as its description, a log message and a severity. In addition, automatic actions can be launched to send event parameters to an external script. This is controlled through the eventconf.xml file.
OpenNMS also comes with a feature rich Notification system. Particular events can be chosen to cause a notification to be sent, such as a page or e-mail.
The following sections will discuss Events in detail. Since this functionality changed between 1.0 (the current production release at the time this document was written) and 1.1 (the current development release), notes will be made as to what is available in which release.
[edit] Events
The eventconf.xml file, found by default in /opt/OpenNMS/etc, is the configuration file that controls how events are handled. Let's look for a moment at the top of that file:
&lt;events xmlns="http://xmlns.opennms.org/xsd/eventconf">
  &lt;global>
    &lt;security>
      &lt;doNotOverride>logmsg&lt;/doNotOverride>
      &lt;doNotOverride>operaction&lt;/doNotOverride>
      &lt;doNotOverride>autoaction&lt;/doNotOverride>
      &lt;doNotOverride>tticket&lt;/doNotOverride>
    &lt;/security>
  &lt;/global>
Every file that contains events starts with an &lt;events> tag and ends with an &lt;/events> tag.
The eventd process listens on port 5817, so other processes, even those external to OpenNMS, can send events to the system. The &lt;security> tag is there so that these events cannot override the actions defined in the eventconf.xml file. This way, no one with access to the OpenNMS machine could send in an "autoaction" to open, say, a root window on their machine.
[edit] Internal Events
Now, after the global settings section of the eventconf.xml file come the events. Each event must be defined in order to be properly treated by the OpenNMS system. Let's look at a common event: nodeLostService:
&lt;event>
  &lt;uei>http://uei.opennms.org/nodes/nodeLostService&lt;/uei>
  &lt;event-label>OpenNMS-defined node event: nodeLostService&lt;/event-label>
  &lt;descr>
    &amp;#38lt;p&amp;#38gt;A %service% outage was identified on interface
    %interface%.&amp;#38lt;/p&amp;#38gt; &amp;#38lt;p&amp;#38gt;A new Outage record has been
    created and service level availability calculations will be
    impacted until this outage is resolved.&amp;#38lt;/p&amp;#38gt;
  &lt;/descr>
  &lt;logmsg dest='logndisplay'>
          %service% outage identified on interface %interface%.
  &lt;/logmsg>
  &lt;severity>Major&lt;/severity>
&lt;/event>
Every event is bracketed by an &lt;event> tag. Within those tags are various other definitions:
The UEI 
The "Universal Event Identifier" is simply a label to uniquely identify the event. The original intent was that this would be some sort of XML namespace, hence the "http://", but it really is just a label. In version 1.1 and beyond, the "http://" has been removed. Note: for internal OpenNMS events, the UEI is generated directly by the code and cannot be changed without modifying the source.
The event-label 
This is a plain text label for the event, sometimes used in the web user interface.
descr 
is the description of the event. You can embed HTML entities should you wish to format the description more fully. Note that there are elements such as %interface% that you can place in the event description and log message (which will be described later in this document).
logmsg 
Is a short description or summary of the event. The "dest" attribute can take on a number of values:
logndisplay 
Both log the event in the database and display it in the Web UI.
logonly 
Log the event in the database, but do not display it.
suppress 
Neither log the event in the database or display it.
donotpersist 
Do not log the event in the database, but still send it to OpenNMS daemons that are listening for this type of event (e.g.: this can be used to generate notifications).
discardtraps 
This only applies to traps coming in via trapd. This will cause trapd to discard the trap without creating an event. Other OpenNMS daemons that are listening for this type of event will not receive this event. This feature was first available in OpenNMS 1.3.0.
severity 
This indicates the severity of the event.
[edit] Severities
When setting severities for events, I often think back to a scene from the movie Spinal Tap. This movie is a "documentary" about an aging hard rock band. In one scene the interviewer is asking a band member to what he attributes the band's popularity. He points to the amplifiers and says "while most amplifiers go to 10, ours go to 11". The interviewer asks why not make them go to 10, but just make 10 louder, to which the band member replies "but ours go to 11".
I have seen this when people set out to set severities for the events in their NMS. Events are either "Critical" (something's wrong with my network!) or "Normal". The severities between the two tend to be ignored. Unfortunately, it makes it real hard to highlight really important events when everything is either green or red. So rather than put everything "at 11", I suggest setting normal network outages as Minor or Major (but treat them just the same as you normally would), and reserving Critical for those events that really matter.
Another way to look at this is to assign actions to severities. Thus a "Critical" event means you wake up the president of your company on Christmas morning and ruin his holiday. So if Helmut's backup ISDN circuit goes down, it is doubtful you would want the severity to be Critical. However, if the entire network backbone is down, it might warrant that phone call.
The following is a list of severities that come with OpenNMS, and my personal description of what they mean. Currently, the default events in OpenNMS don't exactly conform to this list, but they will in the future:
Critical (dark red) 
This event means numerous devices on the network are affected by the event. Everyone who can should stop what they are doing and focus on fixing the problem.
Major (light red) 
A device is completely down or in danger of going down. Attention needs to be paid to this problem immediately.
Minor (orange) 
A part of a device (a service, and interface, a power supply, etc.) has stopped functioning. The device needs attention.
Warning (yellow) 
An event has occurred that may require action. This severity can also be used to indicate a condition that should be noted (logged) but does not require direct action.
Normal (green) 
Informational message. No action required.
Cleared (light grey) 
This event indicates that a prior error condition has been corrected and service is restored.
Indeterminate (yellow-green) 
The severity of the event cannot be determined.
[edit] Additional Parameters
A number of additional parameters can occur between the &lt;event> tags (note that there are others defined in the code but not yet implemented):
&lt;operinstruct> 
This is a set of instructions for the NMS operator when the event occurs.
&lt;mouseovertext> 
This can be text to be displayed when the mouse is placed over the event in the event browser of the Web UI.
&lt;autoaction> 
The text following this tag must be the complete path to an executable program. The program will be executed every time the event occurs.
Note that each tag must be closed with its corresponding "/" tag.
[edit] Elements
Various elements can be included in the description, log message, operator instruction and automatic actions for each event. Not all events will have values for all elements, and some refer to SNMP traps, which will be discussed in the next section.
%uei% 
The Universal Event Identifier for the event.
%source% 
The source of the event (what process).
%time% 
The time of the event.
%nodeid% 
The node ID of the device that caused the event.
%interface% 
The interface associated with the event.
%service% 
The service associated with the event.
%severity% 
The severity of the event.
%snmphost% 
The host of the SNMP agent that generated the event.
%snmp% 
The SNMP information associated with the event.
%id% 
The SNMP Enterprise OID for the event.
%generic% 
The Generic trap number for the event.
%specific% 
The Specific trap number for the event.
%community% 
The community string for the trap.
%version% 
The SNMP version of the trap.
%operinstruct% 
The operator instructions for the event.
%mouseovertext% 
The mouse over text for the event.
In version 1.1 and beyond, there are two more parameters available:
%nodelabel% 
Returns the node label for the node given in %nodeid% if available.
%interfaceresolv% 
Does a reverse lookup on the %interface% and returns its name if available.
There is also a parm element that will be discussed later.
[edit] SNMP Traps
Outside of internally generated events, OpenNMS can also receive SNMP traps via the trapd process. These are controlled in eventconf.xml using the &lt;mask> tag. For example:
&lt;event>
  &lt;mask>
    &lt;maskelement>
      &lt;mename>id&lt;/mename>
      &lt;mevalue>.1.3.6.1.4.1.9.9.70.2&lt;/mevalue>
    &lt;/maskelement>
    &lt;maskelement>
      &lt;mename>generic&lt;/mename>
      &lt;mevalue>6&lt;/mevalue>
    &lt;/maskelement>
    &lt;maskelement>
      &lt;mename>specific&lt;/mename>
      &lt;mevalue>17&lt;/mevalue>
    &lt;/maskelement>
  &lt;/mask>
  &lt;uei>http://uei.opennms.org/vendor/Cisco/traps/ciscoC3800SysAggregateStatusChange&lt;/uei>
  &lt;event-label>CISCO-C3800-MIB defined trap event: ciscoC3800SysAggregateStatusChange&lt;/event-label>
  &lt;descr>&amp;#38lt;p&amp;#38gt;Notification that the aggregate status of a node
         has changed.&amp;#38lt;/p&amp;#38gt;&amp;#38lt;table&amp;#38gt;
         &amp;#38lt;tr&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;b&amp;#38gt;
         c3800SysNextTrapSeqNum&amp;#38lt;/b&amp;#38gt;&lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;%parm[#1]%
         &amp;#38lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;p;&amp;#38gt;&lt;/p&amp;#38gt;&amp;#38lt;/td;&amp;#38gt;&amp;#38lt;/tr&amp;#38gt;&amp;#38lt;tr&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;b&amp;#38gt;
         sysName&amp;#38lt;/b&amp;#38gt;&amp;#38lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;%parm[#2]%
         &amp;#38lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;p;&amp;#38gt;&amp;#38lt;/p&amp;#38gt;&amp;#38lt;/td;&amp;#38gt;&amp;#38lt;/tr&amp;#38gt;&amp;#38lt;tr&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;b&amp;#38gt;
         c3800SysTrapSeverity&amp;#38lt;/b&amp;#38gt;&amp;#38lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;%parm[#3]%
         &amp;#38lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;p;&amp;#38gt;
         clear(1) minor(2) major(3)&amp;#38lt;/p&amp;#38gt;
         &amp;#38lt;/td;&amp;#38gt;&amp;#38lt;/tr&amp;#38gt;&amp;#38lt;tr&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;b&amp;#38gt;
         c3800SysAggregateStatus&amp;#38lt;/b&amp;#38gt;&amp;#38lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;%parm[#4]%
         &amp;#38lt;/td&amp;#38gt;&amp;#38lt;td&amp;#38gt;&amp;#38lt;p;&amp;#38gt;
         clear(1) minor(2) major(3)&lt;/p>
         &amp;#38lt;/td;&amp;#38gt;&amp;#38lt;/tr&amp;#38gt;&amp;#38lt;/table&amp;#38gt;
  &lt;/descr>
  &lt;logmsg dest='logndisplay'>&lt;p>Cisco Event: C3900: Node Status has changed.&lt;/p>&lt;/logmsg>
  &lt;severity>Indeterminate&lt;/severity>
&lt;/event>
This is a Cisco Systems event for their C3800 device. Parts of it looks similar to the internally generated events, with the main difference being the &lt;mask> block. This block consists of &lt;maskelement> tags, and the event will only match if all the defined tags are met.
This particular event will match an SNMP trap whose enterprise OID (id) is equal to ".1.3.6.1.4.1.9.9.70.2", its generic trap value is enterprise specific (6) and its specific trap value is 17.
The possible &lt;mename> values are:
	•	uei
	•	source
	•	host
	•	snmphost
	•	nodeid
	•	interface
	•	service
	•	id
	•	specific
	•	generic
	•	community
It is possible to use the "%" symbol to indicate a wildcard in the mask values. For example, to match all Cisco events, I could use:
&lt;mask>
  &lt;maskelement>
    &lt;mename>id&lt;/mename>
    &lt;mevalue>.1.3.6.1.4.1.9.%&lt;/mevalue>
  &lt;/maskelement>
&lt;/mask>
Note: The order in which events are listed in the eventconf.xml file is extremely important. The search will stop with the first event definition that matches the given event. Thus if the above code with the wildcard was listed before the more specific ciscoC3800SysAggregateStatusChange event, the latter event would never be generated. Also note that the wildcard is simply a substring match. If an event was generated from a Cisco device with the Enterprise OID of ".1.3.6.1.4.1.9" it would not match this event, as there is no trailing ".". If the trailing "." is left off, care must be taken so that a trap with an OID of ".1.3.6.1.4.1.99" is listed before the ".1.3.6.1.4.1.9%" event or else it will match the more generic event.
[edit] The parm Element
Some events, especially SNMP traps, have additional information sent with them called "variable bindings" or "varbinds" for short. In the ciscoC3800SysAggregateStatusChange event listed above, there are four of them, and they can be accessed using the parm element. Each parameter consists of a name and a value.
%parm[all]% 
Will return a space-separated list of all parameter values in the form parmName1="parmValue1" parmName2="parmValue2" etc.
%parm[values-all]% 
Will return a space-separated list of all parameter values associated with the event.
%parm[names-all]% 
Will return a space-separated list of all parameter names associated with the event.
%parm[&lt;name>]% 
Will return the value of the parameter named &lt;name> if it exists.
%parm[##]% 
Will return the total number of parameters.
%parm[#&lt;num>]% 
Will return the value of parameter number &lt;num>.
%parm[name-#&lt;num>]% 
Will return the name of parameter number &lt;num>.
For example, the ciscoC3800SysAggregateStatusChange event description lists out each of the parameters. Thus the second paramater, the sysName is printed out using %parm[#2]%.
[edit] Filtering on varbinds (OpenNMS 1.1 and beyond)
Let's take a look at the example ciscoC3800SysAggregateStatusChange event once more. What should its severity be? Since the event is generated whenever the status changes, you don't know if the change is "bad" (from operational to non-operational) or "good" (the non-operational status is cleared). That information is contained within the parameters that are passed with the event, particularly parameter #3, the trap severity.
With 1.1, the ability to filter on variable bindings was added. This is done in the &lt;mask> block. To re-write the above event:
&lt;mask>
  &lt;maskelement>
    &lt;mename>id&lt;/mename>
    &lt;mevalue>.1.3.6.1.4.1.9.9.70.2&lt;/mevalue>
  &lt;/maskelement>
  &lt;maskelement>
    &lt;mename>generic&lt;/mename>
    &lt;mevalue>6&lt;/mevalue>
  &lt;/maskelement>
  &lt;maskelement>
    &lt;mename>specific&lt;/mename>
    &lt;mevalue>17&lt;/mevalue>
  &lt;/maskelement>
  &lt;varbind>
    &lt;vbnumber>3&lt;/vbnumber>
    &lt;vbvalue>3&lt;/vbvalue>
  &lt;/varbind>
&lt;/mask>
Copying the event and changing the &lt;mask> block to the above will match on the same id, generic and specific values, but also will require that the third parameter is equal to "3" (indicating a Cisco determined trap severity of "major"). Thus you could change the description and/or severity to match the event.
It is also possible to match more than one varbind, and more than one value per varbind:
&lt;varbind>
  &lt;vbnumber>3&lt;/vbnumber>
  &lt;vbvalue>2&lt;/vbvalue>
  &lt;vbvalue>3&lt;/vbvalue>
&lt;/varbind>
&lt;varbind>
  &lt;vbnumber>4&lt;/vbnumber>
  &lt;vbvalue>2&lt;/vbvalue>
  &lt;vbvalue>3&lt;/vbvalue>
&lt;/varbind>
The above code snippet will match if the third parameter has a value of "2" or "3" and the fourth parameter has a value of "2" or "3".
This feature was updated long before the 1.6.0 release to allow a regular expression match on the varbind value. Just specify the expression prefixed with a with a "~".
&lt;varbind>
  &lt;vbnumber>1&lt;/vbnumber>
  &lt;vbvalue>~[Dd]own&lt;/vbvalue>
&lt;/varbind>
This will match a varbind 1 containing the word "Down" or "down" anywhere within its value.
You can also do quick prefix matches with the '%' in a varbind value:
&lt;varbind>
  &lt;vbnumber>1&lt;/vbnumber>
  &lt;vbvalue>Error:%&lt;/vbvalue>
&lt;/varbind>
this will match varbind 1 with any string beginning with "Error:"
Again, note that the order in which events are listed is very important. Put the most specific events first.
[edit] Decoding varbinds (OpenNMS 1.7.0 and beyond)
A lot of MIBs define specific variables to code the value of some OID. As an example the snmp agent returns a numerical value for the ifAdminStatus and ifOperStatus: 1 means Up and 2 means Down.
Because of the fact that OpenNMS has not a MibParser we usually put this map (between numerical encoded value and their meaning) into the event Description.
Configuring the Event properly now are able to decode the numerical value sent into trap varbinds to the corresponding string value into the &lt;logmsg>.
Let consider a Cisco HSRP status changes trap ( OID .1.3.6.1.4.1.9.9.106.2 generic 6 and specific 1), this trap correspond to uei.opennms.org/vendor/Cisco/traps/cHsrpStateChange event.
The trap contains the following varbind: cHsrpGrpStandbyState whose possible values are from 1 to 6 and whose meaning is:
initial(1) learn(2) listen(3) speak(4) standby(5) active(6).
I want to display in logmsg the literal meaning of the HSRP status.
Here is the original event definition:
&lt;event>
 &lt;mask>
  &lt;maskelement>
   &lt;mename>id&lt;/mename>
   &lt;mevalue>.1.3.6.1.4.1.9.9.106.2&lt;/mevalue>
  &lt;/maskelement>
  &lt;maskelement>
   &lt;mename>generic&lt;/mename>
   &lt;mevalue>6&lt;/mevalue>
  &lt;/maskelement>
  &lt;maskelement>
   &lt;mename>specific&lt;/mename>
   &lt;mevalue>1&lt;/mevalue>
  &lt;/maskelement>
 &lt;/mask>
 &lt;uei>uei.opennms.org/vendor/Cisco/traps/cHsrpStateChange&lt;/uei>
 &lt;event-label>CISCO-HSRP-MIB defined trap event: cHsrpStateChange&lt;/event-label>
 &lt;descr>&lt;p>A cHsrpStateChange notification is sent when a
 cHsrpGrpStandbyState transitions to either active or
 standby state, or leaves active or standby state. There
 will be only one notification issued when the state change
 is from standby to active and vice versa.&lt;/p>&lt;table>
 &lt;tr>&lt;td>&lt;b>
 cHsrpGrpStandbyState&lt;/b>&lt;/td>&lt;td>%parm[#1]%
 &lt;/td>&lt;td>&lt;p;>
 initial(1) learn(2) listen(3) speak(4) standby(5) active(6)&lt;/p>
 &lt;/td;>&lt;/tr>&lt;/table>
 &lt;/descr>
 &lt;logmsg dest='logndisplay'>&lt;p>Cisco Event: HSRP State Change.&lt;/p>&lt;/logmsg>
 &lt;severity>Minor&lt;/severity>
 &lt;/event>
An here follows the new event definition in which the status is decoded in the logmsg:
&lt;event>
 &lt;mask>
  &lt;maskelement>
   &lt;mename>id&lt;/mename>
   &lt;mevalue>.1.3.6.1.4.1.9.9.106.2&lt;/mevalue>
  &lt;/maskelement>
  &lt;maskelement>
   &lt;mename>generic&lt;/mename>
   &lt;mevalue>6&lt;/mevalue>
  &lt;/maskelement>
  &lt;maskelement>
   &lt;mename>specific&lt;/mename>
   &lt;mevalue>1&lt;/mevalue>
  &lt;/maskelement>
 &lt;/mask>
 &lt;uei>uei.opennms.org/vendor/Cisco/traps/cHsrpStateChange&lt;/uei>
 &lt;event-label>CISCO-HSRP-MIB defined trap event: cHsrpStateChange&lt;/event-label>
 &lt;descr>&lt;p>A cHsrpStateChange notification is sent when a
 cHsrpGrpStandbyState transitions to either active or
 standby state, or leaves active or standby state. There
 will be only one notification issued when the state change
 is from standby to active and vice versa.&lt;/p>&lt;table>
 &lt;tr>&lt;td>&lt;b>
 cHsrpGrpStandbyState&lt;/b>&lt;/td>&lt;td>%parm[#1]%
 &lt;/td>&lt;td>&lt;p;>
 initial(1) learn(2) listen(3) speak(4) standby(5) active(6)&lt;/p>
 &lt;/td;>&lt;/tr>&lt;/table>
 &lt;/descr>
 &lt;logmsg dest='logndisplay'>&lt;p>Cisco Event: HSRP State Change to %parm[#1]%.&lt;/p>&lt;/logmsg>
 &lt;severity>Minor&lt;/severity>
 &lt;varbindsdecode>
 &lt;parmid>parm[#1]&lt;/parmid>
 &lt;decode varbindvalue="1" varbinddecodedstring="initial"/>
 &lt;decode varbindvalue="2" varbinddecodedstring="learn"/>
 &lt;decode varbindvalue="3" varbinddecodedstring="listen"/>
 &lt;decode varbindvalue="4" varbinddecodedstring="speak"/>
 &lt;decode varbindvalue="5" varbinddecodedstring="standby"/>
 &lt;decode varbindvalue="6" varbinddecodedstring="active"/>
 &lt;/varbindsdecode>
&lt;/event>
Here the parm[#1] (So the first varbind into the trap is translated using the decode map. If the value of the first OID in this trap is 6 the the log message will be:
&lt;p>Cisco Event: HSRP State Change to active.&lt;/p>
[edit] Creating event definitions from trap definition in mibs
There is a program named mib2opennms available to convert trap definitions from mib files to opennms event definitions, see Converting MIBs Using mib2opennms
[edit] The eventconf.xml File
As mentioned above, the eventconf.xml file controls the definition of both internal and external events in OpenNMS. The order in which the events are listed is very important as it is possible to have numerous event definitions for a given event if you start to filter on variable bindings.
All this has caused the file to grow very large. In 1.0.2, eventconf.xml was over 3.1 MB of text. In order to make this file easier to manage, a new tag was introduced in 1.1.0 called &lt;event-file>. This allows files to be "included" as part of eventconf.xml. The new file now looks like:
&lt;event-file>/opt/OpenNMS/etc/events/3Com.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/APC.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Brocade.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/CIM.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Cisco.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Fore.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/HP.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Intel.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Microsoft.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Novell.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Oracle.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/SonicWall.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Xerox.events.xml&lt;/event-file>
&lt;event-file>/opt/OpenNMS/etc/events/Standard.events.xml&lt;/event-file>
The included files must start with a &lt;events> tag and end with an &lt;/events> tag. In between will be &lt;event> definitions just like in the original eventconf.xml file.
All of the events have been broken out by vendor. When OpenNMS starts, each file will be loaded in order, so again the order in which the files are listed is important. There are also still some events in eventconf.xml. These will be loaded before any included files, and best practice states to only list the internal OpenNMS events in that file directly.
At the very bottom of the file is &lt;event-file>/opt/OpenNMS/etc/events/default.events.xml&lt;/event-file>. This contains the generic default events and should always be listed last.
A few tips:
	•	Since the system has to scan through all of the events to find a match, it is best to remove event files you are not using.
	•	If you customize a file, such as Cisco events, you may want to simply copy it to "my.Cisco.events.xml" and list your file first. This way you will have less work should the default file be changed in a future release.
[edit] Sample: Discard rtc login events
To avoid getting the rtc login events displayed and persisted in the database add the following definition in eventconf.xml above the existing event for uei uei.opennms.org/internal/authentication/successfulLogin:
  &lt;event>
    &lt;mask>
      &lt;maskelement>
        &lt;mename>uei&lt;/mename>
        &lt;mevalue>uei.opennms.org/internal/authentication/successfulLogin&lt;/mevalue>
      &lt;/maskelement>
      &lt;varbind>
        &lt;vbnumber>1&lt;/vbnumber>
        &lt;vbvalue>rtc&lt;/vbvalue>
      &lt;/varbind>
    &lt;/mask>
    &lt;uei>uei.opennms.org/internal/authentication/successfulLogin&lt;/uei>
    &lt;event-label>OpenNMS-defined internal event: a user has successfully authentication to the WebUI&lt;/event-label>
    &lt;descr>
      This event is sent by the WebUI when user rtc has successfully authenticated
    &lt;/descr>
    &lt;logmsg dest='donotpersist'>
      OpenNMS user %parm[user]% has logged in from %parm[ip]%.
    &lt;/logmsg>
    &lt;severity>Normal&lt;/severity>
  &lt;/event>
[edit] Activate changes in event configurations
After changing the event configurations use $OPENNMS_HOME/bin/send-event.pl uei.opennms.org/internal/eventsConfigChange to inform opennms that the event configuration has been changed and needs to be reloaded. See also Configuration Files for more details about reloading changes.
[edit] Test event configuration
There are two very useful tools to test event configurations:  $OPENNMS_HOME/bin/send-event.pl and  $OPENNMS_HOME/bin/send-trap.pl Just start the scripts without parameters to get an explanation how to use them
[edit] The Database
Each event that occurs in OpenNMS is written to the database in the events table. To see them, simply access the database with "psql -U opennms opennms" and then browse the events with "SELECT * FROM events;".
One great troubleshooting tool is to look at the eventparms that get sent with the event. For example:
SELECT eventparms FROM events WHERE eventid=204;

                                                        eventparms
---------------------------------------------------------------------------------------------------------------------------
 ds=http(string,text);value=11.75(string,text);threshold=100.0(string,text);trigger=3(string,text);rearm=50.0(string,text)
(1 row)
Here are all of the parameters sent during a highThresholdRearmed event, and they can be used in event filters if needed.
Finally, it is worth noting that the database can get very full, and it may be necessary to delete events from the events table that are no longer needed. If you know SQL, this is pretty simple, but since OpenNMS events are sometimes referenced in the outages table and in notifications, you may not want to delete those (the outage table is required for availability calculation). Here is a sample SQL to delete non-referenced events:
DELETE FROM events WHERE eventid NOT IN (SELECT
svclosteventid FROM
outages) AND eventid NOT IN (SELECT
svcregainedeventid FROM outages);
[edit] Debugging
Edit $OPENNMS_HOME/etc/log4j.properties and set the logging level for the eventd to DEBUG. Then check the &lt;/code>eventd.log&lt;/code> and you will see the parameters and their values for each incoming event like the following lines (date and time cut off for readability):
353 DEBUG [Event TCP Receiver[5817][127.0.0.1:14115]] TcpStreamHandler: Event record converted
353 DEBUG [Event TCP Receiver[5817][127.0.0.1:14115]] TcpStreamHandler: handling event, uei = uei.opennms.org/internal/rtc/subscribe
404 DEBUG [Event TCP Server[5817]] TcpServer: New connection accepted from 127.0.0.1:14117
404 DEBUG [Event TCP Receiver[5817][127.0.0.1:14115]] RunnableConsumerThreadPool$SizingFifoQueue: adjust: started fiber EventHandlerPool-fiber0 ratio = 1.0, alive = 0
404 DEBUG [Event TCP Receiver[5817][127.0.0.1:14115]] TcpStreamHandler: stopping record handler
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl: Event {
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:   uuid  = &lt;not-set>
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:   uei   = uei.opennms.org/internal/rtc/subscribe
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:   src   = RTCPostSubscriber
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:   iface = null
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:   time  = Monday, February 22, 2010 4:26:29 PM GMT
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:   parms {
404 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:     (url, http://localhost:8980/opennms/rtc/post/Infrastructure+CentralSide)
405 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:     (user, rtc)
405 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:     (passwd, rtc)
405 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:     (catlabel, Infrastructure CentralSide)
405 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl:   }
405 DEBUG [EventHandlerPool-fiber0] DefaultEventHandlerImpl: }
405 DEBUG [EventHandlerPool-fiber0] EventConfData: Match found using key: EventKey
[edit] Conclusion
The Event management system is pretty straightforward once it is understood. While time consuming to set up, once configured it is pretty automatic.
It is hoped that this How-To has proved useful. Please direct corrections and comments to the author.
</Text>
        </Document>
        <Document ID="33">
            <Title>Standard Reports</Title>
        </Document>
        <Document ID="29">
            <Title>Trap Handling</Title>
        </Document>
        <Document ID="34">
            <Title>Extending Reports</Title>
        </Document>
        <Document ID="101">
            <Title>Glossary</Title>
            <Text>Critical (Severity)
This event means numerous devices on the network are affected by the event. Everyone who can should stop what they are doing and focus on fixing the problem.
Major (Severity)
A device is completely down or in danger of going down. Attention needs to be paid to this problem immediately.
Minor (Severity)
A part of a device (a service, and interface, a power supply, etc.) has stopped functioning. The device needs attention.
Warning (Severity)
An event has occurred that may require action. This severity can also be used to indicate a condition that should be noted (logged) but does not require direct action.
Normal (Severity)
Informational message. No action required.
Cleared (Severity)
This event indicates that a prior error condition has been corrected and service is restored
Indeterminate (Severity)
No Severity could be associated with this event.
</Text>
        </Document>
        <Document ID="35">
            <Title>Adding new Services</Title>
        </Document>
        <Document ID="102">
            <Title>Event Reduction</Title>
            <Text>OpenNMS Event Reduction Keys
The &lt;reductionKey> tag is added to Events defined in the event-configuration.xml file in order to:
	1.	Identify the Event as an Alarm
	2.	Used to reduce (de-duplicate) the Event in the Alarms table.
The reductionKey is also a column in the Alarms table where it is persisted (stored). This column has a unique index constraint that prevents duplication of alarms and speeds up reduction searches.
The granularity of the reductionKey determines the amount of reduction (see examples below). If a reductionKey is configured to only contain the node ID associated with the Event, then every Event for that node will be reduced. Going the other direction, if the Event time is added to the reductionKey, then only Events with the same nodedid that occur at exact same time (in milliseconds) will be reduced.
Example 1 - little granularity:
&lt;reductionKey>%nodeid%&lt;/reductionKey>
&lt;alarm-data reduction-key="%nodeid%" alarm-type="1" />
Example 2 - much granularity:
&lt;reductionKey>%nodeid%:%time%&lt;/reductionKey>
&lt;alarm-data reduction-key="%nodeid%:%time%" alarm-type="1" />
Example 3 - clearing alarm (see Automations):
&lt;alarm-data reduction-key="%nodeid%" alarm-type="2" clear-uei="uei.opennms.org/nodes/nodeDown" />
The reductionKey value can also be a literal (in example 2 the literal is ':') combined with any one of the event parameters.


Retrieved from "http://www.opennms.org/wiki/Reduction_key"

</Text>
        </Document>
        <Document ID="40">
            <Title>OpenNMS Base</Title>
        </Document>
        <Document ID="36">
            <Title>Adding new Devices</Title>
        </Document>
        <Document ID="41">
            <Title>SNMP Poller</Title>
        </Document>
        <Document ID="37">
            <Title>Asset Management</Title>
        </Document>
        <Document ID="103">
            <Title>Event Substitution</Title>
            <Text>Event substitution parameters are typically (but not limited to) used for SNMP traps that get converted into OpenNMS events and text in notifications. They provide a way to seeing information related to a particular instance of an event.
Trapd converts the varbinds in a trap to event parms when converting a trap to an event.
The list of elements in the eventconf.xml that can have a %element% or %parms[*]% in their value are:
	•	descr
	•	logmsg
	•	operinstruct
	•	autoaction
	•	operaction(/menu)
	•	tticket
	•	alarm-data
The list of elements that can occur as a NaV are :
	•	uei
	•	source
	•	nodeid
	•	time
	•	shorttime (shorter format time stamp)
	•	host
	•	interface
	•	snmphost
	•	service
	•	snmp
	•	id
	•	idtext
	•	version
	•	specific
	•	generic
	•	community
	•	severity
	•	operinstruct
	•	mouseovertext
	•	parm[values-all]
	•	parm[names-all]
	•	parm[name-#] from release 1.6.3
	•	parm[all]
	•	parm[]
	•	parm[##]
	•	parm[#]
Expansions are made so that:
	•	 %element% is replaced by the value of the element
	•	i.e a 'xxx %uei%' would expand to 'xxx &lt;eventuei>'
	•	 %parm[values-all]% is replaced by a space-separated list of all parm values
	•	i.e a 'xxx %parm[values-all]%' would expand to 'xxx parmVal1 parmVal2 ..'
	•	 %parm[names-all]% is replaced by a space-separated list of all parm names
	•	i.e a 'xxx %parm[names-all]%' would expand to 'xxx parmName1 parmName2 ..'
	•	 %parm[name-#&lt;num>]% is replaced by the name of parameter number 'num', if present
	◦	This expansion is available from release 1.6.3
	◦	 %parm[name-#&lt;num>&lt;sep>&lt;token-num>]% is replaced by token number 'token-num' after tokenizing (splitting) the name of parameter number 'num' using the string 'sep' as the separator character
	▪	If 'token-num' is positive, it is interpreted as a one-based index from the beginning of the list of tokens
	▪	Example: for parameter 1 with name .1.3.6.1.4.1.5813.20.1.1 (enterprises.5813.20.1.1), you could extract the enterprise ID using %parm[name-#1.7]%
	▪	If 'token-num' is negative, it is interpreted as a one-based index from the end of the list of tokens
	▪	This is useful when dealing with SNMP traps whose varbind OIDs (which become parameter names) include an instance identifier
	▪	Example: for parameter 1 with name .1.3.6.1.2.1.2.2.1.8.10542 (ifOperStatus.10542), you could extract the ifIndex using %parm[name-#1.-1]%
	◦	 %parm[name-#&lt;num>&lt;sep>&lt;token-num>:&lt;count>]% is replaced by a run of 'count' tokens, starting at the specified 'token-num', with 'sep' reinserted between each pair of tokens
	▪	Again, 'token-num' may be positive or negative
	▪	If 'count' is omitted, tokens will be extracted until the end of the list of tokens is reached
	▪	Negative values of 'count' are allowed but the resulting behavior is undefined
	▪	Example: for parameter 1 with name .1.3.6.1.2.1.15.3.1.1.192.168.254.129 (bgpPeerAddress.192.168.254.129), you could extract the peer IP address using %parm[name-#1.-4:
	•	 %parm[all]% is replaced by a space-separated list of all parmName="parmValue"
	◦	i.e a 'xxx %parm[all]%' would expand to 'xxx parmName1="parmVal1" parmName2="parmVal2" ..'
	•	 %parm[&lt;name>]% is replaced by the value of the parameter named 'name', if present
	•	 %parm[#&lt;num>]% is replaced by the value of the parameter number 'num', if present
Note: If a '%&lt;xx>%' does not have a value to be expanded to, it will not be part of the 'expanded' string
With OpenNMS 1.1, there is also:
nodelabel 
returns the nodelabel for the nodeid if present
interfaceresolve 
returns the DNS name of the IP address in the interface field, if present.
With OpenNMS 1.2, ifalias was added:
ifalias 
returns the value of the ifAlias for the snmp interface, if present. Looks up ifAlias by IP address, so do does not retrieve the ifAlias for non-IP interfaces.
With OpenNMS 1.5.92, %asset[fieldname]% was added. Replace "fieldname" with any valid field in the assets table and the value from that table for the particular nodeid referenced in the event will be returned. If the fieldname is invalid or there is no data, "Unknown" will be returned.
</Text>
        </Document>
        <Document ID="42">
            <Title>DHCPD</Title>
        </Document>
        <Document ID="38">
            <Title>Automations</Title>
        </Document>
        <Document ID="43">
            <Title>Ticketing Integration</Title>
        </Document>
        <Document ID="39">
            <Title>ScriptD</Title>
        </Document>
        <Document ID="104">
            <Title>Event Params</Title>
            <Text>Event parameters are used in the event-configuration.xml and notifications.xml files. The parameters are parsed as tokens delimitted with percent ('%') signs. This is the currrent list of valid paramenters:
Someone should better define these ;-)
You can edit $OPENNMS_HOME/etc/log4j.properties and set the logging level for the eventd to DEBUG. Then check the eventd.log and you will see the parameters and their values for each incoming event.

 %eventid% 
The Event ID xml tag
 %uei% 
The UEI xml tag
 %source% 
The event source xml tag
 %time% 
The event time
 %dpname% 
The event dpname
 %nodeid% 
The event nodeid
 %nodelabel% 
The nodelabel
 %host% 
The host
 %interface% 
The interface
 %interfaceresolve% 
Reverse DNS lookup of the interface
 %ifalias% 
SNMP ifAlias
 %id% 
SNMP ID
 %snmp% 
SNMP
 %idtext% 
SNMP ID Text
 %version% 
SNMP version
 %specific% 
SNMP specific ID
 %generic% 
SNMP generic ID
 %community% 
SNMP community string
 %snmphost% 
SNMP host
 %service% 
OpenNMS service
 %severity% 
OpenNMS severity
 %operinstruct% 
Event defined operator instructions
 %mouseovertext% 
Event defined mouse over text
</Text>
        </Document>
        <Document ID="3">
            <Title>Introduction</Title>
            <Synopsis>General introduction</Synopsis>
            <Text>OpenNMS stands for “Open Network Management System”. It’s “Open” both because the Software itself is licensed as “Open Source” under the GNU Public License. 
A Network as understood by OpenNMS is a number of somehow reachable network elements, called nodes.
It’s fair to put in the first bracket at this point already: OpenNMS can in fact manage more than nodes. You can, for example, manage applications. Or whole networks. But as the starting point for the software was the management of network elements, it’s called “Network Management System” (rather than “anything you can somehow reach and get to tell something about itself Management System (aycsragttsaiMS)).
When it comes to “Management” we have to clarify straight from the beginning that management is done by people with brains. The role of OpenNMS is to provide information about managed nodes. It does so by receiving information from nodes and by reaching out to them to collect information. Once the information is available to OpenNMS it can apply rules and visualize this information. 
OpenNMS is organized around “events”. You will encounter these events as Notifications and Alarms in the WebUI (and if you are one of the lucky administrators, by means of personal communication). Besides those visible events, OpenNMS uses events to organize itself: Everything is an event. The different components of OpenNMS, the Daemons, are connected to an event bus. They use this bus to communicate between each other. You can listen in to their conversations if you look into the log of the EventD, the Event Daemon.
All these elements together are “the system”, which is why we call OpenNMS OpenNM-System and not OpenNM-Application.
This book is intended both for Users and Administrators of OpenNMS. Users will find everything they need to know to work with the Web User Interface in the first part of the book. 
</Text>
        </Document>
        <Document ID="44">
            <Title>Passive Status Keeper</Title>
        </Document>
        <Document ID="4">
            <Title>User</Title>
            <Synopsis>Information for guys in a NOC</Synopsis>
        </Document>
        <Document ID="45">
            <Title>Alarm Management</Title>
        </Document>
        <Document ID="5">
            <Title>Administrator</Title>
        </Document>
        <Document ID="105">
            <Title>Event Translator</Title>
            <Text>The Event Translator (ET) is an enterprise integration feature. The feature provides the network management professional with the ability to transform the and enhance the data contained within OpenNMS events that are being generated by the other daemons (monitoring processes). When an event is published on the event bus for which ET has configuration, it clones the event and changes the event's attributes (fields and parameters) as defined by the administrator in the translator configuration. These transformations can come from the result of SQL queries, regular expression matches, and string literals that are then assigned to attributes of the new event. It is an enterprise grade feature in that it allows for better integration with external systems based on data from sources internal and external to OpenNMS.
Contents
[hide]
	•	1 The Event Translator Service
	◦	1.1 Example 1
	◦	1.2 Example 2
	◦	1.3 Example 3
	•	2 Version History/Availability
[edit] The Event Translator Service
As with most OpenNMS daemons, the EventTranslator has a companion configuration "factory" class that marshals its configuration from an XML formated file. The file, translator-configuration.xml file contains elements and attributes that create/change the fields or parameters of one event and publishes a new event with these changes. This feature has been used extensively for the following two reasons:
	1.	To create passive status events (PSE) (see Passive Status Keeper)
	2.	To associate an event with a different node.
[edit] Example 1
Here is an example for associating an event with a different node. The typical case for this would be an SNMP manager that proxies informational messages from non-SNMP entities to OpenNMS via an SNMP Trap. This allows the user to associate the Traps with a node other than the SNMP manager (typically a passive node) making correlation much easier.
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;event-translator-configuration
xmlns="http://xmlns.opennms.org/xsd/translator-configuration"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" >
  &lt;translation>
    &lt;event-translation-spec uei="uei.opennms.org/mib2opennms/tspEventPCRRepetitionError">
      &lt;!-- Each event can have 1 or more mappings. -->
      &lt;mappings>
        &lt;mapping>
          &lt;!-- This mapping uses the SQL value type to query the DB and change the nodeid of the new event -->
          &lt;assignment type="field" name="nodeid">
            &lt;value type="sql" result="select node.nodeid from node, ipInterface where node.nodeLabel=? and ipinterface.ipaddr=? and node.nodeId=ipinterface.nodeid and ipInterface.isManaged != 'D' and node.nodeType != 'D'" >
              &lt;!-- These are sub value types that are used as parameters to the above sql as in JDBC speak -->
              &lt;value type="parameter" name=".1.3.6.1.4.1.6768.6.2.2.5.0" matches="^([A-z]+) ([0-9]+).*" result="${1}-${2}" />
              &lt;value type="constant" result="169.254.1.1" />
            &lt;/value>
          &lt;/assignment>
        &lt;/mapping>
      &lt;/mappings>
    &lt;/event-translation-spec>
  &lt;/translation>
&lt;/event-translator-configuration>
In this example, the nodeid will be replaced with the resulting nodeid from the SQL query when an event with the uei "uei.opennms.org/mib2opennms/tspEventPCRRepetitionError" is received. The ?'s in the SQL query are populated with the values obtained from the nested value elements. The first of the nested value elements creates a result string from a regular expression match of the event parameter named ".1.3.6.1.4.1.6768.6.2.2.5.0". For this example, we will use the string "DEMO 10 example server" to represent the value of the parameter. The resulting match would produce "DEMO-10". The second nested value is a constant and simply returns the value "169.254.1.1". These are assembled into the SQL query.
SELECT node.nodeid
FROM node, ipInterface
WHERE node.nodeLabel='DEMO-10'
    AND ipinterface.ipaddr='169.254.1.1'
    AND node.nodeId=ipinterface.nodeid
    AND ipInterface.isManaged != 'D'
    AND node.nodeType != 'D'
The query is then executed. The value returned is then subsitituted for the existing nodeid into a new event cloned from the original event.
[edit] Example 2
Here is an example for an Adtran TA4303. I had a bunch of varbinds that I needed to lookup so I made a SQL table 'etlookup'. In that table I would lookup the variable name and the input value and an output value. The specific value that this node uses is different for each of the different problem's that each interface can have (AIS, Yellow alarm, LOS, etc). All of the problems that have a state that I can track are created as services (the T1's tend to flap). Since each of the 80 T1 ports on this box do not have IP addresses I ended up making a new node. When I tried to add those to the existing node it kept disabling all of the services that I added (anywho). To get the IP address I made another SQL table 'ta4303' and to do query with the port &amp; slot to get the description. I store that text value in a new variable 'portdesc'. I use that variable in the event and later the notification '%parm[portdesc]%'.
&lt;event-translation-spec uei="uei.opennms.org/vendor/adtran/traps/adta4303">
&lt;mappings>
	&lt;mapping>
	        &lt;assignment type="field" name="passiveNodeLabel">
			&lt;value type="constant" result="Switch - TA4303" />
		&lt;/assignment>
	        &lt;assignment type="field" name="nodeid">
			&lt;value type="constant" result="307" />
		&lt;/assignment>
		&lt;assignment type="parameter" name="passiveIpAddr">
			&lt;value type="sql" result="select ta4303.ip from ta4303 where ta4303.slot=? and ta4303.port=? " >
				&lt;value type="parameter" name=".1.3.6.1.4.1.664.5.13.2.3.1.1" matches="^([0-9]+)" result="${1}" />
				&lt;value type="parameter" name=".1.3.6.1.4.1.664.5.13.3.3.1.1" matches="^([0-9]+)" result="${1}" />
			&lt;/value>
		&lt;/assignment>
		&lt;assignment type="parameter" name="portdesc">
			&lt;value type="sql" result="select ta4303.description from ta4303 where ta4303.slot=? and ta4303.port=? " >
				&lt;value type="parameter" name=".1.3.6.1.4.1.664.5.13.2.3.1.1" matches="^([0-9]+)" result="${1}" />
				&lt;value type="parameter" name=".1.3.6.1.4.1.664.5.13.3.3.1.1" matches="^([0-9]+)" result="${1}" />
			&lt;/value>
		&lt;/assignment>
		&lt;assignment type="parameter" name="passiveServiceName">
			&lt;value type="sql" result="select etlookup.output from etlookup where etlookup.variable='ta4303service' and etlookup.input=? " >
				&lt;value type="parameter" name="specifc" matches="^([0-9]+)" result="${1}" />
			&lt;/value>
		&lt;/assignment>
        	&lt;assignment type="parameter" name="passiveStatus" >
			&lt;value type="sql" result="select etlookup.output from etlookup where etlookup.variable='ta4303status' and etlookup.input=? " >
				&lt;value type="parameter" name="specifc" matches="^([0-9]+)" result="${1}" />
			&lt;/value>
		&lt;/assignment>
		&lt;assignment type="field" name="uei">
            		&lt;value type="constant" result="uei.opennms.org/services/passiveServiceStatus" />
		&lt;/assignment>
	&lt;/mapping>
&lt;/mappings>
&lt;/event-translation-spec>
[edit] Example 3
Say the SQL statement you want to read data with returns multiple rows. The Event translator will only give you one result row a the new parameter. A workaround is to create an aggregate function in psql that accumulates the output to one single row. A use-case was to include (probably multiple) categories' names into a nodeCategoryMembershipChanged event.
create SQL aggregate

CREATE AGGREGATE array_accum (anyelement)
(
    sfunc = array_append,
    stype = anyarray,
    initcond = '{}'
);
translator config

&lt;event-translation-spec uei="uei.opennms.org/nodes/nodeCategoryMembershipChanged">
	&lt;mappings>
		&lt;mapping>
			&lt;assignment name="uei" type="field" >
				&lt;value type="constant" result="uei.opennms.org/test/nodeCategoryMembershipChanged" />
			&lt;/assignment>
			&lt;assignment name="categories" type="parameter">
				&lt;value type="sql" result="select array_accum(categoryname) from categories,category_node,node where node.nodeid=?::integer and node.nodeid=category_node.nodeid and category_node.categoryid=categories.categoryid;" >
					&lt;value type="field" name="nodeid" matches=".*" result="${0}" />
				&lt;/value>
			&lt;/assignment>
		&lt;/mapping>
	&lt;/mappings>
&lt;/event-translation-spec>
[edit] Version History/Availability
	•	This feature was added in version 1.3.2
	•	This feature was enhanced or modified in version 1.6.1


Retrieved from "http://www.opennms.org/wiki/Event_Translator"

</Text>
        </Document>
        <Document ID="6">
            <Title>Accessing the WebUi</Title>
            <Text>Accessing the WebUI</Text>
        </Document>
        <Document ID="46">
            <Title>Asterisk Gateway</Title>
        </Document>
        <Document ID="50">
            <Title>Data Analysis</Title>
        </Document>
        <Document ID="7">
            <Title>Howto</Title>
            <Text>Technical Information

The OpenNMS Book in English is located in GIT:
ssh://$USER@opennms.git.sourceforge.net/gitroot/opennms/opennmsbook
If you clone this directory you will find 

OpenNMS_Book_English.scriv

Which contains the book. The directory structure underneath is managed by a tool called Scrivener. 
You do not need Scrivener to help writing the book! All the text content is located in
./OpenNMS_Book_English.scriv/Files/Docs
Each Chapter is stored as an individual .rtf in this folder.
The text format is RTF, which means that you can edit text in any RTF-capable editor.
If you have text in another format, or if you have text of which you don’t know where to put, please save it in the ./inbound/ folder.
There will be a recent pdf as well for your convenience (which shows the current status of the book).

Writing Style

There is a style guide written by the BBC in the git repository. It is written for news and not for a manual, but: read through it for inspiration. A lot of the statements true for news are true for this book as well. We do not aim to create an academical work, but a readable, useful book which becomes a permanent reference for the reader.
This means, above all: Short sentences. 
Another consideration is the way in which we address the reader. “How are we doing today” is what the doctor says when we visit (mine does not, but that’s another story). Avoid the “we do this” and then “we do that” - it’s not us, it’s either the writer or the reader. Texts get more lively when it’s active.

</Text>
        </Document>
        <Document ID="8">
            <Title>Introduction</Title>
        </Document>
        <Document ID="47">
            <Title>Provisioning</Title>
        </Document>
        <Document ID="110">
            <Title>Data Collection HT</Title>
            <Text>Introduction
[edit] Purpose
This How-To is one in a series designed to serve as a reference for getting started with OpenNMS. Eventually, these documents will cover everything necessary to get OpenNMS installed and running in your environment.
[edit] Copyright
Content is available under a Creative Commons Attribution-NonCommercial-ShareAlike2.5 License.
[edit] Overview
OpenNMS is an enterprise-grade network management platform developed under the open-source model. Unlike traditional network management products which are very focused on network elements such as interfaces on switches and routers, OpenNMS focuses on the services network resources provide: web pages, database access, DNS, DHCP, etc. (although information on network elements is also available).
There are two major ways that OpenNMS gathers data about the network. The first is through polling. Processes called monitors connect to a network resource and perform a simple test to see if the resource is responding correctly. If not, events are generated. The second is through data collection using collectors. Currently data can be collected by :
	•	SNMP,
	•	NSClient (the Nagios Agent),
	•	JMX,
	•	HTTP
Getting data collection configured properly seems to be one of the more difficult tasks in OpenNMS, but it's just a matter of "getting all your ducks in a row". There are several things that have to happen in order for this to work. For all data collection methods:
capsd 
During the discovery process, capsd discovers whether the various collectable services exist on the discovered node. More specifically for SNMP collection, capd must be able to access SNMP information on that interface and to form some basic relationships, such as IP Address to ifIndex.
collectd-configuration.xml 
Just as in the poller-configuration.xml file (covered elsewhere), interfaces are mapped to packages for collection in this file. If data collection is required on an interface, it needs to exist in a package in this file. The default configuration is suitable for most initial purposes.
[edit] SNMP
For SNMP data collection, the following files must be configured correctly:
snmp-config.xml 
For each interface, a valid community string must exist in this file.
datacollection-config.xml 
Each package in the collectd configuration file points to an snmp-collection definition in this file. Each snmp-collection defines what information to collect via SNMP, and it is pretty powerful as far as configuration goes. The default configuration is fairly complete for basic purposes, and will probably not require much changing initially.
[edit] NSClient
For NSClient data collection, you need to install the NSClient agent on the Windows servers (http://nsclient.ready2run.nl/), configure it with a port/password, and then configure OpenNMS:
nsclient-config.xml 
This is where you configure passwords, timeouts and ports to connect on. Each interface you want to collect on must have a valid password specified in this file (although you can specify a default set of parameters to simplify configuration).
nsclient-datacollection-config.xml 
This file configures named sets of collections which correspond to names specified in the configuration of collectd. These collection sets define which Windows Perfmon counters to collect, and how to identify which servers they should be collected from.
[edit] JMX
For JMX data collection, the following file must be configured:
jmx-datacollection-config.xml 
As for the other datacollection-config files, this file specifies which data points should be collected. In this case, it's MBeans, and which beans/attributes should be collected. Again, these are grouped by named set which corresponds to the names used in packages in collectd.
[edit] HTTP
For HTTP data collection, configure:
http-datacollection-config.xml 
In this config file you specify URLS and the regular expressions to use to extract the data points from the returned pages. Again, collections are grouped by names, corresponding to the names used in packages in collected
The best part about data collection is that if everything goes smoothly, it is completely automated. Particularly, the out of the box configuration requires relatively little customisation (usually just providing SNMP community strings or NSClient passwords) to be usefully functional.
[edit] Data Collection
[edit] snmp-config.xml
The parameters used to connect with SNMP agents are defined in the snmp-config.xml file. Here is an example:
&lt;snmp-config retry="3" timeout="800" read-community="public" write-community="private">
     &lt;definition version="v2c">
          &lt;specific>192.168.0.5&lt;/specific>
     &lt;/definition>
     &lt;definition retry="4" timeout="2000">
          &lt;range begin="192.168.1.1" end="192.168.1.254"/>
          &lt;range begin="192.168.3.1" end="192.168.3.254"/>
     &lt;/definition>
     &lt;definition read-community="bubba" write-community="zeke">
          &lt;range begin="192.168.2.1" end="192.168.2.254"/>
     &lt;/definition>
     &lt;definition port="1161">
          &lt;specific>192.168.5.50&lt;/specific>
     &lt;/definition>
&lt;/snmp-config>
The common attributes for the snmp-config tag are as follows:
retry 
The number of attempts that will be made to connect to the SNMP agent. Default is 1
timeout 
The amount of time, in milliseconds, that OpenNMS will wait for a response from the agent. Default is 3000
read-community 
The default "read" community string for SNMP queries. If not specified, defaults to "public"
write-community 
The default "write" community string for SNMP queries. Note that this is for future development - OpenNMS does not perform SNMP "sets" at the moment.
port 
This overrides the default port of 161.
version 
Here you can force either SNMP version 1 by specifying "v1", version 2c with "v2c", or version 3 with "v3". Default is "v1"
For SNMPv3 authentication and collection (only available when using SNMP4J):
security-name 
A security name for SNMP v3 authentication
auth-passphrase 
The passphrase to use for SNMP v3 authentication
auth-protocol 
The authentication protocol for SNMP v3. Either "MD5" or "SHA". Default is MD5
privacy-passphrase 
A privacy pass phrase used to encrypt the contents of SNMP v3 packets
privacy-protocol 
The privacy protocol used to encrypt the contents of SNMP v3 packets. Either "DES", "AES","AES192" or "AES256". Default is DES.
engine-id 
The engine id of the target agent
context-name 
The name of the context to obtain data from on the target agent.
context-engine-id 
The context engine id of the target entity on the agent.
enterprise-id 
An enterprise id for SNMP v3 collection
More rarely used attributes in the snmp-config tag are:
proxy-host 
A proxy host to use to communicate with the specified agent(s)
max-vars-per-pdu 
Number of variables per SNMP request. Default is 10
max-request-size 
If using SNMP4J as the SNMP library, the maximum size of outgoing SNMP requests. Defaults to 65535, must be at least 484
All of the global parameters can be overridden with definition tags. These new SNMP definitions can apply to ranges or specific IP addresses.
Note that if an interface will qualify in multiple ranges in this file, the first one found will be used.
[edit] nsclient-config.xml
This is the NSClient equivalent of snmp-config.xml, where parameters for connecting to the NSClient agent are defined. An example of such a file is:
&lt;?xml version="1.0"?>
&lt;nsclient-config port="1248" retry="3" timeout="800" password="apassword">
&lt;/nsclient-config>
The parameters that can be configured
retry 
The number of attempts that will be made to connect to the NSClient agent. Default is 1
timeout 
The amount of time, in milliseconds, that OpenNMS will wait for a response from the agent. Default is 3000
port 
This overrides the default port of 1248.
password 
The password (if any) required to authenticate to the NSClient agent. Default is the string "None"
As with snmp-config.xml, all of the global parameters can be overridden with definition tags. These new definitions can apply to ranges or specific IP addresses. Note that if an interface is matched in multiple ranges in this file, the first one found will be used.
[edit] Capabilities
As explained in the Discovery How-To, the capabilities check process starts with a NewSuspect event (generated either manually or through the discovery process). This NewSuspect event is received by the capabilities daemon (capsd).
The capsd process is responsible for testing IP addresses for particular capabilities. Each protocol that can be monitored is defined in the capsd-configuration.xml file. Upon receipt of a NewSuspect event, capsd begins to test each configured protocol to see if it exists on that device.
When testing SNMP, capsd makes an attempt to receive the System Object ID (systemOID) for the device using the community string and port defined in snmp-config.xml. If this succeeds, the SNMP protocol is marked as "true" for this IP address.
The capsd process will first complete all tests for all protocols defined in the file for the interface in question, and after that is complete, if SNMP is "true" for this IP address, more tests are performed by capsd.
First, three threads are generated to collect the data from the system tree, the ipAddrTable and ifTable.
If, for some reason, the ipAddrTable or ifTable are unavailable, the process stops (but the SNMP system data may show up on the node page - this happens a lot with UC-Davis SNMP agents where only the system tree is available, by default, to a query using the "public" community string).
Second, all of the sub-target IP addresses in the ipAddrTable are run through the capsd capabilities scan. Note that this is regardless of how management is configured in the configuration file. This only happens on the initial scan and on forced rescans. On normal rescans (by default, every 24 hours), IP addresses that are "unmanaged" in capsd are not polled.
Third, every IP address in the ipAddrTable that supports SNMP is tested to see if it maps to a valid ifIndex in the ifTable. If this is true, the IP address is marked as a secondary SNMP interface and is a contender for becoming the primary SNMP interface.
Finally, all secondary SNMP interfaces are tested to see if they match a valid package in the collectd-configuration file. If more than one valid IP address meets all three criteria (supports SNMP, has a valid ifIndex and is included in a collection package), then the lowest IP address is marked as primary. All SNMP data collection is performed via the primary SNMP interface.
(Note: in the future we will have the ability to change to a secondary SNMP interface should the primary become unavailable).
When the capsd testing process is complete, events are generated, including NodeGainedService events.
[edit] collectd-configuration.xml
Data collection is handled via the collectd process. collectd listens for NodeGainedService events for the SNMP "service". When this happens, it checks to see if the primary SNMP interface for that node exists in a collection package (which it should by definition). If so, the SNMP collector is instantiated for that IP address.
Unless forced toward one version or another via the snmp-config.xml file, when the collection is initialized it will check to see if SNMPv2 is supported by attempting a GET-BULK on system.sysObjectID. If that fails it will revert to version 1.
This is a change from earlier versions of OpenNMS. capsd is no longer responsible for determining the SNMP version. Whether or not SNMPv2 is supported on a node will no longer show up on the node page.
Let's look at the collectd-configuration.xml file:
&lt;collectd-configuration
        threads="5">

        &lt;package name="example1">
                &lt;filter>IPADDR IPLIKE *.*.*.*&lt;/filter>
                &lt;specific>0.0.0.0&lt;/specific>
                &lt;include-range begin="192.168.0.1" end="192.168.0.254"/>
                &lt;include-url>file:/opt/OpenNMS/etc/include&lt;/include-url>

                &lt;service name="SNMP" interval="300000" user-defined="false" status="on">
                        &lt;parameter key="collection" value="default"/>
                        &lt;parameter key="port" value="161"/>
                        &lt;parameter key="retry" value="3"/>
                        &lt;parameter key="timeout" value="3000"/>
                &lt;/service>

                &lt;outage-calendar>zzz from poll-outages.xml zzz&lt;/outage-calendar>
        &lt;/package>

        &lt;collector service="SNMP"       class-name="org.opennms.netmgt.collectd.SnmpCollector"/>
&lt;/collectd-configuration>
If you are familiar with the poller configuration file, you can probably figure out what this file does.
The threads attribute limits the number of threads that will be used by the data collection process. You can increase or decrease this value based upon your network and the size of your server.
Just like pollers have poller packages, collectors have collection packages. Each package determines how often the device will be polled for SNMP data, and through the collection key, what will be polled and how it will be stored. The example1 package is the default included out of the box.
[edit] What Interfaces are Included in a Package?
The package name is followed by a list of tags that define what interfaces will be included in the package. All of the tags, except for filter, are optional and unbounded. There are five types of these tags:
filter 
Specify a filter that matches the interfaces to be included in the package.
&lt;filter>IPADDR IPLIKE *.*.*.*&lt;/filter>
Each package must have a filter tag that performs the initial test to see if an interface should be included in a package. Filters operate on interfaces (not nodes) and is discussed in depth in this How-To. Only one filter statement can exist per package.
specific  
Specify a specific IP address to include in the package.
&lt;specific>192.168.1.59&lt;/specific>
include-range 
This specifies a particular range of IP addresses to include in a package.
&lt;include-range begin="192.168.0.1" end="192.168.0.254"/>
exclude-range
This specifies a particular range of IP addresses to exclude in a package. This will override an include-range tag.
 &lt;exclude-range begin="192.168.0.100" end="192.168.0.104"/>
include-url 
Specify a file that contains a list of IP addresses to include.
&lt;include-url>file:/opt/OpenNMS/etc/include&lt;/include-url>
This tag will point to a file that consists of a list of IP addresses, one to a line, that will be included in the package. Comments can be embedded in this file. Any line that begins with a "#" character will be ignored, as will the remainder of any line that includes a space followed by "#".
[edit] Services
Again, drawing on the analogy with pollers, each poller package has a set of protocols that it monitors, collectors have a set of services on which they collect data. At the time there is only one: SNMP.
The service tag names the service and also specifies various parameters:
name 
This is the name of the service.
interval 
This specifies the polling interval (5 minutes by default).
user-defined 
In the future, users may be able to define new collection sources (like from a log file) through a GUI, but at the moment this is set to "false".
status 
Also in the future, there will be an admin GUI for collectors just as there is for pollers, and users will be able to turn SNMP data collection on or off from a web page. At the moment, this can only be done by editing this file and setting status to either "off" or "on" (default).
[edit] Service Parameters
There are three parameters available common to all services:
timeout 
The timeout, in milliseconds, to wait for a response to an SNMP request.
retries 
If a timeout does occur, this controls the number of attempts to make before giving up.
port 
This allows you to override the default port for SNMP data collection.
[edit] SNMP
In addition, the SNMP service can have the following parameters specified:
collection 
This points to an SNMP collection in the datacollection-config.xml file that determines what Object IDs (OIDs) will be collected.
oid 
The collector will test to see if the interface supports SNMPv2 by doing a GET-BULK request on this OID. By default it is set to the systemOID. If the GET-BULK is successful, then the rest of the polling for this device will take advantage of SNMPv2. Otherwise, SNMPv1 will be used. The intent was to allow for this parameter to override the systemOID value, but it was never implemented, so you can ignore this parameter for now.
[edit] JBOSS
OpenNMS comes with libraries for JBoss 4.0.2. If you need the JBossCollector in order to collect data from JBoss 4.2.2, these will cause a silent failure. In that case, delete ${OPENNMSHOME}/lib/jboss*4.0.2.jar and ${OPENNMSHOME}/lib/jnp-client-4.0.2.jar, and place your own jbossall-client.jar in ${OPENNMSHOME}/lib/jboss/jbossall-client.jar.
The JBOSS4 and JBoss32 services can have the following additional parameter specified:
factory 
Specifies the method of connecting to the JBOSS server. It can be either HTTP or RMI.
[edit] NSClient
The NSClient service has the following additional parameter:
nsclient-collection 
This points to a collection in the nsclient-datacollection-config.xml file that determines what perfmon counters will be collected.
[edit] Outage Calendar
In order to keep servers operating properly, it is often necessary to bring them down for scheduled maintenance. Instead of having these maintenance outages reflected as a true service outage, they can be included in an "Poller Outage Calendar" and then referenced by the poller package using the outage-calendar tag. This tag contains the name of a valid outage in the poll-outages.xml file.
The outage-calendar tag is optional and unbounded (i.e. you can reference more than one outage).
Since version 1.5.91 you can configure scheduled outages from the GUI, got to Admin -> Scheduled Outages.
Before version 1.5.91, there were three types of outages: weekly, monthly and specific. Since 1.5.91 there is also the possibility to configure daily outages.
If you have the problem nodes are reported to be down thought they are within a daily outage which goes past midnight try to define two timespans within the outage, one until midnight and the other one starting after midnight, e.g. instead of outage 22:00:00-01:00:00 define 22:00:00-23:59:59 and 00:00:00-01:00:00.
 Examples from the poll-outages file:
&lt;outage name="global" type="weekly">
  &lt;time day="sunday" begins="12:30:00" ends="12:45:00"/>
  &lt;time day="sunday" begins="13:30:00" ends="14:45:00"/>
  &lt;time day="monday" begins="13:30:00" ends="14:45:00"/>
  &lt;time day="tuesday" begins="13:00:00" ends="14:45:00"/>
  &lt;interface address="192.168.0.1"/>
  &lt;interface address="192.168.0.36"/>
  &lt;interface address="192.168.0.38"/>
&lt;/outage>
This defines an outage calendar called "global" that is run every week. It specifies four outage times: Sunday starting at 12:30 pm and lasting 15 minutes, Sunday starting at 1:30 pm and lasting an hour and fifteen minutes, the same outage on Monday, and one on Tuesday from 1:00 pm to 2:45 pm. This is to demonstrate that you can have multiple outages on a given day and the same outage on different days. Three interfaces will be affected.
&lt;outage name="hub maintenance" type="monthly">
  &lt;time day="1" begins="23:30:00" ends="23:45:00"/>
  &lt;time day="15" begins="21:30:00" ends="21:45:00"/>
  &lt;time day="15" begins="23:30:00" ends="23:45:00"/>
  &lt;interface address="192.168.100.254"/>
  &lt;interface address="192.168.101.254"/>
  &lt;interface address="192.168.102.254"/>
  &lt;interface address="192.168.103.254"/>
  &lt;interface address="192.168.104.254"/>
  &lt;interface address="192.168.105.254"/>
  &lt;interface address="192.168.106.254"/>
  &lt;interface address="192.168.107.254"/>
&lt;/outage>
This outage calendar is called "hub maintenance" that is run every month. On the first of the month the outage begins at 11:30 pm and lasts 15 minutes. The same outage occurs on the 15th of the month in addition to another outage from 9:30 pm to 9:45 pm. Thus you can have the same outage on different dates as well as more than one outage on a particular date. Eight interfaces are affected by this outage.
&lt;outage name="proxy server tuning" type="specific">
  &lt;time begins="10-Nov-2001 17:30:00" ends="11-Nov-2001 08:00:00"/>
  &lt;interface address="192.168.0.1"/>
&lt;/outage>
It is also possible to include an outage on a specific date and time. This outage named "proxy server tuning" began on November 10th, 2001 at 5:30 pm and lasted until 8:00 am the next day. This affected one interface. You can have more than one "time" entry per specific outage.
If a particular outage calendar is included in a collection package, then collection will not occur during this time.
[edit] Final Tags in collectd-configuration.xml
Just like in the poller configuration file, each service that is collected on must reference the class that is to be used for this collection. Therefore there should be one or more of the following definitions (or your own if you've implemented your own collector class)
&lt;collector service="SNMP"       class-name="org.opennms.netmgt.collectd.SnmpCollector"/>
&lt;collector service="NSClient"       class-name="org.opennms.netmgt.collectd.NSClientCollector"/>
&lt;collector service="JBoss4"       class-name="org.opennms.netmgt.collectd.JBossCollector"/>
&lt;collector service="JBoss32"      class-name="org.opennms.netmgt.collectd.JBossCollector"/>
&lt;collector service="JVM"          class-name="org.opennms.netmgt.collectd.Jsr160Collector"/>
&lt;collector service="HttpDocCount" class-name="org.opennms.netmgt.collectd.HttpCollector" />
[edit] Data Collection Configuration
[edit] datacollection-config.xml
This is one of the more complex files in the product. It determines what values will be collected upon for a given interface and package.
At this point in time, it is probably best to review the structure of things and expand upon them somewhat. Okay, just like there are poller packages for monitoring service levels, there are collection packages that control data collection. Poller packages can monitor numerous protocols, and collection packages can collect on numerous data sources, but for now the only one is SNMP.
The SNMP data collection service points to an SNMP data collection "scheme". I am out of synonyms for "package", and I don't want to get confused between the packages in the collectd configuration file and the SNMP collections in the data collection configuration file, so for the purpose of this How-To, we'll call them schemes. These schemes bring together OIDs for collection into groups and the groups are mapped to systems. The systems are mapped to interfaces by the systemOID. In addition, each "scheme" controls how the data will be collected and stored.
It becomes clearer as we move on.
First, let's check out the datacollection-config.xml file. Outside of the snmp-collection definition (the "scheme"), there is only one parameter:
&lt;datacollection-config
        rrdRepository = "/var/opennms/rrd/snmp/">
This determines in which directory the collected information will be stored. If you change this value, you must also change the rrdRepository values in the following files:
poller-configuration.xml
thresholds.xml
http-datacollection-config.xml
jmx-datacollection-config.xml
nsclient-datacollection-config.xml
[edit] snmp-collection General Set Up
After the repository had been defined, the next tag starts the snmp-collection definition:
&lt;snmp-collection name="default"
                 maxVarsPerPdu = "50"
                 snmpStorageFlag = "all">
The name attribute is pretty self-explanatory. This is the name that must be matched to the key="collection" value in the collectd configuration file. The maxVarsPerPdu places a limit on the number of SNMP variables that will be retrieved with a GET-BULK request in one packet. You should not need to adjust this, but if you have some SNMP agents that are somewhat slow, you could reduce this to ease the load on them.
The snmpStorageFlag is a pretty important attribute. It can be set to "all" (default) or "primary". What this does is determine if SNMP data collection will occur on all interfaces for a particular node or just the interface marked as "primary". This can greatly affect the size of your Round Robin Database (RRD) if you have a number of multi-interface devices like switches, but it won't have much affect on a network consisting mainly of servers (which tend to only have a single interface).
This is one instance where you may want to have two collection packages and two collection schemes. You could build a collection package for just routers where snmpStorageFlag is set to "all" in the collector scheme and then have everything else in another package where it is set to "primary" in the scheme.
[edit] RRD Configuration
&lt;rrd step = "300">
  &lt;rra>RRA:AVERAGE:0.5:1:8928&lt;/rra>
  &lt;rra>RRA:AVERAGE:0.5:12:8784&lt;/rra>
  &lt;rra>RRA:MIN:0.5:12:8784&lt;/rra>
  &lt;rra>RRA:MAX:0.5:12:8784&lt;/rra>
&lt;/rrd>
The next section of the scheme configuration specifies RRD (round robin database) parameters for storing and rolling up the collected data sampes. RRDTool is a product that grew out of MRTG. It creates a very compact database structure for the storage of periodic data, such as is gathered by OpenNMS. RRD data is stored in files that are created when initialized to hold data for a certain amount of time. This means that with the first data collection these files are as large as they will ever get, but it also means that you will see an initially large decrease in disk space as collection is first started.. Once the RRD file is full, the oldest data is discarded.
OpenNMS releases up to and including 1.2.9 used RRDTool proper by default via a JNI (Java Native Interface), meaning that the resulting files could be read by other applications capable of consuming RRDTool's file format. The files written by OpenNMS via the JNI RRD strategy have a .rrd extension by default. Beginning with the 1.3.2 release, the default is to use JRobin, a pure-Java implementation of RRDTool 1.0's functionality. The files produced via the JRobin RRD strategy have a .jrb extension by default, and are not compatible with RRDTool proper. See the JRobin site for the motivation behind this decision.
The first line, the rrd step size, determines the granularity of the data. By default this is set to 300 seconds, or five minutes, which means that the data will be saved once every five minutes per step. Note that this is also one of the few places where time in OpenNMS is referenced in seconds instead of milliseconds.
Each RRD is made up of Round-Robin Archives. An RRA consists of a certain number of steps. All of the data that is collected in those steps is then consolidated into a single value that is then stored in the RRD. For instance, if I poll a certain SNMP variable once a minute, I could have an RRA that would collect all samples over a step of five minutes, average the (five) values together, and store the average in the RRD.
The RRA statements take the form:
RRA:Cf:xff:steps:rows
RRA 
This string defines the line as an RRA configuration command. It does not change, and is always the text "RRA".
Cf 
This field represents the "consolidation function". It can take one of four values, AVERAGE, MAX, MIN, or LAST. They are detailed below.
xff 
This is the "x-files factor". If we are trying to consolidate a number of samples into one, there is a chance that there could be gaps where a value wasn't collected (the device was down, etc.). In that case, the value would be UNKNOWN. This factor determines how many of the samples can be UNKNOWN for the consolidated sample is considered UNKNOWN. By default this is set to 0.5 or 50%.
steps 
This states the number of "steps" that make up the RRA. For example, if the step size is 300 seconds (5 minutes) and the number of steps is 12, then the RRA is 12 x 5 minutes = 60 minutes = 1 hour long, and it will stored the consolidated value for that hour.
rows 
The rows field determine the number of values that will be stored in the RRA.
[edit] Consolidation Functions
These are used in the "Cf" part of an RRA statement.
AVERAGE 
Average all the values over the number of steps in the RRA.
MAX 
Store the maximum value collected over the number of steps in the RRA.
MIN 
Store the minimum value collected over the number of steps in the RRA.
LAST 
Store the last value collected over the number of steps in the RRA.
Let's bring this all together with some more examples. Take the first RRA line in the configuration:
RRA:AVERAGE:0.5:1:8928
This says to create an archive consisting of the AVERAGE value collected over 1 step and store up to 8928 of them. If, for any step, more than 50% of the values are UNKNOWN, then the average value will be UNKNOWN. Since the default step size is 300 seconds, or five minutes, and the default polling cycle (in the collectd configuration) is five minutes, we would expect there to be one value per step, and so the AVERAGE should be the same as the MIN or MAX or LAST. 8928 five minute samples at 12 samples per hour and 24 hours per day is 31 days. Thus this RRA will hold five minute samples for 31 days before discarding data.
The next lines get a little more interesting:
RRA:AVERAGE:0.5:12:8784
RRA:MIN:0.5:12:8784
RRA:MAX:0.5:12:8784
The only difference between these lines is the consolidation function. We are going to "roll up" the step 1 samples (5 minutes) into 12 step samples (1 hour). We are also going to store three values: the average of all samples during the hour, the minimum value of those samples and the maximum value. This data is useful for various reports (the AVERAGE shows throughput whereas MAX and MIN show peaks and valleys). These will be stored as one hour samples 8784 times, or 366 days.
So, to summarize, by default the SNMP collector will poll once every five minutes. This value will be stored as collected for 31 days. Also, hourly samples will be stored which include the MIN, MAX and AVERAGE.
You can easily change these numbers to increase or decrease the amount of data stored. A few caveats. First, increasing the amount and/or frequency of samples will have a direct affect on the amount of disk space required. You could add a MIN and MAX RRA for the single step RRA, which would increase necessary disk space by up to 50%, but since by default there is only one value, MIN, MAX and AVERAGE will be the same, so it is not really necessary unless you also increase the polling rate. Second, you cannot change these numbers once collection has started without losing all of the collected data up to that point. So it is important to set your values early. When you change these numbers, you must delete all .jrb files in order for them to be re-created.
A note for international users: if your LOCALE is set to something other than "en_US" you may need to use a "comma" instead of a "period" in the xff, for example:
RRA:AVERAGE:0,5:12:8784
RRA:MIN:0,5:12:8784
RRA:MAX:0,5:12:8784
You have to do this if you see a "can't parse argument 'RRA:AVERAGE:0.5:1:8928'" in the collectd log file.
[edit] Resource Types
If you wish to collect tabular or "columnar" data from MIB tables that are indexed on some instance identifier other than ifIndex, you will need to have a custom resourceType element for each unique table-indexing strategy. Details on this type of collection are available separately in this article.
[edit] Groups
If you are still with me, let's talk about something a little more intuitive with respect to SNMP data collection, the SNMP variables themselves. OpenNMS comes with a utility (OPENNMS_HOME/contrib/mibparser/dist/parseMib.sh) that automates much of the work involved in importing OIDs for collection, but its output almost always requires some amount of work by a human operator. Each value is spelled out in a group entry:
&lt;groups>
  &lt;group  name = "mib2-interfaces" ifType = "all">
    &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.10" instance="ifIndex" alias="ifInOctets" type="counter"/>
    &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.13" instance="ifIndex" alias="ifInDiscards" type="counter"/>
    &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.14" instance="ifIndex" alias="ifInErrors" type="counter"/>
    &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.16" instance="ifIndex" alias="ifOutOctets" type="counter"/>
    &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.19" instance="ifIndex" alias="ifOutDiscards type="counter"/>
    &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.20" instance="ifIndex" alias="ifOutErrors" type="counter"/>
  &lt;/group>
SNMP variable collections are placed into groups to make it easier to associate with specific kinds of devices. A group consists of a group name and the types of interfaces (ifType) for which the member objects should be collected.
The ifType attribute can take on the following values:
all 
This means that all interface type will be polled for the OIDs included in the group.

ignore 
This is used for scalar values, i.e. those that appear only once on a device, such as the "load average" for a router. This value will be collected and stored once for the device.
[specific numeric value] 
You may want to poll certain value from ATM interfaces, others from point-to-point WAN links, and still others from Ethernet interfaces. For example:
&lt;group  name = "my-ATM-example" ifType = "37">
&lt;group  name = "ethernet-example" ifType = "6,62">
See http://www.iana.org/assignments/ianaiftype-mibfor a comprehensive list of ifType values.
As a special case, groups containing object definitions for tabular (aka "columnar") data from tables indexed by any instance identifier other than ifIndex must have a group type of all. This type of data is referred to as generic index data, and is described in more detail in article Collecting SNMP data from tables with arbitrary indexes.
It is important never to mix scalar data, interface-level data, and generic-index data in the same group.
Each SNMP MIB variable consists of an OID plus an instance. Usually, that instance is either zero (0) or an index to a table. At the moment, OpenNMS only understands the ifIndex index to the ifTable. All other instances have to be explicitly configured. The alias must be no more than 19 characters in length (a limitation stemming from the design of RRDTool), unique per combination of device type and resource type, and usually should be unique per OID. The RRD file that is created will have the alias as its filename.
OpenNMS understands four types of numeric variables to collect: gauge, timeticks, integer, counter. Since RRD only understands numeric data, any string types encountered will be parsed to a number before being persisted in RRD storage. If the conversion cannot be made (perhaps you are trying to collect on systemName, for example), a log message will be generated. Starting with OpenNMS 1.3.2, a type of string can be used to collect string values and store their values separately from RRD files.
[edit] Systems
Once the groups are defined, the last step is to associate them with the systems to be monitored. The SNMP systemOID (.1.3.6.1.2.1.1.2, instance 0) returns another OID that is meant to uniquely identify the type of equipment being used.
&lt;systems>
  &lt;systemDef name = "Net-SNMP">
    &lt;sysoidMask>.1.3.6.1.4.1.2021.250.&lt;/sysoidMask>
    &lt;collect>
      &lt;includeGroup>mib2-interfaces-net-snmp&lt;/includeGroup>
      &lt;includeGroup>mib2-host-resources-storage&lt;/includeGroup>
      &lt;includeGroup>mib2-host-resources-system&lt;/includeGroup>
      &lt;includeGroup>mib2-host-resources-memory&lt;/includeGroup>
      &lt;includeGroup>ucd-loadavg&lt;/includeGroup>
    &lt;/collect>
  &lt;/systemDef>
In this system definition, any device with a system OID that is being used for SNMP data collection whose systemOID starts with ".1.3.6.1.4.1.2021.250." will collect on five MIB groups: mib2-interfaces-net-snmp, mib2-host-resources-storage, mib2-host-resources-system, mib2-host-resources-memory and ucd-loadavg.
If you want to match against a specific oid use &lt;sysoid> instead of &lt;sysoidMask>
So, to review once again - you set up collection packages, similar to poller packages, in the collectd-configuration.xml file. A key in that file points to a particular snmp-collection tag in datacollection-config.xml (this is what I have referred to as a scheme). For each scheme, you set up how the data will be stored, whether all interfaces will be collected on or just the primary interface for each node, what MIB OIDs are included in each MIB group, and what MIB groups are associated with what systems, based on the system definition.
Got it? Whew.
[edit] Modular Configuration
As of OpenNMS 1.8.4 and 1.9.1, it is now possible to modularly include multiple configuration files into datacollection-config.xml, much like eventconf.xml.
First, make sure you have an $OPENNMS_HOME/etc/datacollection directory. If not, make it. Then, create one or more configuration files in that directory. The opening tag should be "&lt;datacollection-group>" with a name set, which then can contain any number of resourceType, group, and systemDef definitions, just like the main datacollection-config.xml file. For example:

 &lt;?xml version="1.0"?>
 &lt;datacollection-group name="Cisco">
 
     &lt;resourceType name="cbgpPeerAddrFamilyPrefixEntry" label="Cisco BGP Peer / Address Family"
                   resourceLabel="Peer ${subIndex(0,4)}">
       &lt;persistenceSelectorStrategy class="org.opennms.netmgt.collectd.PersistAllSelectorStrategy"/>
       &lt;storageStrategy class="org.opennms.netmgt.dao.support.IndexStorageStrategy"/>
     &lt;/resourceType>
 
       &lt;group name="cisco-bgp-peer-addr-family-prefix-stats" ifType="all">
         &lt;mibObj oid=".1.3.6.1.4.1.9.9.187.1.2.4.1.1" instance="cbgpPeerAddrFamilyPrefixEntry"
                   alias="cbgpPeerAcceptedPfx" type="gauge" />
       &lt;/group>
 
     &lt;systemDef name="Cisco Routers">
       &lt;sysoidMask>.1.3.6.1.4.1.9.1.&lt;/sysoidMask>
       &lt;collect>
         &lt;includeGroup>adsl-line&lt;/includeGroup>
         &lt;includeGroup>rfc1315-frame-relay&lt;/includeGroup>
         &lt;includeGroup>mib2-X-interfaces&lt;/includeGroup>
         &lt;includeGroup>ietf-bgp4-peer-stats&lt;/includeGroup>
         &lt;includeGroup>cisco-bgp-peer-addr-family-prefix-stats&lt;/includeGroup>
         &lt;/collect>
       &lt;/systemDef>
 
 &lt;/datacollection-group>
Then, add it near the end of datacollection-config.xml within an snmp-collection tag, using the group name you defined in the individual XML file:
 &lt;?xml version="1.0"?>
 &lt;datacollection-config ...>
   &lt;snmp-collection ...>
 ...
     &lt;include-collection dataCollectionGroup="Cisco" />
   &lt;/snmp-collection>
 
 &lt;/datacollection-config>
[edit] nsclient-datacollection-config.xml
First, a simple example:
&lt;nsclient-datacollection-config rrdRepository="/opt/opennms/share/rrd/snmp/">
  &lt;nsclient-collection name="default">
    &lt;rrd step="300">
      &lt;rra>RRA:AVERAGE:0.5:1:8928&lt;/rra>
      &lt;rra>RRA:AVERAGE:0.5:12:8784&lt;/rra>
      &lt;rra>RRA:MIN:0.5:12:8784&lt;/rra>
      &lt;rra>RRA:MAX:0.5:12:8784&lt;/rra>
    &lt;/rrd>

    &lt;wpms>
      &lt;!--  A group for collecting processor stats.
        Check the keyvalue "% Processor Time" - if it's there (should be) collect this whole group.
      Check every recheckInterval milliseconds (3600000 = 1hr) -->
      &lt;wpm name="Processor" keyvalue="\Processor(_Total)\% Processor Time" recheckInterval="3600000">
      	&lt;!--  Collect these attributes.  Name is the name to pass to NSClient.  
      	Alias is the local name for the RRD file 
      	Type is used to convert values around
      	maxval/minval are optional-->
      	&lt;attrib name="\Processor(_Total)\% Processor Time" alias="cpuProcTime" type="Gauge"/>
      	&lt;attrib name="\Processor(_Total)\% Interrupt Time" alias="cpuIntrTime" type="Gauge"/>
      	&lt;attrib name="\Processor(_Total)\% Privileged Time" alias="cpuPrivTime" type="Gauge"/>
      	&lt;attrib name="\Processor(_Total)\% User Time" alias="cpuUserTime" type="Gauge"/>
      &lt;/wpm>
  &lt;/nsclient-datacollection>
&lt;/nsclient-datacollection-config>
As for datacollection-config.xml, the name attribute specifies a name that must be matched to the key="collection" value in the collectd configuration file. Similarly, the RRD section has the same syntax and meaning as in datacollection-config.xml; see the earlier section on that for details.
The performance monitor counters to collect are defined in the &lt;wpms> section. Groups of counters are defined within a &lt;wpm> tag. Each &lt;wpm> has:
name 
arbitrary and for your own purposes
keyvalue 
if the keyvalue perfmon counter can be obtained from the agent, then the rest of the counters in the group are collected as well.
recheckInterval 
The presence of the key value is rechecked every recheckInterval milliseconds, to avoid causing undue load on the server checking for non-existent values.
Note that the value obtained from the keyvalue is not stored, unless explicitly mention in an additional attrib
Perfmon counters that should actually be collected and stored are defined in an attrib tag, which has the following parameters:
name 
The performance counter to collect. This name is the full path to the counter, which is typically \&lt;section>(&lt;specific_instance>)\&lt;counter>. Specific instance is only used where there are more than one of a counter available. For example, in the Processor section the specific index can be either 0-(num processors-1), or _Total to see the total of counters across all instances. See the example configuration files for other examples of syntax in specifying the counter name.
alias 
This is the same as in the mibObj tag in datacollection-config.xml, and defines the name of the RRD data item that will be stored. RRD limitations require it to be 19 characters or less in length.
type 
Again the same as for mibObj in datacollection-config.xml, defining the interpretation of the data point. A "gauge" is a point in time value, e.g Processor usage, where as a "counter" is for monotonically increasing counter values such as "number of http requests".
[edit] jmx-datacollection-config.xml
Again, we start with an example:
&lt;?xml version="1.0"?>
&lt;jmx-datacollection-config
    rrdRepository = "/opt/opennms/rrd/snmp/">
    &lt;jmx-collection name="jboss"
        maxVarsPerPdu = "50">
        &lt;rrd step = "300">
            &lt;rra>RRA:AVERAGE:0.5:1:8928&lt;/rra>
            &lt;rra>RRA:AVERAGE:0.5:12:8784&lt;/rra>
            &lt;rra>RRA:MIN:0.5:12:8784&lt;/rra>
            &lt;rra>RRA:MAX:0.5:12:8784&lt;/rra>
        &lt;/rrd>
      
        &lt;mbeans>   
          &lt;mbean name="SystemInfo" objectname="jboss.system:type=ServerInfo">  
              &lt;attrib name="FreeMemory"   alias="FreeMemory"       type="gauge"/> 
              &lt;attrib name="TotalMemory"  alias="TotalMemory"      type="gauge"/>  
          &lt;/mbean> 
        &lt;/mbeans>
   &lt;/jmx-collection>
&lt;/jmx-datacollection-config>
The initial tags have the same layout and meaning as for SNMP (datacollection-config.xml) and NSClient (nsclient-datacollection-config.xml). The top level tag defines where RRD data is stored, the jmx-collection tag has a name that matches a service configuration in collectd-configuration.xml, and the RRD configuration has exactly the same syntax and meaning.
Actual data values to collect are defined within the mbeans tag. This tag has a list of mbean tags that represent the MBeans to collect. Each mbean tag has:
name 
An arbitrary name for your own use
objectname 
The object name used to identify the desired object to the JMX agent
Within each mbean tag, the attributes of that obtained object that should be collected are defined in attrib tags. Each attrib has:
name 
The name of the attribute to get out of the mbean object
alias 
This is the same as in the mibObj tag in datacollection-config.xml, and defines the name of the RRD data item that will be stored. RRD limitations require it to be 19 characters or less in length.
type 
Again the same as for mibObj in datacollection-config.xml, defining the interpretation of the data point. A "gauge" is a point in time value, e.g Processor usage, where as a "counter" is for monotonically increasing counter values such as "number of http requests".
[edit] http-datacollection-config.xml
Again, and example:
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;http-datacollection-config  
    xmlns:http-dc="http://xmlns.opennms.org/xsd/config/http-datacollection" 
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
    xsi:schemaLocation="http://xmlns.opennms.org/xsd/config/http-datacollection http://www.opennms.org/xsd/config/http-datacollection-config.xsd" 
    rrdRepository="@install.share.dir@/rrd/snmp/" >
  &lt;http-collection name="doc-count">
    &lt;rrd step="300">
      &lt;rra>RRA:AVERAGE:0.5:1:8928&lt;/rra>
      &lt;rra>RRA:AVERAGE:0.5:12:8784&lt;/rra>
      &lt;rra>RRA:MIN:0.5:12:8784&lt;/rra>
      &lt;rra>RRA:MAX:0.5:12:8784&lt;/rra>
    &lt;/rrd>
    &lt;uris>
      &lt;uri name="document-counts">
        &lt;url path="/test/resources/httpcolltest.html"
             user-agent="Mozilla/5.0 (Macintosh; U; PPC Mac OS X; en) AppleWebKit/412 (KHTML, like Gecko) Safari/412" 
             matches=".*([0-9]+).*" response-range="100-399" >
        &lt;/url>
        &lt;attributes>
          &lt;attrib alias="documentCount" match-group="1" type="counter32"/>
        &lt;/attributes>
      &lt;/uri>
    &lt;/uris>
  &lt;/http-collection>
&lt;/http-datacollection-config>
[edit] Where Does All the Data Go?
In the last section, RRD files were mentioned pretty often. Where do they go? Well, they go into the RRD repository, defined in datacollection-config.xml, which by default is /var/opennms/rrd/snmp.
For each node for which data is collected, there will exist a directory that consists of the node number. Thus, if the system was collecting data on node 18, there would be a directory called /var/opennms/rrd/snmp/18.
RRDs that are collected for that node (i.e, the node OID matches the system's sysoidMask, and the mibObj of the groups included in that system have ifType != ignore) will be present in this directory. The files will be names with the alias defined in the mibObj element, plus ".rrd" (JNI) or ".jrd" (JRobin). For example: cpuPercentBusy.rrd and memorySize.rrd. The extension depends whether the RRD is configured to use JNI or JRobin (the default now).
For each interface on the node that is being used for data collection, a subdirectory will exist consisting of the interface description (ifDescr) and the MAC address. The MAC address was added because on some switches, multiple ports will have the same ifDescr. There have been tales of devices where the interfaces had both the same ifDescr and MAC address, and at the moment no solution exists for that case. So, if on node 18 there was an interface described as "eth0", its RRD directory would be /var/opennms/rrd/snmp/18/eth0-[MAC Addr.]. Into that directory would go all interface specific RRD files such as ifInOctets.rrd and ifOutOctets.rrd.
[edit] Troubleshooting
Here are a few tips to help troubleshooting SNMP data collection issues.
[edit] verify snmp access to device
Verify that the node supports SNMP and is reachable from your opennmns server. First try to ping the device. If successfull routing to the device is working.
Try snmpwalk like
snmpwalk -v 2c -c secret nodename
from the server opennms is running on. There might be firewall or routing issues if you try it from another machine.
If this fails check if the device has configured snmp access as expected, check the snmp community, snmp version (might be Version 1, 2c or 3) and that there are no firewalls, access-lists or whatever denying access to the device.
[edit] verify opennms snmp access to device
If still no SNMP information shows up on the node page in the WebUI, check the snmp-config.xml file to insure that the proper community name is configured (and as above insure that a given address is not included in multiple ranges, as only the first match will be used).
The next thing to check is the capsd.log file. If this is a new installation, look to see where capsd tested that device. If it is an old installation, you can force a rescan from the node page, and this should create new logs.
Look to see that the SNMP service was set to "true" for that IP address. If not, check the SNMP community name once again. Play with it until a rescan does produce a "true".
If you have gotten this far, then SNMP information from the system tree should show up on the node page.
[edit] verify snmp oid to collect
The next error to look for will be something like:
IfTable: snmpTimeoutError for: ipaddress
This would indicate that something is wrong as we try to get the ipAddrTable and ifTable information.
Two things to try here:
	1.	Run "snmpwalk -c community_name ipaddress". This should walk the entire SNMP MIB for that device. Some UCD SNMP agents by default will only return the system tree.
	2.	Try forcing the version to version 1 in snmp-config.xml and doing a rescan on the node. The ifTable and ipAddrTable can be large, and thus benefit from using the SNMPv2 GET-BULK command. However, we have seen on at least one device that something gets fragmented with the command and we never get to see the tables. If this happens and is fixed by setting the version to 1, please, please, please report it and if possible get a tcpdump of the SNMP packets sent during the capsd scan. Note that the snmpwalk command from the command line uses SNMPGET from version 1 and will not reproduce a problem with version 2.
If you have a valid ifIndex (it will be displayed on the interface page of the WebUI), then you should be able to collect SNMP information. Check the database:
	1.	Run "psql -U opennms opennms".
	2.	At the command prompt, run "SELECT * FROM ipInterface WHERE nodeid=x;" and "x" is the node's ID number.
	3.	Check to see if at least one interface is marked as primary ("P").
	4.	To exit, type "\q"
If no IP addresses are listed as primary, check your collectd configuration file to insure that at least one IP address that supports SNMP is included in a package. Correct the omission and rescan the node.
Up to this point, you should be checking the capsd.log for errors. For the next steps, start looking at collectd.log
[edit] verify collectd is collecting your data
Looking at collectd.log for the primary interface of your node, you should see attempts being made to collect via that interface. While the datacollection-config.xml file controls data collection, by default any sysObjectID that starts ".1.3.6.1.4.1", which is to my knowledge all of them, will match the mib-2 group which collects on ifInOctets, ifOutOctets, ifInErrors, ifOutErrors, and ifOutDiscards. See if there are any useful log messages (such as timeouts, etc.) that can give you a clue.
You may also enable debugging in $OPENNMS_HOME/etc/log4j.properties (change WARN to DEBUG for collectd:
 log4j.category.OpenNMS.Collectd=DEBUG, COLLECTD
and look again into the collectd log files to see if data is really collected.
If there is nothing to see that opennms is trying to collect the desired data look into the dokumentation and your configuration for $OPENNMS_HOME/etc/datacollection-config.xml and $OPENNMS_HOME/etc/collectd-configuration.xml.
[edit] check if collected data is written to rrd files
Finally, look in the /var/opennms/rrd/snmp/nodeid directory where nodeid is the node ID number for the device you are interested in. You should see *.jrb files being updated, and you can use this command to see if the RRD actually contains data:
$OPENNMS_HOME/bin/jrobin-inspector
If the files in the /var/opennms/rrd/snmp/nodeid directory end in ".rrd", you have configured opennms to use RRD instead of jRRD-Tools, which has a slightly different file format (jRRD ist the default since version 1.3.2). Use
rrdtool dump RRDfilename
to view collected data.
If there still is no data check your rrd configuration in $OPENNMS_HOME/etc/opennms.properties&lt;code>. If there is a line like
org.opennms.rrd.storeByGroup=false
then there should be a *.rrd or *.jrd file for every mib variable you collect. If the line looks like this
org.opennms.rrd.storeByGroup=true
different mib variables are written to a common file. If you add new mib values to collect you have to delete this file so opennms has to create a new one including the new mib values. You will loose all data that is in this file!
[edit] Conclusion
SNMP Data Collection in OpenNMS is one of the more difficult things to set up. Once configured, however, the process can be completely automatic. It is hoped that this How-To has proved useful. Please direct corrections and comments to the author.
[edit] What Now?
	•	SNMP Reports How-To
	•	Data coll graphing
</Text>
        </Document>
        <Document ID="106">
            <Title>Adding Event Defs</Title>
            <Text>If you have some devices that have SNMP trap events that are not supported by OpenNMS, we welcome you to contribute your configuration files to OpenNMS. The official OpenNMS documentation on creating events files from SNMP MIBs using the mib2opennms utility is at SourceForge (Part 5 - Configuring Events).
Not only will this help others in the community, you won't have to worry about merging your events in during an upgrade.
Simply sub a bug with a severity of "enhancement" and attach your events file to the bug.
There are only a few guidelines, and it would save a lot of time if you would submit your events using them.
First, add a short description in front of the log message for the event. For example, instead of:
snTrapL4ConnectionRateExceedThreshold trap received snAgGblTrapMessage=%parm[#1]%
use:
Foundry Event: snTrapL4ConnectionRateExceedThreshold trap received snAgGblTrapMessage=%parm[#1]%
It's very useful to know at a glance what type of devices or application generated the event.
Second, replace "mib2opennms" in the UEI with something that identifies the vendor, such as "vendor/foundry/traps" etc..
Finally, if possible, set the severity to something other than "Indeterminate". Sometimes this is not possible, as the trap might be "The server changed state" which could mean that it is either thankfully back up after dying or totally pooched.
Severities should always be based, not on the problem itself, but your reaction to it. While the fact that a link is down is critical for everyone on the other side of it, if that person happens to be Zeke and Zeke's off fishing then you wouldn't exactly want to drop everything to fix it.
Critical 
Drop everything except your cell phone, and start working on the issue with one hand while calling everyone you know on the other. If the problem doesn't rate calling the President of your company on Christmas morning at 3am, then don't use it.
Seriously 
unless the event indicates a major problem with a number of devices on the network, don't use Critical. It can be used in an automation to priority promote a Major event, but should rarely be set on its own.
Major 
Drop what you are doing, and fix this issue. This indicates that a device is no longer operational. In other words, a power supply had failed, a RAID array is dead, etc.
Minor 
This issue requires attention, but you don't have to drop what you are doing. This would be along the lines of the failure of a redundant power supply (but the main one is still up), etc. The device is still functional, but it will require attention or may fail completely.
Warning 
This is an informational message with a negative tone. You might have to take some action, but it may just be a temporary condition that will resolve itself. For example, if you are running portsentry and someone port scans your devices, the fact that they have been dropped via a routing table rule would be a warning.
Normal 
This is an informational message with a neutral or positive tone. It usually doesn't require any action, but is "for your information." Someone logged on to the server, etc. It can also be used to indicate that an error condition has been resolved, when it is not sure that an event that the error condition exist will be sent.
Cleared
If the device will send an error event such as "backup power supply offline,, the resolution event, such as "backup power supply now online" should have this severity. No action should be required, and this message is basically the same a Normal.
While all of OpenNMS doesn't follow these rules, we are trying to standardize on them, so please consider using them when submitting events.
A few points that are not covered in the official "Configuring Events" document that presented problems in my experience:
Use "-6" with mib2opennms 
mib2opennms needs the "-6" option to force the generic trapType to enterpriseSpecific(6) because libsmi (used by mib2opennms) tends to mis-report it as 1
Enumerated types mangled 
In several of the MIBs I've compiled (all from the same vendor), The enumerated types in the trap varbinds come out mangled. I don't know whether this is due to flaws in the vendor MIBs or to a bug in libsmi / mib2opennms. At any rate, these need to be corrected by hand. You can find them by searching for the string "(-" in the XML output.
Trap Enterprise OIDs come up short 
The "mevalue" for "id" came out one level short, so I corrected these by hand. The vendor had subdivided the traps branch of their MIB ( vendorMgmt.vendorTraps(4) ) into sub-categories ( e.g. vendorMgmt.vendorTraps(4).vendorHwTraps(1), vendorMgmt.vendorTraps(4).ModuleTraps(2) ), but the "mevalue" for the trap enterprise OID was coming out in every case to be the top-level traps branch.
</Text>
        </Document>
        <Document ID="51">
            <Title>Data Collection</Title>
        </Document>
        <Document ID="9">
            <Title>Capability Overview</Title>
            <Synopsis>What can OpenNMS do?</Synopsis>
            <Text>List of things OpenNMS can do</Text>
        </Document>
        <Document ID="48">
            <Title>CapsD &amp; Discovery</Title>
        </Document>
        <Document ID="52">
            <Title>HTTP Datacollection</Title>
        </Document>
        <Document ID="111">
            <Title>SNMP Reports Howto</Title>
            <Text>This guide asumes that you know how to play with RRDTOOL and that you are somewhat familiar with OpenNMS. All the examples presented here were tested on Redhat Linux 7.3, version 1.0 of OpenNMS with Net-SNMP 4.2.3 agents running on the managed stations.
I tried to be as accurate as possible, so read this material at your own risk.
Author:
Jose Vicente Nunez Zuleta (josevnz@newbreak.com)
Newbreak LLC System Administrator
Newbreak LLC
Contents
[hide]
	•	1 Capture the appropriate data using SNMP (edit datacollection-config.xml)
	•	2 Edit the snmp graphic properties (file snmp-graph.properties)
	◦	2.1 Check your RRDTOOL files
	▪	2.1.1 Inspecting with RRDTool
	▪	2.1.2 Inspecting with the JRobin RRD Inspector
	◦	2.2 Add your graph definitions
	•	3 Add New Reports to the reports Property
	•	4 Restart OpenNMS
	•	5 Resources
	•	6 Debugging
[edit] Capture the appropriate data using SNMP (edit datacollection-config.xml)
You will need to define the OIDs you want to capture in the file datacollection-config.xml file. In our example we want to capture the following Net-SNMP OIDs (memory, host uptime and interface traffic):
 &lt;!-- Net-SNMP memory stats, josevnz@newbreak.com --> 
 &lt;group name="ucd-memory" ifType="ignore">
 &lt;!-- Total Swap Size configured for the host. -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.4.3" instance="0" alias="memTotalSwap" type="integer" />   
 &lt;!-- Available Swap Space on the host. --> 
 &lt;mibObj oid=".1.3.6.1.4.1.2021.4.4" instance="0" alias="memAvailSwap" type="integer" />   
 &lt;!-- Total Real/Physical Memory Size on the host. -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.4.5" instance="0" alias="memTotalReal" type="integer" />  
 &lt;!-- Available Real/Physical Memory Space on the host. -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.4.6" instance="0" alias="memAvailReal" type="integer" />  
 &lt;!-- Error flag.  1 indicates very little swap space left -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.4.100" instance="0" alias="memSwapError" type="integer" />
 &lt;/group>

 &lt;!-- Net-SNMP system stats, josevnz@newbreak.com -->
 &lt;group name="ucd-systemStats" ifType="ignore">
 &lt;!-- Amount of memory swapped to disk (kB/s). -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.11.4" instance="0" alias="ssSwapOut" type="integer" />  
 &lt;!-- percentages of user CPU time. -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.11.9" instance="0" alias="ssCpuUser" type="integer" />  
 &lt;!-- percentages of user CPU system. -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.11.10" instance="0" alias="ssCpuSystem" type="integer" / 
 > &lt;!-- percentages of user CPU idle. -->
 &lt;mibObj oid=".1.3.6.1.4.1.2021.11.11" instance="0" alias="ssCpuIdle" type="integer" /> 
 &lt;/group>

 &lt;group  name = "mib2-interfaces-net-snmp" ifType = "all">
 &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.10" instance="ifIndex" alias="ifInOctets"    type="counter"/ >
 &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.11" instance="ifIndex" alias="ifInUcastPkts" type="counter"/ >
 &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.12" instance="ifIndex" alias="ifInNUcastPkts" type="counter"/>
 &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.14" instance="ifIndex" alias="ifInErrors"    type="counter"/>
 &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.16" instance="ifIndex" alias="ifOutOctets"   type="counter"/>
 &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.19" instance="ifIndex" alias="ifOutDiscards" type="counter"/>
 &lt;mibObj oid=".1.3.6.1.2.1.2.2.1.20" instance="ifIndex" alias="ifOutErrors"   type="counter"/>
 &lt;/group>
Then make sure your OIDs make it into the "systemDef" tag:
 &lt;systemDef name = "Net-SNMP">
 &lt;!-- &lt;sysoidMask>.1.3.6.1.4.1.2021.250.&lt;/sysoidMask> -->
 &lt;sysoidMask>.1.3.6.1.4.1.2021.&lt;/sysoidMask>
 &lt;collect>
 &lt;includeGroup>mib2-interfaces-net-snmp&lt;/includeGroup>
 &lt;includeGroup>mib2-host-resources-storage&lt;/includeGroup> 
 &lt;includeGroup>mib2-host-resources-system&lt;/includeGroup>
 &lt;includeGroup>mib2-host-resources-memory&lt;/includeGroup> 
 &lt;includeGroup>ucd-loadavg&lt;/includeGroup>
 &lt;includeGroup>ucd-systemStats&lt;/includeGroup>
 &lt;includeGroup>ucd-memory&lt;/includeGroup>
 &lt;/collect>
 &lt;/systemDef>
Please note than if a given OID cannot be retrieved (for example the SNMP agent doesn't support it) then the RRDtool file is not created (normally stored in /var/opennms/rrd/snmp/)
Additional notes  Make sure &lt;sysoidMask> matches your systems oid or else it will not find it.
$ snmpget -v2c -On -c &lt;community> &lt;host> sysObjectID.0
.1.3.6.1.2.1.1.2.0 = OID: .1.3.6.0.0.0.0.0.0.0
would give you:
&lt;sysoidMask>.1.3.6.0.0.0.0.0.0.0.&lt;/sysoidMask>
[edit] Edit the snmp graphic properties (file snmp-graph.properties)
[edit] Check your RRDTOOL files
You need to define your custom graphs as follows:
name
choose the name you will use for the "reports" property
columns
which RRD files you will use in your graph
type
use 'interfaceSnmp' if the stat is related with each network interface, use 'nodeSnmp' if affects the whole node, or if the stat is a generic-indexed resourceType such as hrStorageIndex, use the name of the type.
NOTE: If you are using an OpenNMS release older than 1.3.2, the types are interface and node instead of interfaceSnmp and nodeSnmp. Also, please upgrade!
externalValues
database values, currently only ifSpeed
propertiesValues
values collected as string and persisted to 'strings.properties'
command
This is a command line suitable for passing to RRDTool. If using JRobin (the default since OpenNMS 1.3.2), this command is translated internally into appropriate JRobin method invocations.
[edit] Inspecting with RRDTool
This subsection applies only if you are using the JniRrdStrategy. Since OpenNMS 1.3.2, the default is to use the JRobinRrdStrategy; see below.
One way to know which datasources exist on your RRD file (used in the DEF part of the rrdtool command line) is to get the information from the rrdtool file like this:
[root@lnxdev0001 16]# rrdtool info hrSystemUptime.rrd
filename = "hrSystemUptime.rrd"
rrd_version = "0001"
step = 300
last_update = 1026942859
ds[hrSystemUptime].type = "GAUGE"
ds[hrSystemUptime].minimal_heartbeat = 600
ds[hrSystemUptime].min = NaN
ds[hrSystemUptime].max = NaN
ds[hrSystemUptime].last_ds = "UNKN"
ds[hrSystemUptime].value = 2.0296406018e+10
ds[hrSystemUptime].unknown_sec = 0
rra[0].cf = "AVERAGE"
rra[0].rows = 8928
rra[0].pdp_per_row = 1
rra[0].xff = 5.0000000000e-01
rra[0].cdp_prep[0].value = NaN
rra[0].cdp_prep[0].unknown_datapoints = 0
rra[1].cf = "AVERAGE"
rra[1].rows = 8784
rra[1].pdp_per_row = 12
rra[1].xff = 5.0000000000e-01
rra[1].cdp_prep[0].value = 7.8203629928e+08
rra[1].cdp_prep[0].unknown_datapoints = 0
rra[2].cf = "MIN"
rra[2].rows = 8784
rra[2].pdp_per_row = 12
rra[2].xff = 5.0000000000e-01
rra[2].cdp_prep[0].value = 7.8068611333e+07
rra[2].cdp_prep[0].unknown_datapoints = 0
rra[3].cf = "MAX"
rra[3].rows = 8784
rra[3].pdp_per_row = 12
rra[3].xff = 5.0000000000e-01
rra[3].cdp_prep[0].value = 7.8338616000e+07
rra[3].cdp_prep[0].unknown_datapoints = 0
As you can see, the ds is 'hrSystemUptime'
If for some reason one of the rrdtool files that are part of a graphic definition is missing, then the graphic doesn't show up at all in the web console.
Remember than you can always check the contents of a rrdtool file typing:
rrdtool dump hrSystemUptime.rrd (shows a huge amount of data).
[edit] Inspecting with the JRobin RRD Inspector
Since OpenNMS 1.3.2, the default is to use JRobin instead of RRDTool. While JRobin lacks an equivalent of the rrdtool info and rrdtool dump commands, it does include a Swing GUI application that allows you to inspect the contents of a JRobin RRD file interactively. You can launch the inspector as follows:
$ java -cp /opt/opennms/lib/jrobin-1.5.8.jar org.jrobin.inspector.RrdInspector hrSystemUptime.jrb
[edit] Add your graph definitions
You can always check if the graphs you want to plot are accurate by running the rrdtool command line; Here is an example to graph the host uptime by hand:
 rrdtool graph uptime.png --title "Host uptime" --vertical-label \
 Days "DEF:timeticks=hrSystemUptime.rrd:hrSystemUptime:AVERAGE" \
 "CDEF:days=timeticks,8640000,/" AREA:days#FF0000:"Days" \
 GPRINT:days:AVERAGE:"Avg  \\: %8.1lf %s" GPRINT:days:MIN:"Min  \\: %8.1lf %s" \
 GPRINT:days:MAX:"Max  \\: %8.1lf %s"
As you can see in the next lines, you will "copy and paste" part of this line in the graphic definition.
Here is the graphic configuration to capture the host uptime, memory usage and link accuracy, file snmp-graph.properties:
#### Newbreak LLC Custom reports. Jose Vicente Nunez Zuleta (josevnz@newbreak.com) #####
# Be very careful with trailing spaces, otherwise you can get problems with the graphics!!!

# Get the Uptime if the host uses Net-SNMP (josevnz@newbreak.com). To get the graphic manually:
report.netsnmp.uptime.name=Uptime
report.netsnmp.uptime.columns=hrSystemUptime
report.netsnmp.uptime.type=nodeSnmp
report.netsnmp.uptime.command=--title "Host uptime" \
  --vertical-label Days \
  DEF:timeticks={rrd1}:hrSystemUptime:AVERAGE \
  CDEF:days=timeticks,8640000,/ \
  AREA:days#FF0000:"Days" \
  GPRINT:days:AVERAGE:"Avg  \\: %8.1lf %s" \
  GPRINT:days:MIN:"Min  \\: %8.1lf %s" \
  GPRINT:days:MAX:"Max  \\: %8.1lf %s" \

# Get the Memory statistics if the host uses Net-SNMP (josevnz@newbreak.com).
report.netsnmp.memory.name=Memory
report.netsnmp.memory.columns=memTotalSwap,memAvailSwap,memTotalReal,memAvailReal
report.netsnmp.memory.type=nodeSnmp
report.netsnmp.memory.command=--title "Host memory usage" \
  --vertical-label bytes \
  DEF:mtotalSwap={rrd1}:memTotalSwap:AVERAGE \
  DEF:mavailSwap={rrd2}:memAvailSwap:AVERAGE \
  DEF:mtotalReal={rrd3}:memTotalReal:AVERAGE \
  DEF:mavailReal={rrd4}:memAvailReal:AVERAGE \
  CDEF:totalSwap=mtotalSwap,1024,* \
  CDEF:availSwap=mavailSwap,1024,* \
  CDEF:totalReal=mtotalReal,1024,* \
  CDEF:availReal=mavailReal,1024,* \
  LINE3:totalSwap#FF0000:"Total swap" \
  LINE1:availSwap#00FF00:"Available swap" \
  LINE3:totalReal#0000FF:"Total real" \
  LINE1:availReal#000000:"Available real" \
  GPRINT:totalSwap:AVERAGE:"Avg  \\: %8.1lf %s" \
  GPRINT:totalSwap:MIN:"Min  \\: %8.1lf %s" \
  GPRINT:totalSwap:MAX:"Max  \\: %8.1lf %s" \
  GPRINT:availSwap:AVERAGE:"Avg  \\: %8.1lf %s" \
  GPRINT:availSwap:MIN:"Min  \\: %8.1lf %s" \
  GPRINT:availSwap:MAX:"Max  \\: %8.1lf %s" \
  GPRINT:totalReal:AVERAGE:"Avg  \\: %8.1lf %s" \
  GPRINT:totalReal:MIN:"Min  \\: %8.1lf %s" \
  GPRINT:totalReal:MAX:"Max  \\: %8.1lf %s" \
  GPRINT:availReal:AVERAGE:"Avg  \\: %8.1lf %s" \
  GPRINT:availReal:MIN:"Min  \\: %8.1lf %s" \
  GPRINT:availReal:MAX:"Max  \\: %8.1lf %s" \

# Calculate the network accuracy using SNMP (josevnz@newbreak.com).
# Check the following man pages for more info on rrdtool graph:
# - rrdgraph_graph
# - rrdgraph
# - rrdgraph_examples
# Also if you forgot anbout the RPN notation (like me :) ) then go to:
# - http://people.ee.ethz.ch/~oetiker/webtools/rrdtool/tutorial/rpntutorial.html
#
# Use CDEF to calculate this expression:
# accuracy = 100 - ( (DifInErr*100) / (DifInUcast + DifInNUcast) )
# In my case, Net-SNMP doesn't show up the ifInNUcastPkts OID on Linux (Solaris works fine)
# but the amount of traffic is very low, so even taking out that value the estimate is good.
report.netsnmp.accuracy.name=Accuracy
#report.netsnmp.accuracy.columns=ifInErrors,ifInUcastPkts,ifInNUcastPkts
report.netsnmp.accuracy.columns=ifInErrors,ifInUcastPkts
report.netsnmp.accuracy.type=interfaceSnmp
report.netsnmp.accuracy.command=--title "Link accuracy" \
  DEF:error={rrd1}:ifInErrors:AVERAGE \
  DEF:ucast={rrd2}:ifInUcastPkts:AVERAGE \
  CDEF:accuracy=100,error,100,*,ucast,/,- \
  LINE2:accuracy#FF0000:"% Accuracy" \
  GPRINT:accuracy:AVERAGE:"Avg  \\: %8.1lf %s" \
  GPRINT:accuracy:MIN:"Min  \\: %8.1lf %s" \
  GPRINT:accuracy:MAX:"Max  \\: %8.1lf %s" \

# If you are sure than all your machines have information about multicast traffic, then:
# DEF:nucast={rrd3}:ifInNUcastPkts:AVERAGE \
# CDEF:accuracy=100,error,100,*,ucast,nucast,+,/,- \
[edit] Add New Reports to the reports Property
Finally, don't forget to add the reports into the value of the reports property.
#report keys, list ALL prefab reports here!
reports=traffic, octets, errors, discards, avgbusy5, freemem, \
  bufferfails, kerneltasks, kernelmem, \
  cpuPercentBusy, \
  novell.numberOfNLMsLoaded, novell.openFiles, novell.licensedConnections, \
  memory, \
  novell.codeDataMemory, novell.cacheBuffers, \
  novell.diskSpaceSys, novell.diskSpaceVol2, \
  winnt2k.diskSpaceC, winnt2k.diskSpaceD, \
  checkpoint.pktsAccepted, checkpoint.pktsRejected, \
  checkpoint.pktsDropped, checkpoint.pktsLogged, \
  loadavg, netsnmp.uptime, netsnmp.memory, netsnmp.accuracy \
Our reports are netsnmp.uptime, netsnmp.memory, netsnmp.accuracy on the last line. If you miss this step, you will get web exceptions when attempting to run performance reports. The exceptions will look something like the following:
Missing Parameter

The request you made was incomplete. It was missing the  report parameter.

The following parameters are required:
report
node
[edit] Restart OpenNMS
/etc/init.d/opennms restart
/etc/init.d/tomcat4 restart
Probably you will have to wait a couple of minutes before you get useful data to display.
If you only changed the snmp-graph.properties you do not need to restart; a reload of the report will reread the properties file.
[edit] Resources
Please have a look How to present data sources from n Nodes on how to create Graphs which show datasources from several nodes.
Tutorial on how to use the RPN notation:
 http://oss.oetiker.ch/rrdtool/tut/rpntutorial.en.html
Check the following man pages for more info on rrdtool graph:
 rrdgraph_graph
 rrdgraph
 rrdgraph_examples
[edit] Debugging
If you have problems getting your customized reports to work check the $OPENNMS_HOME/logs/webapp/jetty.log logfile for errors.
Check if there is data collected at all for the variables you want to graph. Get the nodeid (on the node's page you can see it in the URL) and go to $OPENNMS_HOME/share/rrd/snmp/nodeid#. There should be at least one file in this directory or in some subdirectories named *.jrb. If not check the wiki on how to configure data collection.
Use $OPENNMS_HOME/bin/jrobin-inspector to took into this/those files to see if they contain valid data.
</Text>
        </Document>
        <Document ID="107">
            <Title>Alarms</Title>
            <Text>Events reduced to Alarms
[edit] Why Alarms
Ever since I started using OpenNMS, I have all wondered what the heck it means to acknowledge an Event. Well, it really doesn't mean anything to me unless the event is something that I acutally cared about. Now, OpenNMS gives you the ability to indicate which events are important and they become alarms. Also, with this ability, you can now reduce these important events to one row in the alarms table with a reduction key tag (&lt;reductionKey>) in the event configuration files. This allows you to decide the granularity of the reduction as you will see in the sample images below (also see: Configuring alarms).
[edit] The new Alarms link
#
[edit] Alarms with new style sheet and sorted by severity
This is much nicer and generally more useful than the raw events.
￼
[edit] Alarms sorted by count
This shows the number of events that were reduced to a single alarm row.
￼
[edit] Un-reduced event list
Click on a count in the alarm listing and and jump to the list of un-reduced events for that alarm.
￼
[edit] See Also
Configuring alarms


Retrieved from "http://www.opennms.org/wiki/Alarms"

</Text>
        </Document>
        <Document ID="49">
            <Title>Active Polling</Title>
        </Document>
        <Document ID="53">
            <Title>JMX Datacollection</Title>
        </Document>
        <Document ID="54">
            <Title>WMI Datacollection</Title>
        </Document>
        <Document ID="112">
            <Title>Data from several nodes</Title>
            <Text>Situation
Assume you are collecting data on several nodes. These information are related to each other. In my example, I am collecting the number of open radius sessions from my radius servers.
The idea is now to show two graphs:
	•	One with a line per server, showing how the load is distributed over the servers
[[1]]
	•	One with an Area showing the total number of concurrent users
[[2]]

[edit] Problem
If you look at the way OpenNMS is storing the data you see that that happens by node. The creation of the graphs happens as well "per node". With the given tools it's not possible to aggregate data sources from several nodes into a single graph.
[edit] Solution
The solution to this problem was described by Tarus on the opennms discussion mailinglist. As he was only sketching out the concept, I will describe in more detail how to get to the intended graphs.
The basic idea is the following: If opennms insists on having all the data sources in one node, we make sure it has. This can be done by using symlinks - the data source is a file. So you can simply add a symlink on the filesystem to make the data source from node A available to opennms in the directory of node B.
Using a real server as a "master" has however the disadvantage that this real server is connected to the "real" world. In the real world servers change names, place, interface or dedication.
But there's no need to use a "real" server as the point of reporting. A node can well be created without having an interface.
So the solution is: Create a virtual node, link all the datasources into it and define your graph. That simple.
[edit] Step by Step
[edit] Creating a "virtual" Node
Idea: Use the "Provisioning Groups" to add a node without interface.
	•	Log in as Administrator to OpenNMS
	•	Select the Admin Menu
	•	Then "Manage Provisioning Groups"
	•	'Enter a name for the new Group', eg "Reporting Nodes" and 'then' click on "Add New Group" (clicking on "Add New Group" without giving a name will throw an ugly error)
	•	Click on the Group Name ("Reporting Nodes")
	•	"Add Node", enter a name for the Node ("App XYZ Reporting") and Save; there's no need to add an interface or anything else
	•	You are done here - click on "done"
	•	Now you should be back in the Provisioning Groups overview
	•	The Nodes in Group / Nodes in DB field for your new group says "1/0"
	•	To import the new nodes, click on "Import"
	•	Reload the page
	•	The Nodes in Group / DB should read 1/1 now - your node is in the DB
	•	You can now search for your node (use the global search function)
	•	'Write down the NodeID' - in my case it's "2790"
The new virtual node should now be in the DB.
[edit] Link the Data Sources
Idea: Create symlinks in the file system to make the data sources available in the virtual node.
	•	Log on to your opennms system
	•	cd into your share/rrd/snmp/ directory
Now you need to find the datasources you want to link. This means that you need to know two things:
The filename of the data source and the NodeIDs of the nodes you want to report on.
In my cases the datasources are named "radusers_eins(zwei,drei).jrb":
/opt/OpenNMS/share/rrd/snmp/2790$ ls -l 
total 521
lrwxrwxrwx 1 root root     20 2008-04-02 19:01 radusers_eins.jrb -> ../2777/radusers.jrb
lrwxrwxrwx 1 root root     20 2008-04-02 19:00 radusers_zwei.jrb -> ../1185/radusers.jrb
lrwxrwxrwx 1 root root     20 2008-04-02 19:01 radusers_drei.jrb -> ../1252/radusers.jrb
The Nodes I retrieve the data from are 2777, 1185 and 1252:
radusers_eins.jrb -> ../2777/radusers.jrb
^^^^^^^^^^^^^           ^^^^^^^^^^^^^     
New ds-name          Real Datasource
Note: Under Windows 2000+ running OpenNMS, Hardlinks can be archived by using the GNU Tools for Win32 using the ln.exe command the same as on a Linux server. GNU Tools for Win32 can be found at http://sourceforge.net/projects/unxutils. Windows 2008 and Vista can create hardlinks using the native MKLINK command which is now included with Windows.
When this step is finished I have
	•	A virtual Node
	•	Symbolically linked data sources in the rrd/snmp/$NODE directory
[edit] Creating a Graph
Idea: Now we use the standard graphing facilities and make the graph.
Not much special anymore, but because creating RRD Graphs is such a major pain, I add the config I did:
Note that the new linked names are referenced in the report columns definition, while the original file names are used in the graphs DEF.
# radius.allusers
report.radius.allusers.name=allusers
report.radius.allusers.columns=radusers_eins,radusers_zwei,radusers_drei
report.radius.allusers.type=nodeSnmp
report.radius.allusers.command=--title="Users per Server" \
    --vertical-label Number \
    DEF:broker={rrd1}:radusers:AVERAGE \
    DEF:broker2={rrd2}:radusers:AVERAGE \
    DEF:broker3={rrd3}:radusers:AVERAGE \
    LINE1:broker#ff0000:"drei" \
    GPRINT:broker:MIN:"Min\\: %8.2lf %s" \
    GPRINT:broker:AVERAGE:"Avg\\: %8.2lf %s" \
    GPRINT:broker:MAX:"Max\\: %8.2lf %s\\n" \
    LINE1:broker2#00ff00:"zwei" \
    GPRINT:broker2:MIN:"Min\\: %8.2lf %s" \
    GPRINT:broker2:AVERAGE:"Avg\\: %8.2lf %s" \
    GPRINT:broker2:MAX:"Max\\: %8.2lf %s\\n" \
    LINE1:broker3#0000ff:"drei            " \
    GPRINT:broker3:MIN:"Min\\: %8.2lf %s" \
    GPRINT:broker3:AVERAGE:"Avg\\: %8.2lf %s" \
    GPRINT:broker3:MAX:"Max\\: %8.2lf %s\\n" \
Where getting three lines was fairly simple, the aggregation took a bit more effort:
# radius.totalusers
report.radius.totalusers.name=totalusers
report.radius.totalusers.columns=radusers_eins,radusers_zwei,radusers_drei
report.radius.totalusers.type=nodeSnmp
report.radius.totalusers.command=--title="Total Concurrent Users" \
    --vertical-label Number \
    DEF:eins={rrd1}:radusers:AVERAGE \
    DEF:zwei={rrd2}:radusers:AVERAGE \
    DEF:drei={rrd3}:radusers:AVERAGE \
    CDEF:totalusers=eins,zwei,drei,+,+ \
    AREA:totalusers#bacaff:"Total Users" \
    GPRINT:totalusers:MIN:"Min  \\: %8.2lf %s" \
    GPRINT:totalusers:AVERAGE:"Avg  \\: %8.2lf %s" \
    GPRINT:totalusers:MAX:"Max  \\: %8.2lf %s\\n" \
For examples of the graphs, look for the links on the top of the page.
'Key Learnings':
	•	{rrdn} (n=1,2..) is related to the Columns (thanks, Karl)
	•	CDEF is a pain to configure but works and is "sort of" logical
[edit] Result
If you have added the new reports (radius.allusers, radius.totalusers) to your list of prefabricated reports they should show now when you look at the resource graphs of your virtual node.
That's it.
'Tags': RRD, Jrobin, graphing, symlinks, node
</Text>
        </Document>
        <Document ID="55">
            <Title>NSClient</Title>
        </Document>
        <Document ID="108">
            <Title>Configuring Alarms</Title>
            <Text>Configuration Details
Alarms are derived from OpenNMS Events with an &lt;alarm-data> element that includes a &lt;reductionKey>. The event definitions live in the XML files that are included via an &lt;event-file> directive in the eventconf.xml file and are formatted as shown here:
[edit] Example
&lt;event>
  &lt;mask>

    &lt;maskelement>
      &lt;mename>id&lt;/mename>
      &lt;mevalue>.1.3.6.1.4.1.3955.2.2.1&lt;/mevalue>
    &lt;/maskelement>

    &lt;maskelement>
      &lt;mename>generic&lt;/mename>
      &lt;mevalue>6&lt;/mevalue>
    &lt;/maskelement>

    &lt;maskelement>
      &lt;mename>specific&lt;/mename>
      &lt;mevalue>1&lt;/mevalue>
    &lt;/maskelement>

  &lt;/mask>
  &lt;uei>uei.opennms.org/vendor/Linksys/traps/linksysConnTrap&lt;/uei>
  &lt;event-label>Linksys Connection Trap&lt;/event-label>
  &lt;descr>&lt;p>This trap signifies that a TCP/UDP connection has been made. &lt;/p>
  &lt;/descr>
  &lt;logmsg dest="logndisplay">&lt;p>Linksys Event: %parm[#1]%.&lt;/p>&lt;/logmsg>
  &lt;severity>Normal&lt;/severity>
  &lt;alarm-data reduction-key="%uei%:%dpname%:%nodeid%" alarm-type="1" auto-clean="true"/>
&lt;/event>
[edit] Getting Started
First off, any event that has a reductionKey assigned, will result in the creation of an alarm which is persisted to the alarms table, if also, the Event does not have the element logmsg dest set to "donotpersist":
&lt;logmsg dest="donotpersist" />
[edit] Alarm Data
The alarm-data element contains the following attributes:
	1.	reduction key (replaceable text)
	2.	alarm type (positive integer)
	3.	auto-clean (boolean)
[edit] Reduction Keys
The following sample reduction-key attribute tells the alarm writer in OpenNMS to parse the UEI and the node ID from the event and store it in the reductionKey column. (Note: This alarm-data element containing the reduction-key attribute is a new format for version 1.3.1. The previous format was an element like: "&lt;reductionKey>")
&lt;alarm-data reduction-key="%uei%:%nodeid%" alarm-type="1" auto-clean="false" />
The alarm writer now uses that key to determine if a previous event with the same reductionKey has already been converted to an alarm and inserted into the alarms table. If it has, it only updates the lastEventTime, lastEventID, and the counter columns. Otherwise, it inserts a new alarm.
[edit] Alarm Types
The alarm-type attribute is added to assist with automations, correlation, and other integration such as the new OSS/J implementation. The current model defines an alarm-type set to "1" to be a problem that has a possible resolution, alarm-type set to "2" to be a resolution event, and alarm-type set to "3" for events that have no possible resolution. Feel free to use this field at your own discression (positiveIntegers only), however, the default integrations make use of these values (i.e. cosmicClear automation).
[edit] Auto Cleaning
As an event is processed into the Alarm model, the auto-clean attribute is used to removed the historical events. All previous events matching the reduction key of the current event will be removed from the DB. This is best used with events that do not have outages, notifications, etc. associated with them. Mainly for just tallying events that can later be used to trigger an action in an automation.
[edit] Mailing List Discussion
Jeff Gehlbach to opennms-users on Wed, 3 Sep 2008 16:56:21 -0400 (EDT) Message-Id: &lt;6F6A91A9-16B6-421B-B773-68EEB99C72F1@opennms.org>
Setting auto-clean="true" in an event's alarm-data annotation causes all old events for an alarm to be deleted from the database when a new event comes in that is reduced under an existing alarm. The use case for this attribute is events that tend to be numerous and for which the alarm counter is as useful as the individual events, such as SNMP authenFailure traps. So normally one sets this only on trouble (alarm- type="1") events. In fact, since Normal-severity resolution (alarm- type="2") alarms are deleted periodically by an automation, setting auto-clean="true" on a resolution event makes no real sense.
So there's what cleaning is about. Clearing is about automatically setting the severity of an alarm to "Cleared" when a resolution (alarm- type="2") is received whose clear-key matches the reduction-key of the trouble (alarm-type="1") alarm. I think you've got a pretty good grasp on clearing, with a couple of caveats:
	•	The "clear-uei" attribute is deprecated and should not be used in new definitions
	•	Clearing is performed by an automation, so its effect is not instantaneous.
 Jeff Gehlbach to opennms-discuss on 2008-10-15 14:28:51 Message-Id: &lt;AF1E72F2-72FB-457B-93B7-49908D8E261D@opennms.org>
When you acknowledge an alarm, you're saying "I acknowledge that this problem is genuine and I am taking ownership of its resolution." Acknowledged alarms never go away, this is by design. There is a default automation that deletes unacknowledged alarms whose severity is "Cleared", so if you want an alarm to go away, it should be cleared and unacknowledged.
Alarms are designed to be self-clearing when possible. For event types that come in "trouble" and "resolution" flavors, we try to define the alarms so that the "resolution" event clears an alarm that was created by the corresponding "trouble" event. Not every event comes in these nice pairs, though, so sometimes it's necessary to clear an alarm manually as you have done.
[edit] Benefits
The great benefits of this new feature are:
	1.	You can now view only those events that actually represent problems
	2.	Immediately see how many of each of these have been received
	3.	Probably view almost all events (now alarms) in one view
Another new feature added to support Alarms is called Automations. Even though Automations provides more potential for your network management processes than just handling alarms, this is a good time to get started with them and see how alarm management can be dramatically enhanced. With the use of Automations, OpenNMS can now provide Event/Alarm correlation, escalation, and better Event/Alarm management.
</Text>
        </Document>
        <Document ID="56">
            <Title>JDBC Datacollection</Title>
        </Document>
        <Document ID="60">
            <Title>OpenNMS Configuration</Title>
        </Document>
        <Document ID="113">
            <Title>RPN Tutorial</Title>
            <Text>rpntutorial
ATT - NEED TO ASK PERMISSION BEFORE INCLUDING 

DESCRIPTION
This tutorial should help you get to grips with RRDtool RPN expressions as seen in CDEF arguments of RRDtool graph.
Reading Comparison Operators
The LT, LE, GT, GE and EQ RPN logic operators are not as tricky as they appear. These operators act on the two values on the stack preceding them (to the left). Read these two values on the stack from left to right inserting the operator in the middle. If the resulting statement is true, then replace the three values from the stack with "1". If the statement if false, replace the three values with "0".
For example, think about "2,1,GT". This RPN expression could be read as "is two greater than one?" The answer to that question is "true". So the three values should be replaced with "1". Thus the RPN expression 2,1,GT evaluates to 1.
Now consider "2,1,LE". This RPN expression could be read as "is two less than or equal to one?". The natural response is "no" and thus the RPN expression 2,1,LE evaluates to 0.
Reading the IF Operator
The IF RPN logic operator can be straightforward also. The key to reading IF operators is to understand that the condition part of the traditional "if X than Y else Z" notation has *already* been evaluated. So the IF operator acts on only one value on the stack: the third value to the left of the IF value. The second value to the left of the IF corresponds to the true ("Y") branch. And the first value to the left of the IF corresponds to the false ("Z") branch. Read the RPN expression "X,Y,Z,IF" from left to right like so: "if X then Y else Z".
For example, consider "1,10,100,IF". It looks bizarre to me. But when I read "if 1 then 10 else 100" it's crystal clear: 1 is true so the answer is 10. Note that only zero is false; all other values are true. "2,20,200,IF" ("if 2 then 20 else 200") evaluates to 20. And "0,1,2,IF" ("if 0 then 1 else 2) evaluates to 2.
Notice that none of the above examples really simulate the whole "if X then Y else Z" statement. This is because computer programmers read this statement as "if Some Condition then Y else Z". So it's important to be able to read IF operators along with the LT, LE, GT, GE and EQ operators.
Some Examples
While compound expressions can look overly complex, they can be considered elegantly simple. To quickly comprehend RPN expressions, you must know the algorithm for evaluating RPN expressions: iterate searches from the left to the right looking for an operator. When it's found, apply that operator by popping the operator and some number of values (and by definition, not operators) off the stack.
For example, the stack "1,2,3,+,+" gets "2,3,+" evaluated (as "2+3") during the first iteration and is replaced by 5. This results in the stack "1,5,+". Finally, "1,5,+" is evaluated resulting in the answer 6. For convenience, it's useful to write this set of operations as:
 1) 1,2,3,+,+    eval is 2,3,+ = 5    result is 1,5,+
 2) 1,5,+        eval is 1,5,+ = 6    result is 6
 3) 6
Let's use that notation to conveniently solve some complex RPN expressions with multiple logic operators:
 1) 20,10,GT,10,20,IF  eval is 20,10,GT = 1     result is 1,10,20,IF
read the eval as pop "20 is greater than 10" so push 1
 2) 1,10,20,IF         eval is 1,10,20,IF = 10  result is 10
read pop "if 1 then 10 else 20" so push 10. Only 10 is left so 10 is the answer.
Let's read a complex RPN expression that also has the traditional multiplication operator:
 1) 128,8,*,7000,GT,7000,128,8,*,IF  eval 128,8,*       result is 1024
 2) 1024   ,7000,GT,7000,128,8,*,IF  eval 1024,7000,GT  result is 0
 3) 0,              7000,128,8,*,IF  eval 128,8,*       result is 1024
 4) 0,              7000,1024,   IF                     result is 1024
Now let's go back to the first example of multiple logic operators, but replace the value 20 with the variable "input":
 1) input,10,GT,10,input,IF  eval is input,10,GT  ( lets call this A )
Read eval as "if input > 10 then true" and replace "input,10,GT" with "A":
 2) A,10,input,IF            eval is A,10,input,IF
read "if A then 10 else input". Now replace A with it's verbose description again and--voila!--you have an easily readable description of the expression:
 if input > 10 then 10 else input
Finally, let's go back to the first most complex example and replace the value 128 with "input":
 1) input,8,*,7000,GT,7000,input,8,*,IF  eval input,8,*     result is A
where A is "input * 8"
 2) A,7000,GT,7000,input,8,*,IF          eval is A,7000,GT  result is B
where B is "if ((input * 8) > 7000) then true"
 3) B,7000,input,8,*,IF                  eval is input,8,*  result is C
where C is "input * 8"
 4) B,7000,C,IF
At last we have a readable decoding of the complex RPN expression with a variable:
 if ((input * 8) > 7000) then 7000 else (input * 8)
Exercises
Exercise 1:
Compute "3,2,*,1,+ and "3,2,1,+,*" by hand. Rewrite them in traditional notation. Explain why they have different answers.
Answer 1:
    3*2+1 = 7 and 3*(2+1) = 9.  These expressions have
    different answers because the altering of the plus and
    times operators alter the order of their evaluation.
Exercise 2:
One may be tempted to shorten the expression
 input,8,*,56000,GT,56000,input,*,8,IF
by removing the redundant use of "input,8,*" like so:
 input,56000,GT,56000,input,IF,8,*
Use traditional notation to show these expressions are not the same. Write an expression that's equivalent to the first expression, but uses the LE and DIV operators.
Answer 2:
    if (input &lt;= 56000/8 ) { input*8 } else { 56000 }
    input,56000,8,DIV,LT,input,8,*,56000,IF
Exercise 3:
Briefly explain why traditional mathematic notation requires the use of parentheses. Explain why RPN notation does not require the use of parentheses.
Answer 3:
    Traditional mathematic expressions are evaluated by
    doing multiplication and division first, then addition and
    subtraction.  Parentheses are used to force the evaluation of
    addition before multiplication (etc).  RPN does not require
    parentheses because the ordering of objects on the stack
    can force the evaluation of addition before multiplication.
Exercise 4:
Explain why it was desirable for the RRDtool developers to implement RPN notation instead of traditional mathematical notation.
Answer 4:
    The algorithm that implements traditional mathematical
    notation is more complex then algorithm used for RPN.
    So implementing RPN allowed Tobias Oetiker to write less
    code!  (The code is also less complex and therefore less
    likely to have bugs.)

AUTHOR
Steve Rader &lt;rader@wiscnet.net>
</Text>
        </Document>
        <Document ID="109">
            <Title>Discovery</Title>
            <Text>Introduction
[edit] Purpose
This How-To is one in a series designed to serve as a reference for getting started with OpenNMS. Eventually, these documents will cover everything necessary to get OpenNMS installed and running in your environment.
[edit] Copyright
Content is available under a Creative Commons Attribution-NonCommercial-ShareAlike2.5 License.
[edit] Corrections and Omissions
Please submit any corrections and omissions to the author.
[edit] Overview
OpenNMS is an enterprise-grade network management platform developed under the open-source model. Unlike traditional network management products which are very focused on network elements such as interfaces on switches and routers, OpenNMS focuses on the services network resources provide: web pages, database access, DNS, DHCP, etc. (although information on network elements is also available).
Since the majority of network services are provided using the TCP/IP protocol, OpenNMS is very IP-centric. The basic monitored "element" is called an "interface", and an interface is uniquely identified by an IP address. Services are mapped to interfaces, and if a number of interfaces are discovered to be on the same device (either via SNMP or SMB) then they may be grouped together as a "node".
Discovery in OpenNMS consists of two parts: discovering an IP address to monitor and then discovering the services supported by that IP address. The first part is much simpler than the second.
[edit] Discovery
[edit] Discovery User Interface
The most straight forward way of initiating Discovery is through the web interface. Navigate to Admin > Configure Discovery.
There you are presented with 2 options.
	1.	Specifics - which allows you to enter IP addresses of known individual interfaces.
	2.	Include URLs - which allows you to specify a file containing IP addresses to be included in discovery.
	3.	Include Ranges - which instructs OpenNMS to scan a range of IP addresses for active interfaces.
Nodes will appear in the Node List as they are discovered.
[edit] The Discovery Configuration File
Discovery in OpenNMS is controlled by the discovery-configuration.xml file (located in the /opt/OpenNMS/etc directory.
Let's look at that file:
&lt;discovery-configuration threads="1" packets-per-second="1"
                         initial-sleep-time="300000"
                         restart-sleep-time="86400000"
                         retries="3" timeout="800">

  &lt;include-range retries="2" timeout="3000">
    &lt;begin>192.168.0.1&lt;/begin>
    &lt;end>192.168.0.254&lt;/end>
  &lt;/include-range>

  &lt;include-url>file:/opt/OpenNMS/etc/include&lt;/include-url>
&lt;/discovery-configuration>
Now, all this file controls is a process that will send an ICMP "ping" to a particular set of IP addresses. If there is a response within the timeout, a "new suspect" event is generated. Otherwise, the IP address is ignored.
The global discovery attributes are:
threads 
This is the number of threads that will be used for discovery. By default this is set to 1.
packets-per-second 
This is the number of ICMP packets that will be generated each second. The default is 1. Note that there is a relationship between the packets-per-second and the number of threads. If your network has an average latency of 500ms, then setting packets-per-second to 2 would double the speed at which NewSuspect messages were created. But if there is only one thread available, setting this number to 3 would have little effect - the single thread would be processing as many packets as it could as fast as it could.
initial-sleep-time 
This is the time, in milliseconds, before the discovery process will commence after OpenNMS is started (by default 5 minutes). This delay is put in place to allow the product to fully start before generating new events.
restart-sleep-time 
Once the discovery process has completed, this is the time, in milliseconds, before it will start again. By default, the process will repeat 24 hours after the last discovery run has completed.
timeout 
this is the amount of time, in milliseconds, that the discovery process will wait for a response from a given IP address before deciding that there is nothing there. This can be overridden later in the file.
retries 
this is the number of attempts that will be made to query a given IP address before deciding that there is nothing there. This can be overridden later in the file.
Once the defaults are in place (defaults meaning the global values that will be used if they are not overridden in the tags below), the only thing left to tell the discovery process is which IP addresses to try. This is controlled by four different tags:
specific 
specify a IP address to be discovered. Multiple specific tags can be used.
&lt;specific>ip-address&lt;/specific>
Where ip-address is the address you want discovered. Note the lack of spaces between the tags.
include-range 
Specify a range of IP addresses to be discovered. Multiple include-range tags can be used.
&lt;include-range>
  &lt;begin>start-ip-address&lt;/begin>
  &lt;end>end-ip-address&lt;/end>
&lt;/include-range>
Where start-ip-address is the beginning of a range to be scanned and end-ip-address is the end of that range.
exclude-range 
Specify a range of IP address to be excluded from discovery.
&lt;exclude-range>
  &lt;begin>start-ip-address&lt;/begin>
  &lt;end>end-ip-address&lt;/end>
&lt;/exclude-range>
Where start-ip-address is the beginning of a range to be excluded and end-ip-address is the end of that range. Note that the exclude-range tag will only override addresses in an include-range. It will not override specific IP addresses or addresses included in a file. There is no "specific" version of the exclude tag - if you want to exclude a specific IP address use an exclude-range where the beginning and ending IP addresses are the same.
include-url 
Specify a file containing IP addresses to be included in discovery.
&lt;include-url>file:filename&lt;/include-url>
Where filename is the full path to a text file listing IP addresses, one to a line. Comments can be imbedded in this file. Any line that begins with a "#" character will be ignored, as will the remainder of any line that includes a space followed by "#".
All tags are optional and unbounded (you can have as many as you wish).
[edit] Another Way to Discover Interfaces
Now that the discovery configuration file has been explained, there are two short-comings that need to be pointed out. First, any changes to this file, like most of the configuration files within OpenNMS, requires that OpenNMS be restarted. Second, what if you want to discover a service, such as a web server, on a device you cannot ping?
Remember that all the discover process does is generate a newSuspect event. Included in the /opt/OpenNMS/bin directory is a Perl script called send-event.pl. You can use this script to generate an internal NewSuspect event - bypassing the discovery process altogether. Combined with a script, you could generate any number of NewSuspect events (just make sure that the IP address really does have some services that can be monitored by OpenNMS. Otherwise, you will have an interface in the system with no services associated with it).
The format of the send-event.pl is as follows:
/opt/opennms/bin/send-event.pl --interface ip-address uei.opennms.org/internal/discovery/newSuspect
 Replace ip-address with the address you want discovered.
For csv with FQDN as hosts location
cat fqdnlist.csv | awk -F "," '{print "host " $1}' | sh |awk '{print "./send-event.pl --interface " $4 " uei.opennms.org/internal/discovery/newSuspect"}'
For csv file with IP for host location
cat ipaddresss.csv |gawk -F "," '{print "host " $1 }' |sh |gawk '{print "./send-event.pl --interface " $1 " uei.opennms.org/internal/discovery/newSuspect"}'
note: use same format as exported assets.
Depending on what Perl modules you have installed, you may get an error running this script (such as a complaint about Getopt::Mixed). To automatically add the necessary modules, try:
perl -MCPAN -e 'install mod_name'
Replace mod_name with the name of the missing module.
Or, if you hate use CPAN (there is some problems, the CPAN can cose to lib's), you can just use rpm pakage
wget ftp://ftp.debian.nl/disk1/redhat-contrib/libc5/i386/Getopt-Mixed.pm-1.008-4.i386.rpm

rpm -Uv Getopt-Mixed.pm-1.008-4.i386.rpm 
(END)
[edit] Logs
You can watch the discovery process by examining the discovery.log file in the /opt/opennms/logs/daemon directory.
[edit] Capabilities
Okay, if the discovery process just generates NewSuspect events, what does all the work? This would be the capabilities daemon, capsd. capsd is responsible for discovering all the services to be monitored, such as httpd, DNS, etc., as well as if any collectors are present (at the time this is only SNMP).
The capsd process is controlled by the capsd-configuration.xml file. This file consists of some basic parameters and a collection of "protocols" to be tested. If the protocol is not in the file, then OpenNMS will not discover it.
On a restart of OpenNMS it schedules the scans based on the last capsd scan timestamps in the DB and the configured rescan interval.
[edit] Process Parameters for capsd
The first few lines of the capsd-configuration.xml file control how capsd will behave.
&lt;capsd-configuration rescan-frequency="86400000"
                     initial-sleep-time="300000"
                     management-policy="managed"
                     max-suspect-thread-pool-size="6"
                     max-rescan-thread-pool-size="3"
                     abort-protocol-scans-if-no-route="false">
	•	rescan-frequency  capsd will continue to check each interface to see if new services have been added. The frequency of these rescans is controlled by this parameter. The default value is 24 hours in milliseconds.  initial-sleep-time  like the discovery process, capsd will sleep for a certain amount of time after OpenNMS starts. The default value is 5 minutes in milliseconds.  management-policy  this parameter controls the default behavior of capsd. If it is set to "managed", then all IP addresses in NewSuspect events will be scanned, unless included in an "unmanaged" range defined at the end of this file. If this parameter is set to "unmanaged", then all NewSuspect events will be ignored unless the IP address in the event is expressly included in a "managed" range (also defined at the end of this file).  max-suspect-thread-pool-size  This value determines how many threads will be created to perform capability scans on IP addresses supplied by NewSuspect events. Increasing this value will make the initial discovery move more quickly at the cost of more system resources.  max-rescan-thread-pool-size  This value determines how many threads will be created to perform capability scans on interfaces that have already been discovered. Rescans are either automatically scheduled (see rescan-frequency) or generated ad hoc through the Web UI.  abort-protocol-scans-if-no-route  This is an extremely important parameter for modifying the behavior of capsd. When attempting to connect to a specific port to test for a service, it is possible to receive a "no route to host" exception. In theory, this is because the host is not reachable, but in practice any number of things, such as firewalls, can cause this error. If this parameter is set to "false", these "no route to host" messages are ignored. But if it is set to "true", then capsd will stop checking for additional services. This can greatly improve the speed of discovery if the capsd file has been "tuned" (discussed below).   [edit] Protocols  OpenNMS tests the existence of a particular network service through the use of "protocols". At the most basic, this could be a connection to a TCP port to test for a particular banner, but there are also special classes for a variety of other protocols. The current protocols supported out of the box are: 
	◦	Citrix
	◦	DHCP
	◦	DNS
	◦	Domino IIOP
	◦	FTP
	◦	General Purpose (script based)
	◦	HTTP
	◦	HTTPS
	◦	ICMP
	◦	IMAP
	◦	JBOSS
	◦	JDBC
	◦	JDBC Stored Procedure
	◦	JSR160
	◦	K5
	◦	LDAP
	◦	Microsoft Exchange
	◦	MX4J
	◦	Notes HTTP
	◦	NSClient (Nagios Agent)
	◦	NRPE (Nagios Remote Plugin Executor)
	◦	NTP
	◦	POP3
	◦	Radius
	◦	SMB
	◦	SMTP
	◦	SNMP
	◦	SSH
	◦	TCP
	◦	Windows Services (SNMP-based)
	•	When a newSuspect event is received by capsd and the management policy for the IP address in that event is "managed," the capsd process will work its way through this file testing one protocol after another, in the order they are listed in this file. The first protocol to be tested is ICMP:  &lt;protocol-plugin protocol="ICMP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.IcmpPlugin"
	•	                 scan="on">
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 Each protocol starts with a protocol-plugin tag. This tag has four attributes:  protocol  This is the name of the protocol.  class-name  This defines the protocol class that will be used to test for the service.  scan  Capsd scans can be turned "on" or "off" per protocol with this attribute.  user-defined  In versions prior to something like 1.6.9 and 1.7.9 there has been a tag user-defined="value" (see example below) which could have had the values true or false. This tag obviously was never used in the code and has now been removed.   In addition, each protocol-plugin can have a number of properties defined by a key and a value. The possible properties for each protocol will be discussed in the next section, although almost all include a timeout value and the number of times to try to make a connection.  There is a little-known feature available in capsd. This is the ability to configure each protocol based on IP addresses. This is through the protocol-configuration tag. The best way to describe this is through an example. Let's take the ICMP configuration from above and modify it:  &lt;protocol-plugin protocol="ICMP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.IcmpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;protocol-configuration scan="on" user-defined="false">
	•	    &lt;range begin="192.168.10.0" end="192.168.10.254"/>
	•	    &lt;property key="timeout" value="4000"/>
	•	    &lt;property key="retry" value="3"/>
	•	  &lt;/protocol-configuration>
	•	
	•	  &lt;protocol-configuration scan="off" user-defined="false">
	•	    &lt;range begin="192.168.20.0" end="192.168.20.254"/>
	•	  &lt;/protocol-configuration>
	•	
	•	  &lt;protocol-configuration scan="enable" user-defined="false">
	•	    &lt;specific>192.168.30.1&lt;/specific>
	•	  &lt;/protocol-configuration>
	•	
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 There are three protocol-configuration tags that have been added. Suppose you have one subnet that is over a slow link and it may take a little longer for an ICMP request to be returned. In the first example, the 192.168.10.0 subnet is allowed a 4 second response instead of the default of 2, and three retries.  Suppose you have another segment that you just don't want to scan for ICMP. In the second example, scan is set to "off", and that range will not be tested for ICMP.  Finally, the third example demonstrates setting scan to "enable", which forces the protocol to be associated with the device without testing for it. This is useful if you know the protocol is going to exist on a device, but for some reason it has not been added yet or it is down. Note that "enable" only works for protocol-configuration tags and not the main plugin tag.  [edit] Plugin Properties  The following table shows all of the property tags that are available for each protocol plugin. The default values are the ones hard-coded into the plugin itself, not the defaults in the configuration file.  [edit] Citrix  port  The port to connect to. Default is "1494".  timeout  The time in milliseconds to wait for a response. The default is "5000".  retries  The number of attempts made to detect the service. The default is "0".   [edit] DHCP  port  The port to connect to. Default is "67".  timeout  The time in milliseconds to wait for a response. Default is "3000".  retries  The number of attempts made to detect the service. Default is "3".   [edit] DNS  port  The port to connect to. Default is "53".  timeout  The time in milliseconds to wait for a response. Default is "3000".  retries  The number of attempts made to detect the service. Default is "3".  lookup  The default host name to attempt to resolve. Default is "localhost".   [edit] Domino IIOP  ports  The port to connect to. Default is "63148".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  IOR port  Port to look for the IOR via HTTP. Default is "80".   [edit] FTP  port  The port to connect to. Default is "21".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] HTTP  ports  The port to connect to (can be more than one, separated by a comma). Default is "80,8080,8000".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  max-ret-code (1.3.2 and later)  The highest numerical HTTP response code that will be taken to indicate success. Default is 399 if a URL is specified, 600 if not.  check-return-code (1.3.2 and later)  Boolean indicating whether or not to check the HTTP response code for success/failure. Default is "true". Note that illegal return codes (99 &lt;= code >= 600, per RFC1945) still indicate failure.   [edit] HTTPS  ports  The port to connect to (can be more than one, separated by a comma). Default is "443".  timeout  The time in milliseconds to wait for a response. Default is "30000".  retries  The number of attempts made to detect the service. Default is "1".  max-ret-code (1.3.2 and later)  The highest numerical HTTP response code that will be taken to indicate success. Default is 399 if a URL is specified, 600 if not.  check-return-code (1.3.2 and later)  Boolean indicating whether or not to check the HTTP response code for success/failure. Default is "true". Note that illegal return codes (99 &lt;= code >= 600, per RFC1945) still indicate failure.   [edit] ICMP  timeout  The time in milliseconds to wait for a response. Default is "800".  retries  The number of attempts made to detect the service. Default is "2".   [edit] IMAP  port  The port to connect to. Default is "143".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] JBOSS factory  The method of connecting to JMX. Default is "RMI". The other acceptable value is "HTTP".  timeout  The time in milliseconds to wait for a response. Default is "3000".  version  The version of JBOSS being detected. Default is "4".  port  The TCP port to use for the connection. Default is "1099".   [edit] JDBC Unlike nearly all the other plugins, the JDBC plugin is *highly* unlikely to work with the default configuration values. You will have to configure user. password, url and driver to match your database before this will work.  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  user  The username with which to authenticate to the database. Default is "sa"  password  The password corresponding to the username. Default is blank  url  The url of the database (JDBC url foramt). Default is "jdbc:sybase:Tds:OPENNMS_JDBC_HOSTNAME/tempdb"  driver  The JDBC driver class to create the connection from. Default is "com.sybase.jdbc2.jdbc.SybDriver"  host  The host the database lives on. Default is "OPENNMS_JDBC_HOSTNAME"   [edit] JDBC Stored Procedure Configuration is as for the JDBC plugin, except there is an additional parameter to define the stored procedure to run. Caveats regarding configuration of the JDBC plugin apply here also. The additional parameter:  stored-procedure  The name of the stored procedure to run after connecting to the database. Default is "isRunning". The stored procedure must have a single output parameter of type java.sql.Types.BIT. The actual return value is discarded   [edit] JSR160 timeout  The time in milliseconds to wait for a response. Default is "5000".   [edit] LDAP  port  The TCP port on which to look for the LDAP service. Default is "389".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] Microsoft Exchange  timeout  The port to connect to. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0"  pop3 port  The port to look for the POP3 protocol. Default is "110".  imap port  The port to look for the IMAP protocol. Default is "143".  mapi port  The port to look for the MAPI protocol. This port/service is used by Exchange for doing RPC over HTTP. Default is "593".   [edit] MX4J timeout  The time in milliseconds to wait for a response. Default is "5000".   [edit] Notes HTTP ports  The port to connect to (can be more than one, separated by a comma). Looks for the string "Notes" in the banner. Default is "80,8080,8000".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] NRPE command  The command to send to the NRPE agent. Default is the NRPE Hello command ("_NRPE_CHECK")  port  The port the NRPE agent is listening on. Default is "5666"  padding  The padding to use in the packet. Default is 2  timeout  The time in milliseconds to wait for a response. Default is "5000".  retry  The number of attempts made to detect the service. Default is "0".  usessl (available from OpenNMS 1.3.10) Whether to use NRPE over SSL. Default is "false". Set to "true" to enable.   [edit] NSClient command  The command to send to the NSClient agent. Default is the client version check ("1").  port  The port on which the agent is listening. Default is "1248"  parameter  A parameter to send along with the command. Default is null  criticalPercent  If the command sent returns a value which can be compared, this value is the comparison value for a critical level. Default is "0"  warningPercent  If the command sent returns a value which can be compared, this value is the comparison value for a warning level. Default is "0"  password  The password needed to connect to the agent. Default is "None"  timeout  The time in milliseconds to wait for a response. Default is "5000".  retry  The number of attempts made to detect the service. Default is "0".   [edit] NTP port  The port to connect to. Default is "123".  timeout  The time in milliseconds to wait for a response. Default is "3000".  retries  The number of attempts made to detect the service. Default is "3".   [edit] POP3  port  The port to connect to. Default is "110".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] Radius authport  The port the radius daemon uses for authentication. Default is 1812  acctport  The port the radius daemon uses for accounting. Default is 1813  authtype  The type of authentication the radius daemon requires. Default is "pap"  user  A username that can be used to test authentication. Default is "OpenNMS"  password  A corresponding password that can be used to test authentication. Default is "OpenNMS"  secret  The shared secret with the radius daemon. Default is "secret"  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] SMB  No properties for SMB plugin.  [edit] SMTP  port  The port to connect to. Default is "25".  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".   [edit] SNMP  port  The port to connect to. Default is "161".  timeout  The time in milliseconds to wait for a response. Default is null.  retries  The number of attempts made to detect the service. Default is null.  force version  The protocol version (SNMPv1 or SNMPv2) to use to check for the service. Default is null.  vbname  The OID to query. Default is ".1.3.6.1.2.1.1.2" (this is SNMPv2-MIB::sysObjectID.0).  vbvalue  The (optional) value to check for if the OID returns one. Default is null.   [edit] SSH timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  port  The port the ssh daemon is listening on. Default is "22".  match  A regular expression to check for in the response from the SSH server. Default is null  banner  If match is not defined, another regular expression to check for in the response. Default is null   [edit] TCP  port  The port to connect to. Default is null.  timeout  The time in milliseconds to wait for a response. Default is "5000".  retries  The number of attempts made to detect the service. Default is "0".  banner  Check the "banner" string against the string that is returned if the connection is successful. Default is null.   [edit] Windows Services (Win32ServicePlugin)  service-name  The name of the Windows service you are interested in, e.g. "Task Scheduler". Default is "Server".   This plugin extends the SnmpPlugin and therefore also accepts all of its properties.  [edit] Mapping Protocol Plugins to Services  Note that the protocol plugins represent the code that is used to determine if a particular service exists. It is not the service itself. The capsd-configuration.xml file is where the services are actually defined.  For example, look at the HTTP service:  &lt;protocol-plugin protocol="HTTP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.HttpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="ports" value="80"/>
	•	  &lt;property key="timeout" value="3000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 This service will use the HTTP plugin to check for a service on port 80 and create a service called "HTTP". With a simple change of port number, you can create a new service:  &lt;protocol-plugin protocol="HTTP-8080"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.HttpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="ports" value="8080"/>
	•	  &lt;property key="timeout" value="3000"/>
	•	  &lt;property key="retry" value="2"/>
	•	&lt;/protocol-plugin>
	•	 Same protocol plugin, but a completely different service as far as OpenNMS is concerned. In some cases, mainly with HTTP, you can check multiple ports. If you do this, then the service will be considered to exist if a valid response is received on any or all of the ports tested.  One of the more powerful plugins is the TCP plugin. Here it is used to test for the existence of secure shell:  &lt;protocol-plugin protocol="SSH"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.TcpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="banner" value="SSH"/>
	•	  &lt;property key="port" value="22"/>
	•	  &lt;property key="timeout" value="3000"/>
	•	  &lt;property key="retry" value="3"/>
	•	&lt;/protocol-plugin>
	•	 This will connect to port 22 and look for the string "SSH" to be returned. By using this banner check you could create different services for different version of software, such as Oracle7 versus Oracle8, as long as the information was included in the banner (to check the banner, you can use telnet ip-address port). Currently, the match is strictly a substring search. In future versions regular expression may be allowed.  [edit] Server Message Block (SMB)  SMB is used by Windows servers to share files, similar to NFS. OpenNMS does not poll either SMB or NFS, but it can use some of the information provided by SMB to name nodes and group interfaces into nodes. If SMB is discovered on a device, it will be noted on the node page. You can allow OpenNMS to "log in" to an SMB share using the following tag:  &lt;smb-config>
	•	  &lt;smb-auth user="guest" password="guest" type="domain">WORKGROUP&lt;/smb-auth>
	•	&lt;/smb-config>
	•	 Here you can enter in a valid username, password and domain for OpenNMS to use when trying to connect to an interface.  [edit] Management Policies  As mentioned in the beginning of this section, the default management policy is "managed", which means that capsd will attempt a services scan on all interfaces in newSuspect events. This can be overridden with the ip-management tag. From the default capsd-configuration.xml configuration file:  &lt;ip-management policy="managed">
	•	  &lt;range begin="192.168.0.0" end="192.168.0.255"/>
	•	  &lt;include-url>file:/opt/OpenNMS/etc/include&lt;/include-url>
	•	&lt;/ip-management>
	•	
	•	&lt;ip-management policy="unmanaged">
	•	  &lt;specific>0.0.0.0&lt;/specific>
	•	  &lt;range begin="127.0.0.0" end="127.255.255.255"/>
	•	&lt;/ip-management>
	•	 This tag has a policy attribute which can be either managed or unmanaged. Then you can define ranges, specific IP addresses and files as needed. Note that the "managed" example is used specifically as an example: since the default policy is "managed" it is not needed.  [edit] SNMP  The SNMP protocol is a special case. While most of the other services will eventually be polled, the SNMP service is used to collect data. Let's look at its definition in the configuration file:  &lt;protocol-plugin protocol="SNMP"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.SnmpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="force version" value="SNMPv1"/>
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="3"/>
	•	&lt;/protocol-plugin>
	•	 Note the force version property. Since SNMP version 2 agents will respond to SNMP version 1 requests, this test will find both agents. This property has nothing to do with how the data will be collected. The SNMP collector automatically checks for SNMPv2 and will use GET-BULK commands to retrieve the data (unless overridden in the snmp-config.xml file). But if you wanted to manage a service called "SNMPv2" you could create one with:  &lt;protocol-plugin protocol="SNMPv2"
	•	                 class-name="org.opennms.netmgt.capsd.plugins.SnmpPlugin"
	•	                 scan="on" user-defined="false">
	•	  &lt;property key="force version" value="SNMPv2"/>
	•	  &lt;property key="timeout" value="2000"/>
	•	  &lt;property key="retry" value="3"/>
	•	&lt;/protocol-plugin>
	•	 Note that the "SNMPv2" that existed in early 0.9 is no longer checked by default.  [edit] The snmp-config.xml File  The parameters used to connect with SNMP agents are defined in the snmp-config.xml file. Here is an example:  &lt;snmp-config retry="3" timeout="800"
	•	             read-community="public" write-community="private">
	•	  &lt;definition version="v2c">
	•	    &lt;specific>192.168.0.5&lt;/specific>
	•	  &lt;/definition>
	•	
	•	  &lt;definition retry="4" timeout="2000">
	•	    &lt;range begin="192.168.1.1" end="192.168.1.254"/>
	•	    &lt;range begin="192.168.3.1" end="192.168.3.254"/>
	•	  &lt;/definition>
	•	
	•	  &lt;definition read-community="bubba" write-community="zeke">
	•	    &lt;range begin="192.168.2.1" end="192.168.2.254"/>
	•	  &lt;/definition>
	•	
	•	  &lt;definition port="1161">
	•	    &lt;specific>192.168.5.50&lt;/specific>
	•	  &lt;/definition>
	•	&lt;/snmp-config>
	•	 The attributes for the snmp-config tag are as follows:  retry  The number of attempts that will be made to connect to the SNMP agent.  timeout  The amount of time, in milliseconds, that OpenNMS will wait for a response from the agent.  read-community  The default "read" community string for SNMP queries.  write-community  The default "write" community string for SNMP queries. Note that this is for future development - OpenNMS does not perform SNMP "sets" at the moment.   All of the global parameters can be overridden with definition tags. These new SNMP definitions can apply to ranges or specific IP addresses. In addition, there are two other attributes available:  port  This overrides the default port of 161.  version  Here you can force either SNMP version 1 "v1" or version 2c "v2c".   [edit] capsd and SNMP  When testing SNMP, capsd makes an attempt to receive the sysObjectID for the device using the community string and port defined in snmp-config.xml. If this succeeds, the SNMP protocol is marked as "true" for this IP address. Note that it takes the first valid match in snmp-config.xml for that IP address, something to look for if the address is included in multiple ranges.  Once all of the protocols have been tested, if SNMP is true for this IP address, more tests are performed by capsd.  First, three threads are generated three SNMP requests are made to collect the data from the system tree, the ipAddrTable and ifTable.  If, for some reason, the ipAddrTable or ifTable are unavailable, the process stops (but the SNMP system data may show up on the node page - this happens a lot with UC-Davis SNMP agents where only the system tree is available to a query using the "public" community string).  Second, all of the sub-target IP addresses in the ipAddrTable are run through the capsd capabilities scan. Note that this is regardless of how management is configured in the configuration file. This only happens on the initial scan and on forced rescans. On normal rescans (by default, every 24 hours), IP addresses that are "unmanaged" in capsd are not polled.  Third, every IP address in the ipAddrTable that supports SNMP is tested to see if it maps to a valid ifIndex in the ifTable. If this is true, the IP address is marked as a secondary SNMP interface and is a contender for becoming the primary SNMP interface.  Finally, all secondary SNMP interfaces are tested to see if they match a valid package in the collectd-configuration file. If more than one valid IP address meets all three criteria (supports SNMP, has a valid ifIndex and is included in a collection package), then the lowest IP address is marked as primary. All SNMP data collection is performed via the primary SNMP interface.  (Note: in the future we will have the ability to change to a secondary SNMP interface should the primary become unavailable).  When the capsd testing process is complete, events are generated, including NodeGainedService events.  SNMP data collection is covered in another How-To (Data Collection Configuration How-To).  [edit] Conclusion  It is hoped that this How-To has proved useful. Please direct corrections and comments to the author.
</Text>
        </Document>
        <Document ID="57">
            <Title>Presenting Information</Title>
        </Document>
        <Document ID="61">
            <Title>Backup &amp; Restore</Title>
            <Text>Database
Always use pg_dump/pg_restore with the -Fc option when backing up and restoring! It is much smarter than a "raw SQL" dump.
Backing Up 
pg_dump -U opennms -Fc -f /tmp/opennms.pgsql.gz opennms
Restoring 
createdb -U opennms opennms
pg_restore -U opennms -Fc -d opennms /tmp/opennms.pgsql.gz
If you're using PostgreSQL 8.4 or higher, when you restore, you can do it with multiple threads, to speed up the process, by adding the -j flag, eg, pg_restore -U opennms -Fc -j4 -d opennms /tmp/opennms.pgsql.gz.
</Text>
        </Document>
        <Document ID="62">
            <Title>Asset Management</Title>
        </Document>
        <Document ID="114">
            <Title>IPLIKE</Title>
            <Text>how iplike works
Iplike is a stored procedure which has to be installed within the postgres database. There are two different implementations of iplike: one implementation is realized with the postgres plpgsql language, the other version is a compiled C function (within a shared library) which normally should be installed into the postgres opennms db during installation. The performance of the compiled C function is much better than the performance of the plpgsql-version. Having the plpgsql version running may lead to major performance problems if you have a lot of nodes in the database and if you use a lot of IPLIKE statements in your filter rules.
You can check which version is installed in your db with
 psql -U postgres -h localhost -d opennms -c '\df+ iplike' | head
The output should look like
                                                  List of functions
  Schema |  Name  | Result data type | Argument data types | Volatility |  Owner  | Language | Source code | Description 
 --------+--------+------------------+---------------------+------------+---------+----------+-------------+-------------
  public | iplike | boolean          | text, text          | volatile   | opennms | c        | iplike      | 
Check the value of the "Language" column. If it's "plpgsql", then you're using the plpgsql version of this stored procedure. This may happen due to wrong installation precedence, version problems or if you have dropped the database to start all over. Look for a script named install_iplike.sh, it should reside in /usr/sbin or /usr/local/sbin and start the script:
 /usr/local/sbin/install_iplike.sh
Check again which version is installed. If you get the following error during install_iplike.sh
 ERROR:  incompatible library "/usr/local/lib/iplike.so": missing magic block
 HINT:  Extension libraries are required to use the PG_MODULE_MAGIC macro.
then your version of the compiled C function is not compatible with your postgres version. You probably have to get the source code for iplike from the repository and compile it yourself.
[edit] compiling iplike
download package for iplike &amp; unzip / untar it in your installation directory
cd to your installation directory
./configure
make
make install
If you're installing on Postgres 9.0, the directory may have moved. On CentOS 5.5 you will have to modify the configure line to look like this.
./configure --with-pgsql=/usr/pgsql-9.0/bin/pg_config
If this does not work, you will have to find pg_config and edit the command above accordingly.
look within the last lines of messages from make install where the script install_iplike.sh went to
start postgres database
run install_iplike.sh 
check if iplike is installed into postgres-db like described above
If you get following error message from the ./configure process
configure: error: PostgreSQL is required
than you have to install the postgres-devel-... package befor configuring
[edit] compiling iplike on FreeBSD
IPLIKE is available via ports at databases/iplike If you haven't done so already, update your ports &amp; 
	1.	cd /usr/ports/databases/iplike
	2.	make install clean
[edit] IPLIKE Binaries
IPLIKE binaries are available for most platforms now, as part of that platform's packaging system (Fink on Mac OSX, Debian or RPM packages on Linux, etc.)
[edit] IPLIKE on Windows
This will walk you through setting up IPLIKE on Windows.
	1.	Download the IPLIKE DLL for your PostgreSQL version from SVN.
	2.	Copy the file to your PostgreSQL lib directory. (ie, C:\Program Files\PostgreSQL\lib)
	3.	Start pgAdmin and connect to your database.
	4.	In the menu, click Tools -> Query Tool.
	5.	Put the following line in the top window (replace 'IPLIKE-8.3.dll' with the relevant DLL name): CREATE OR REPLACE FUNCTION IPLIKE(text,text) RETURNS bool AS 'IPLIKE-8.3.dll' LANGUAGE 'c' WITH(isttrict)
	6.	In the menu, click Query -> Execute.
[edit] Version History/Availability
	•	This feature was added in version 1.0
	•	This feature was enhanced or modified in version 1.3.3
	•	This feature was enhanced or modified in version 1.9.5
</Text>
        </Document>
        <Document ID="59">
            <Title>System Configuration</Title>
        </Document>
        <Document ID="63">
            <Title>Asset Management</Title>
        </Document>
        <Document ID="64">
            <Title>Graphical Representation</Title>
        </Document>
        <Document ID="115">
            <Title>Popular Discussions</Title>
            <Text>Why is it Java?
Why is it using RRD and not a RDBMS for storing graph data?
Why is there so much XML?
Why do I need to restart OpenNMS for some configuration changes and for others not?
</Text>
        </Document>
        <Document ID="65">
            <Title>Workflow Integration</Title>
        </Document>
        <Document ID="120">
            <Title>Controlling daemons</Title>
            <Text>Controlling daemons
[edit] Startup and Shutdown
The daemons are controlled at startup and shutdown of the main OpenNMS JVM by the service-configuration.xml configuration file (this also controls how status information, if any, is gathered for the daemons).
[edit] Remote control with JMX
The daemons can also be controlled remotely with JMX once the main OpenNMS JVM is running. This is used by the opennms.sh startup/shutdown script to tell OpenNMS to begin shutting down and to get status information. It can also be used to restart individual services after making a configuration change. Here is an example of restarting the RTC daemon (based on a post by User:Tarus to the discuss list on 2006-01-05):
  wget --proxy=off -O /dev/null "http://manager:manager@localhost:8181/invoke?objectname=OpenNMS%3AName%3DRtcd&amp;operation=stop"
  wget --proxy=off -O /dev/null "http://manager:manager@localhost:8181/invoke?objectname=OpenNMS%3AName%3DRtcd&amp;operation=init"
  wget --proxy=off -O /dev/null "http://manager:manager@localhost:8181/invoke?objectname=OpenNMS%3AName%3DRtcd&amp;operation=start"
</Text>
        </Document>
        <Document ID="116">
            <Title>Managing Configuration</Title>
            <Text>OpenNMS's configuration is very powerful, but with that unfortunately comes a level of complexity that can make it difficult to manage upgrades. One of the easiest ways to mitigate this is by using git to manage your configuration upgrades.
Contents
[hide]
	•	1 Getting Started
	◦	1.1 Start with a Pristine etc Directory
	◦	1.2 Initialize Git
	◦	1.3 Create Your Branch
	◦	1.4 Make Your Changes
	•	2 Doing an Upgrade
	◦	2.1 Stop OpenNMS
	◦	2.2 Make Sure All Changes are Committed
	◦	2.3 Switch to the Pristine Branch
	◦	2.4 Upgrade OpenNMS
	◦	2.5 Switch to Your Modified Branch
	◦	2.6 Apply Changes from Master
	◦	2.7 Check for Other Changes
	◦	2.8 Start OpenNMS
[edit] Getting Started
First, if you're not familiar with Git, you may want to read our developing with Git page, as well as the Git tutorial.
If you have an existing OpenNMS install, the easiest way to get started is to make sure you have a version of OpenNMS which provides the "etc-pristine" directory (in the 1.6.x series, OpenNMS 1.6.9 or higher, and in the 1.7/1.8 series, OpenNMS 1.7.9 or higher).
If this is your first time installing OpenNMS, you can skip to the Create Your Branch section.
Don't forget to stop OpenNMS before messing with the etc directory!
[edit] Start with a Pristine etc Directory
If you have an existing OpenNMS installation, move your existing $OPENNMS_HOME/etc directory out of the way:
 mv $OPENNMS_HOME/etc $OPENNMS_HOME/etc.bak
Then, copy etc-pristine to $OPENNMS_HOME/etc. On RPM-based installations, this is in $OPENNMS_HOME/share/etc-pristine, on Debian, it's in /usr/share/opennms/etc-pristine.
 cp -pR $OPENNMS_HOME/share/etc-pristine $OPENNMS_HOME/etc
[edit] Initialize Git
Next, you'll turn your etc directory into a git repository, and add the pristine files as the first commit.
 cd $OPENNMS_HOME/etc
 git init 
 git add .
 git commit -m "Initial checkin of OpenNMS x.x.x configuration."
[edit] Create Your Branch
Finally, you'll create your branch to make local modifications. This way, whenever it's time to upgrade, you can put your git repo back in "pristine" mode to catch changes since the last version.
 cd $OPENNMS_HOME/etc
 git branch local-modifications
 git checkout local-modifications
[edit] Make Your Changes
Now, just edit your configuration normally, and you're all set. If you have an existing config that was backed up, you can just run:
 rsync -avr $OPENNMS_HOME/etc.bak/ $OPENNMS_HOME/etc/
Whenever you're done making changes to your etc directory, add and commit them, like so:
 cd $OPENNMS_HOME/etc
 git add .
 git commit -m "Added initial discovery ranges."
Now you can start OpenNMS back up again.
[edit] Doing an Upgrade
Now that you've got your etc directory managed by Git, it should be much easier to perform an upgrade. You just need to switch back to the pristine copy, upgrade, switch back, and merge the changes. Git has a very good merge algorithm, and will merge most configuration changes without any need for manual intervention.
[edit] Stop OpenNMS
Of course, the first step before doing an upgrade is to stop OpenNMS. =)
 /etc/init.d/opennms stop
[edit] Make Sure All Changes are Committed
Don't forget that if any files are modified, you'll want to add and commit them before doing the upgrade.
 git status
If it says there are modifications, follow the instructions in Make Your Changes above.
[edit] Switch to the Pristine Branch
The next step is to put your $OPENNMS_HOME/etc back to its pristine condition. To do this, all you need to do is check out the "master" branch, which contains the version of files from Initialize Git above.
 git checkout master
[edit] Upgrade OpenNMS
Now, upgrade OpenNMS in the usual manner (yum upgrade opennms, apt-get upgrade opennms, etc.). You should have no conflicts, since the etc files looked exactly like they did in the original package.
If you now run status:
 git status
... you'll see any changes to the default configs since the version of OpenNMS you've installed.
If any files have been deleted, you'll need to run 'git rm &lt;filename>' to tell git that they're gone.
 git rm CHANGELOG
 git rm map.disable
...and so on.
Then you'll need to add the new and changed files, and save these changes to the pristine/master branch, by adding and committing them:
 git add .
 git commit -m 'Upgraded to OpenNMS x.x.x.'
[edit] Switch to Your Modified Branch
Now that OpenNMS is upgraded, and Git knows about the changes to the configs since your previous release, it's time to switch back to your modified branch. All you need to do is check it out:
 git checkout local-modifications
[edit] Apply Changes from Master
Finally, you need to apply the changes from master to your local branch. To do so, you just need to 'merge' master into the current working branch ("local-modifications") like so:
 git merge master
If all goes well, you will see that the files that were changed in the master branch were auto-applied to your "local-modifications" working copy.
If not, you'll see something like this:
 git merge master
 Removing CHANGELOG
 Removing map.disable
 Auto-merging surveillance-views.xml
 CONFLICT (content): Merge conflict in surveillance-views.xml
 Automatic merge failed; fix conflicts and then commit the result.
A 'git status' will reveal the things that need modification with a "both modified":
 ...
 # Unmerged paths:
 #   (use "git reset HEAD &lt;file>..." to unstage)
 #   (use "git add &lt;file>..." to mark resolution)
 #
 #	both modified:      surveillance-views.xml
Git will insert a standard CVS-style conflict marker in each file where there are conflicts between your local changes and the updates. The changes from your branch will be first, and the changes from the pristine copies will be second, like so:

      &lt;rows>
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
        &lt;row-def row="1" label="Monkey Heads" >
          &lt;category name="Monkey Heads"/>
        &lt;/row-def>
        &lt;row-def row="2" label="Routers" >
          &lt;category name="Routers"/>
        &lt;/row-def>
        &lt;row-def row="3" label="Switches" >
          &lt;category name="Switches" />
        &lt;/row-def>
        &lt;row-def row="4" label="Servers" >
=======
        &lt;row-def label="Routers" >
          &lt;category name="Routers"/>
        &lt;/row-def>
        &lt;row-def label="Switches" >
          &lt;category name="Switches" />
        &lt;/row-def>
        &lt;row-def label="Servers" >
>>>>>>> master
          &lt;category name="Servers" />
        &lt;/row-def>
...just edit the files until they look like you'd expect (in this case, the 'row="n"' bits were removed from surveillance-views.xml) and add and commit the changes:
 git add .
 git commit
[edit] Check for Other Changes
If you're curious what changed in the last upgrade (ie, the last change done to the 'master' branch), you can diff master against its previous commit like so:
 git diff master~1..master
This works no matter which branch you're in at the time.
[edit] Start OpenNMS
Now that everything's merged, just start OpenNMS back up, and you should be in business!
</Text>
        </Document>
        <Document ID="70">
            <Title>Surveillance Categories</Title>
        </Document>
        <Document ID="66">
            <Title>Standard Services (out of the box)</Title>
        </Document>
        <Document ID="71">
            <Title>Filters</Title>
        </Document>
        <Document ID="67">
            <Title>Path Outages</Title>
        </Document>
    </Documents>
</SearchIndexes>